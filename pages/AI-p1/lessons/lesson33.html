<html><head><title>Lesson 33 == Imbalanced Dataset: ThÃ¡ch thá»©c vÃ  giáº£i phÃ¡p trong Machine Learning ==========//</title><style>body { font-family: Arial, sans-serif; transition: background 0.3s, color 0.3s; }.dark-mode { background-color: #121212; color: #e0e0e0; }.light-mode { background-color: #ffffff; color: #333333; }h1 { text-align: center; color: #73d9f5; }pre { padding: 15px; border-radius: 5px;       white-space: pre-wrap; word-wrap: break-word;       overflow-x: auto; max-width: 100%;       transition: background 0.3s, color 0.3s; }.dark-mode pre { background: #1e1e1e; color: #e0e0e0; }.light-mode pre { background: #f5f5f5; color: #333333; }#backTop, #backBottom {    font-size: 2em; padding: 20px 40px;    background: #bb86fc; color: white; text-decoration: none;    border-radius: 10px; display: inline-block; text-align: center; }#backTop:hover, #backBottom:hover { background: #9b67e2; }button { font-size: 1.5em; padding: 15px 30px;    background: #03dac6; color: #121212; border: none;    cursor: pointer; border-radius: 5px; display: block; margin: 10px auto; }button:hover { background: #02b8a3; }.dark-mode a { color: #03dac6; } .light-mode a { color: #007bff; }</style></head><body onload='applyTheme(); checkPageHeight()'><div class='container'><a id='backTop' href='../AI-learning-list.html'>ğŸ”™ Quay láº¡i danh sÃ¡ch</a><br><h1>Lesson 33 -- Imbalanced Dataset: ThÃ¡ch thá»©c vÃ  giáº£i phÃ¡p trong Machine Learning -//</h1><pre># 1. Giá»›i thiá»‡u 
	Trong lÄ©nh vá»±c Machine Learning, má»™t trong nhá»¯ng thÃ¡ch thá»©c phá»• biáº¿n mÃ  cÃ¡c nhÃ  khoa há»c dá»¯ liá»‡u gáº·p pháº£i lÃ  váº¥n Ä‘á» Imbalanced Dataset (Dá»¯ liá»‡u máº¥t cÃ¢n báº±ng). Äiá»u nÃ y thÆ°á»ng xuáº¥t hiá»‡n trong cÃ¡c á»©ng dá»¥ng thá»±c táº¿, nÆ¡i mÃ  má»™t hoáº·c nhiá»u lá»›p cá»§a táº­p dá»¯ liá»‡u cÃ³ sá»‘ lÆ°á»£ng máº«u lá»›n hÆ¡n nhiá»u so vá»›i cÃ¡c lá»›p cÃ²n láº¡i.
	
	VÃ­ dá»¥, trong cÃ¡c bÃ i toÃ¡n phÃ¢n loáº¡i y táº¿, sá»‘ lÆ°á»£ng bá»‡nh nhÃ¢n máº¯c bá»‡nh cá»¥ thá»ƒ cÃ³ thá»ƒ Ã­t hÆ¡n ráº¥t nhiá»u so vá»›i sá»‘ lÆ°á»£ng ngÆ°á»i khá»e máº¡nh.
	
	Trong bÃ i toÃ¡n phÃ¢n loáº¡i email, náº¿u cÃ³ 10.000 email mÃ  chá»‰ 500 trong sá»‘ Ä‘Ã³ lÃ  spam, chÃºng ta cÃ³ má»™t táº­p dá»¯ liá»‡u máº¥t cÃ¢n báº±ng. Tá»· lá»‡ máº«u giá»¯a cÃ¡c lá»›p cÃ³ thá»ƒ khÃ´ng Ä‘á»“ng Ä‘á»u, vÃ  Ä‘iá»u nÃ y thÆ°á»ng gÃ¢y ra váº¥n Ä‘á» lá»›n trong viá»‡c huáº¥n luyá»‡n mÃ´ hÃ¬nh Machine Learning.

# 2. TÃ¡c Ä‘á»™ng cá»§a Imbalanced Dataset Ä‘áº¿n mÃ´ hÃ¬nh Machine Learning

	https://aicandy.vn/wp-content/uploads/2024/11/aicandy_imbalanced_dataset_2.jpg
	
	Khi lÃ m viá»‡c vá»›i Imbalanced Dataset, mÃ´ hÃ¬nh Machine Learning cÃ³ thá»ƒ gáº·p pháº£i má»™t sá»‘ váº¥n Ä‘á» nghiÃªm trá»ng. CÃ¡c váº¥n Ä‘á» chÃ­nh bao gá»“m:

	## 2.1. ThiÃªn vá»‹ dá»± Ä‘oÃ¡n 
		MÃ´ hÃ¬nh cÃ³ thá»ƒ há»c cÃ¡ch dá»± Ä‘oÃ¡n lá»›p chiáº¿m Ä‘a sá»‘ má»™t cÃ¡ch thÆ°á»ng xuyÃªn, dáº«n Ä‘áº¿n viá»‡c bá» qua hoáº·c sai sÃ³t khi dá»± Ä‘oÃ¡n cÃ¡c máº«u thuá»™c lá»›p chiáº¿m thiá»ƒu sá»‘.
		
	## 2.2. Hiá»‡u suáº¥t Ä‘Ã¡nh giÃ¡ khÃ´ng chÃ­nh xÃ¡c 
		Chá»‰ sá»‘ Ä‘á»™ chÃ­nh xÃ¡c (Accuracy) cÃ³ thá»ƒ bá»‹ Ä‘Ã¡nh lá»«a, vÃ­ dá»¥: náº¿u mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n táº¥t cáº£ cÃ¡c máº«u thuá»™c lá»›p chiáº¿m Ä‘a sá»‘, nÃ³ cÃ³ thá»ƒ Ä‘áº¡t Ä‘Æ°á»£c Ä‘á»™ chÃ­nh xÃ¡c cao máº·c dÃ¹ khÃ´ng thá»±c sá»± hiá»‡u quáº£ trong viá»‡c phÃ¢n loáº¡i.
		
	## 2.3. Máº¥t kháº£ nÄƒng tá»•ng quÃ¡t hÃ³a 
		MÃ´ hÃ¬nh cÃ³ thá»ƒ khÃ´ng há»c Ä‘Æ°á»£c cÃ¡c Ä‘áº·c trÆ°ng quan trá»ng Ä‘á»ƒ phÃ¢n biá»‡t cÃ¡c lá»›p, Ä‘áº·c biá»‡t lÃ  lá»›p chiáº¿m thiá»ƒu sá»‘, do Ä‘Ã³ kháº£ nÄƒng tá»•ng quÃ¡t hÃ³a cá»§a mÃ´ hÃ¬nh sáº½ bá»‹ áº£nh hÆ°á»Ÿng nghiÃªm trá»ng. 
		
		
	## 2.4. áº¢nh hÆ°á»Ÿng cá»§a Imbalanced Dataset
		Giáº£ sá»­ chÃºng ta cÃ³ má»™t táº­p dá»¯ liá»‡u phÃ¢n loáº¡i nhá»‹ phÃ¢n vá»›i 95% thuá»™c lá»›p A vÃ  5% thuá»™c lá»›p B. Náº¿u má»™t mÃ´ hÃ¬nh chá»‰ Ä‘Æ¡n giáº£n dá»± Ä‘oÃ¡n táº¥t cáº£ cÃ¡c máº«u thuá»™c lá»›p A, nÃ³ sáº½ cÃ³ Ä‘á»™ chÃ­nh xÃ¡c 95%, nhÆ°ng nÃ³ hoÃ n toÃ n khÃ´ng há»¯u Ã­ch vÃ¬ khÃ´ng thá»ƒ nháº­n diá»‡n Ä‘Æ°á»£c cÃ¡c máº«u thuá»™c lá»›p B.
		
#  3. CÃ¡c phÆ°Æ¡ng phÃ¡p Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh vá»›i Imbalanced Dataset 
	Khi lÃ m viá»‡c vá»›i Imbalanced Dataset, viá»‡c sá»­ dá»¥ng cÃ¡c phÆ°Æ¡ng phÃ¡p Ä‘Ã¡nh giÃ¡ thÃ´ng thÆ°á»ng nhÆ° Ä‘á»™ chÃ­nh xÃ¡c (Accuracy) cÃ³ thá»ƒ gÃ¢y hiá»ƒu nháº§m vá» hiá»‡u suáº¥t thá»±c sá»± cá»§a mÃ´ hÃ¬nh. Do Ä‘Ã³, cáº§n sá»­ dá»¥ng cÃ¡c chá»‰ sá»‘ khÃ¡c phÃ¹ há»£p hÆ¡n vá»›i Ä‘áº·c thÃ¹ cá»§a dá»¯ liá»‡u máº¥t cÃ¢n báº±ng Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ hiá»‡u quáº£ cá»§a mÃ´ hÃ¬nh. DÆ°á»›i Ä‘Ã¢y lÃ  nhá»¯ng phÆ°Æ¡ng phÃ¡p Ä‘Ã¡nh giÃ¡ quan trá»ng khi lÃ m viá»‡c vá»›i Imbalanced Dataset.
	
	## 3.1. Confusion Matrix 
		https://aicandy.vn/wp-content/uploads/2024/09/aicandy_confusion.png 
		
		
		Confusion Matrix lÃ  má»™t cÃ´ng cá»¥ quan trá»ng Ä‘á»ƒ phÃ¢n tÃ­ch chi tiáº¿t cÃ¡c dá»± Ä‘oÃ¡n cá»§a mÃ´ hÃ¬nh. NÃ³ cung cáº¥p thÃ´ng tin vá» sá»‘ lÆ°á»£ng dá»± Ä‘oÃ¡n Ä‘Ãºng vÃ  sai cá»§a mÃ´ hÃ¬nh trÃªn tá»«ng lá»›p, giÃºp báº¡n hiá»ƒu rÃµ hÆ¡n vá» cÃ¡c lá»—i cá»§a mÃ´ hÃ¬nh. Confusion Matrix bao gá»“m cÃ¡c thÃ nh pháº§n sau:
		
		True Positives (TP): Sá»‘ máº«u Ä‘Æ°á»£c dá»± Ä‘oÃ¡n Ä‘Ãºng thuá»™c lá»›p dÆ°Æ¡ng tÃ­nh.
		False Positives (FP): Sá»‘ máº«u Ä‘Æ°á»£c dá»± Ä‘oÃ¡n lÃ  dÆ°Æ¡ng tÃ­nh nhÆ°ng thá»±c táº¿ thuá»™c lá»›p Ã¢m tÃ­nh.
		True Negatives (TN): Sá»‘ máº«u Ä‘Æ°á»£c dá»± Ä‘oÃ¡n Ä‘Ãºng thuá»™c lá»›p Ã¢m tÃ­nh.
		False Negatives (FN): Sá»‘ máº«u Ä‘Æ°á»£c dá»± Ä‘oÃ¡n lÃ  Ã¢m tÃ­nh nhÆ°ng thá»±c táº¿ thuá»™c lá»›p dÆ°Æ¡ng tÃ­nh.
		
		Confusion Matrix báº¡n cÃ³ thá»ƒ tÃ­nh toÃ¡n cÃ¡c chá»‰ sá»‘ nhÆ° Precision, Recall, vÃ  F1-score, nhá»¯ng chá»‰ sá»‘ nÃ y sáº½ giÃºp Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh tá»‘t hÆ¡n trong trÆ°á»ng há»£p dá»¯ liá»‡u máº¥t cÃ¢n báº±ng.
		
	
	## 3.2. Precision vÃ  Recall 
		Precision vÃ  Recall lÃ  hai chá»‰ sá»‘ Ä‘Ã¡nh giÃ¡ quan trá»ng, Ä‘áº·c biá»‡t khi dá»¯ liá»‡u bá»‹ máº¥t cÃ¢n báº±ng.
			Precision: ÄÆ°á»£c tÃ­nh báº±ng tá»· lá»‡ sá»‘ máº«u dÆ°Æ¡ng tÃ­nh thá»±c sá»± trong tá»•ng sá»‘ máº«u Ä‘Æ°á»£c dá»± Ä‘oÃ¡n lÃ  dÆ°Æ¡ng tÃ­nh. Precision Ä‘áº·c biá»‡t quan trá»ng trong cÃ¡c bÃ i toÃ¡n mÃ  viá»‡c dá»± Ä‘oÃ¡n sai lá»›p dÆ°Æ¡ng tÃ­nh (False Positives) cÃ³ thá»ƒ gÃ¢y ra háº­u quáº£ nghiÃªm trá»ng.
				Precision = \frac{TP}{TP + FP}

		
			Recall (Sensitivity, True Positive Rate): ÄÆ°á»£c tÃ­nh báº±ng tá»· lá»‡ sá»‘ máº«u dÆ°Æ¡ng tÃ­nh thá»±c sá»± Ä‘Æ°á»£c mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n Ä‘Ãºng so vá»›i tá»•ng sá»‘ máº«u dÆ°Æ¡ng tÃ­nh trong dá»¯ liá»‡u. Recall ráº¥t quan trá»ng trong cÃ¡c bÃ i toÃ¡n mÃ  viá»‡c bá» sÃ³t cÃ¡c máº«u dÆ°Æ¡ng tÃ­nh thá»±c sá»± (False Negatives) lÃ  váº¥n Ä‘á» lá»›n.
			
				Recall = \frac{TP}{TP + FN}

			Trong má»™t sá»‘ tÃ¬nh huá»‘ng, Precision vÃ  Recall cÃ³ thá»ƒ cÃ³ má»‘i quan há»‡ trÃ¡i ngÆ°á»£c, do Ä‘Ã³ cáº§n pháº£i cÃ¢n nháº¯c giá»¯a chÃºng khi Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh. Má»™t cÃ¡ch tiáº¿p cáº­n lÃ  sá»­ dá»¥ng F1-score, má»™t chá»‰ sá»‘ tá»•ng há»£p cáº£ Precision vÃ  Recall.

	## 3.3. F1-score 
		F1-score lÃ  trung bÃ¬nh hÃ i hÃ²a giá»¯a Precision vÃ  Recall, giÃºp cÃ¢n báº±ng giá»¯a viá»‡c tá»‘i Æ°u hÃ³a hai chá»‰ sá»‘ nÃ y. Khi Precision vÃ  Recall cÃ³ giÃ¡ trá»‹ chÃªnh lá»‡ch lá»›n, F1-score lÃ  chá»‰ sá»‘ há»¯u Ã­ch Ä‘á»ƒ cung cáº¥p má»™t cÃ¡i nhÃ¬n tá»•ng quÃ¡t hÆ¡n vá» hiá»‡u suáº¥t cá»§a mÃ´ hÃ¬nh.
		
		F1-score: 
			F1 = 2 \times \frac{Precision \times Recall}{Precision + Recall}
			
			F1-score Ä‘áº·c biá»‡t há»¯u Ã­ch trong cÃ¡c bÃ i toÃ¡n mÃ  cáº£ Precision vÃ  Recall Ä‘á»u quan trá»ng, vÃ  chÃºng ta cáº§n má»™t chá»‰ sá»‘ duy nháº¥t Ä‘á»ƒ so sÃ¡nh cÃ¡c mÃ´ hÃ¬nh hoáº·c Ä‘iá»u chá»‰nh cÃ¡c siÃªu tham sá»‘.

	## 3.4. ROC Curve vÃ  AUC (Area Under the Curve) 
		`ROC Curve (Receiver Operating Characteristic Curve) vÃ  AUC (Area Under the Curve) lÃ  cÃ¡c cÃ´ng cá»¥ Ä‘Ã¡nh giÃ¡ khÃ¡c Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i trong cÃ¡c bÃ i toÃ¡n phÃ¢n loáº¡i nhá»‹ phÃ¢n, Ä‘áº·c biá»‡t khi dá»¯ liá»‡u máº¥t cÃ¢n báº±ng. ROC Curve lÃ  Ä‘á»“ thá»‹ thá»ƒ hiá»‡n má»‘i quan há»‡ giá»¯a True Positive Rate (TPR) vÃ  False Positive Rate (FPR) á»Ÿ cÃ¡c ngÆ°á»¡ng phÃ¢n loáº¡i khÃ¡c nhau.
		
		True Positive Rate (TPR): CÃ²n Ä‘Æ°á»£c gá»i lÃ  Recall, Ä‘o lÆ°á»ng tá»· lá»‡ cÃ¡c máº«u dÆ°Æ¡ng tÃ­nh thá»±c sá»± Ä‘Æ°á»£c dá»± Ä‘oÃ¡n Ä‘Ãºng. 
			TPR = \frac{TP}{TP + FN}
			
		False Positive Rate (FPR): Äo lÆ°á»ng tá»· lá»‡ cÃ¡c máº«u Ã¢m tÃ­nh thá»±c sá»± nhÆ°ng Ä‘Æ°á»£c dá»± Ä‘oÃ¡n sai lÃ  dÆ°Æ¡ng tÃ­nh.
			FPR = \frac{FP}{FP + TN}

		ROC Curve giÃºp báº¡n Ä‘Ã¡nh giÃ¡ kháº£ nÄƒng phÃ¢n biá»‡t giá»¯a cÃ¡c lá»›p cá»§a mÃ´ hÃ¬nh á»Ÿ nhiá»u ngÆ°á»¡ng khÃ¡c nhau. AUC, diá»‡n tÃ­ch dÆ°á»›i Ä‘Æ°á»ng cong ROC, lÃ  má»™t giÃ¡ trá»‹ duy nháº¥t tÃ³m táº¯t hiá»‡u suáº¥t cá»§a mÃ´ hÃ¬nh; AUC cÃ ng gáº§n 1, mÃ´ hÃ¬nh cÃ ng tá»‘t. AUC lÃ  chá»‰ sá»‘ Ä‘áº·c biá»‡t há»¯u Ã­ch khi lÃ m viá»‡c vá»›i Imbalanced Dataset vÃ¬ nÃ³ khÃ´ng phá»¥ thuá»™c vÃ o tá»· lá»‡ giá»¯a cÃ¡c lá»›p.
	
	## 3.5. Precision-Recall Curve 
		Precision-Recall Curve lÃ  má»™t cÃ´ng cá»¥ khÃ¡c giÃºp Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh khi dá»¯ liá»‡u máº¥t cÃ¢n báº±ng. ÄÃ¢y lÃ  Ä‘á»“ thá»‹ thá»ƒ hiá»‡n má»‘i quan há»‡ giá»¯a Precision vÃ  Recall á»Ÿ cÃ¡c ngÆ°á»¡ng khÃ¡c nhau. Khi dá»¯ liá»‡u bá»‹ máº¥t cÃ¢n báº±ng nghiÃªm trá»ng (vÃ­ dá»¥: khi má»™t lá»›p ráº¥t hiáº¿m), Precision-Recall Curve thÆ°á»ng lÃ  má»™t cÃ´ng cá»¥ Ä‘Ã¡nh giÃ¡ tá»‘t hÆ¡n so vá»›i ROC Curve.
		
		Má»™t cÃ¡ch Ä‘á»ƒ tÃ³m táº¯t hiá»‡u suáº¥t cá»§a mÃ´ hÃ¬nh tá»« Precision-Recall Curve lÃ  tÃ­nh Average Precision (AP), tÆ°Æ¡ng tá»± nhÆ° AUC nhÆ°ng Ã¡p dá»¥ng cho Precision-Recall Curve. AP cao thá»ƒ hiá»‡n mÃ´ hÃ¬nh cÃ³ kháº£ nÄƒng giá»¯ Precision cao Ä‘á»“ng thá»i tÄƒng Recall.
	
# 4. Giáº£i phÃ¡p cho váº¥n Ä‘á» Imbalanced Dataset 	
	Äá»ƒ giáº£i quyáº¿t váº¥n Ä‘á» Imbalanced Dataset, cÃ³ nhiá»u phÆ°Æ¡ng phÃ¡p khÃ¡c nhau Ä‘Æ°á»£c Ã¡p dá»¥ng trong thá»±c táº¿. DÆ°á»›i Ä‘Ã¢y lÃ  má»™t sá»‘ giáº£i phÃ¡p phá»• biáº¿n.
	
	## 4.1. Resampling
		Resampling lÃ  má»™t ká»¹ thuáº­t phá»• biáº¿n Ä‘á»ƒ Ä‘á»‘i phÃ³ vá»›i dá»¯ liá»‡u máº¥t cÃ¢n báº±ng. CÃ³ hai loáº¡i Resampling chÃ­nh:
	
		###  Over-sampling 
			TÄƒng sá»‘ lÆ°á»£ng máº«u thuá»™c lá»›p chiáº¿m thiá»ƒu sá»‘ báº±ng cÃ¡ch sao chÃ©p cÃ¡c máº«u hiá»‡n cÃ³ hoáº·c táº¡o ra cÃ¡c máº«u má»›i sá»­ dá»¥ng cÃ¡c ká»¹ thuáº­t nhÆ° SMOTE (Synthetic Minority Over-sampling Technique). Äiá»u nÃ y giÃºp cÃ¢n báº±ng láº¡i táº­p dá»¯ liá»‡u báº±ng cÃ¡ch tÄƒng tá»· lá»‡ máº«u thuá»™c lá»›p thiá»ƒu sá»‘.
			
		### Under-sampling
			Giáº£m sá»‘ lÆ°á»£ng máº«u thuá»™c lá»›p chiáº¿m Ä‘a sá»‘ báº±ng cÃ¡ch loáº¡i bá» má»™t sá»‘ máº«u Ä‘á»ƒ cÃ¢n báº±ng láº¡i vá»›i lá»›p thiá»ƒu sá»‘. Tuy nhiÃªn, Ä‘iá»u nÃ y cÃ³ thá»ƒ dáº«n Ä‘áº¿n máº¥t thÃ´ng tin quan trá»ng.
			
		DÆ°á»›i Ä‘Ã¢y lÃ  vÃ­ dá»¥ vá» sá»­ dá»¥ng SMOTE trong PyTorch:	
	
	## 4.2. Sá»­ dá»¥ng cÃ¡c thuáº­t toÃ¡n nháº¡y cáº£m vá»›i Imbalanced Dataset 
		Má»™t sá»‘ thuáº­t toÃ¡n Machine Learning cÃ³ kháº£ nÄƒng xá»­ lÃ½ tá»‘t vá»›i dá»¯ liá»‡u máº¥t cÃ¢n báº±ng, cháº³ng háº¡n nhÆ°:
		
		### Tree-based methods 
			CÃ¡c phÆ°Æ¡ng phÃ¡p dá»±a trÃªn cÃ¢y quyáº¿t Ä‘á»‹nh nhÆ° Random Forest hay Gradient Boosting thÆ°á»ng phÃ¢n tÃ¡ch cÃ¡c lá»›p má»™t cÃ¡ch hiá»‡u quáº£ hÆ¡n khi Ä‘á»‘i máº·t vá»›i dá»¯ liá»‡u máº¥t cÃ¢n báº±ng. Nhá»¯ng thuáº­t toÃ¡n nÃ y cÃ³ thá»ƒ tÃ¬m ra nhá»¯ng Ä‘áº·c trÆ°ng quan trá»ng giÃºp phÃ¢n biá»‡t giá»¯a cÃ¡c lá»›p ngay cáº£ khi dá»¯ liá»‡u bá»‹ máº¥t cÃ¢n báº±ng.
	
		### Ensemble methods 
			CÃ¡c phÆ°Æ¡ng phÃ¡p káº¿t há»£p nhiá»u mÃ´ hÃ¬nh nhÆ° Bagging vÃ  Boosting cÃ³ thá»ƒ cáº£i thiá»‡n hiá»‡u suáº¥t trÃªn dá»¯ liá»‡u máº¥t cÃ¢n báº±ng báº±ng cÃ¡ch táº­p trung vÃ o cÃ¡c máº«u khÃ³ phÃ¢n loáº¡i, Ä‘áº·c biá»‡t lÃ  máº«u thuá»™c lá»›p thiá»ƒu sá»‘.
			
	## 4.3. Thay Ä‘á»•i trá»ng sá»‘ (Cost-sensitive Learning) 
		Thay Ä‘á»•i trá»ng sá»‘ lÃ  má»™t phÆ°Æ¡ng phÃ¡p khÃ¡c Ä‘á»ƒ xá»­ lÃ½ dá»¯ liá»‡u máº¥t cÃ¢n báº±ng. Trong phÆ°Æ¡ng phÃ¡p nÃ y, cÃ¡c lá»›p thiá»ƒu sá»‘ Ä‘Æ°á»£c gÃ¡n trá»ng sá»‘ lá»›n hÆ¡n trong hÃ m máº¥t mÃ¡t (loss function) Ä‘á»ƒ mÃ´ hÃ¬nh chÃº Ã½ hÆ¡n Ä‘áº¿n cÃ¡c lá»›p thiá»ƒu sá»‘ trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n. Äiá»u nÃ y giÃºp cÃ¢n báº±ng láº¡i tÃ¡c Ä‘á»™ng cá»§a cÃ¡c lá»›p thiá»ƒu sá»‘ vÃ  lá»›p chiáº¿m Ä‘a sá»‘ trong viá»‡c tá»‘i Æ°u hÃ³a mÃ´ hÃ¬nh. PhÆ°Æ¡ng phÃ¡p nÃ y Ä‘áº·c biá»‡t há»¯u Ã­ch khi khÃ´ng muá»‘n thay Ä‘á»•i sá»‘ lÆ°á»£ng máº«u trong táº­p dá»¯ liá»‡u (nhÆ° trong cÃ¡c phÆ°Æ¡ng phÃ¡p resampling).
		
		### VÃ­ dá»¥ vá» thay Ä‘á»•i trá»ng sá»‘ trong PyTorch 
			Trong PyTorch, báº¡n cÃ³ thá»ƒ dá»… dÃ ng Ã¡p dá»¥ng trá»ng sá»‘ lá»›p trong hÃ m máº¥t mÃ¡t nhÆ° sau:

	## 4.4. Sá»­ dá»¥ng phÆ°Æ¡ng phÃ¡p Ä‘Ã¡nh giÃ¡ thÃ­ch há»£p 
		Khi Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh trÃªn táº­p dá»¯ liá»‡u máº¥t cÃ¢n báº±ng, viá»‡c sá»­ dá»¥ng cÃ¡c chá»‰ sá»‘ Ä‘Ã¡nh giÃ¡ nhÆ° Accuracy khÃ´ng pháº£i lÃºc nÃ o cÅ©ng pháº£n Ã¡nh Ä‘Ãºng hiá»‡u suáº¥t cá»§a mÃ´ hÃ¬nh. Thay vÃ o Ä‘Ã³, cáº§n sá»­ dá»¥ng cÃ¡c chá»‰ sá»‘ nhÆ° Precision, Recall, F1-score, vÃ  ROC-AUC.
		
		### Precision 
			ÄÆ°á»£c sá»­ dá»¥ng Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ tá»· lá»‡ dá»± Ä‘oÃ¡n chÃ­nh xÃ¡c trong cÃ¡c dá»± Ä‘oÃ¡n dÆ°Æ¡ng tÃ­nh. Äáº·c biá»‡t quan trá»ng trong cÃ¡c bÃ i toÃ¡n mÃ  viá»‡c dá»± Ä‘oÃ¡n sai lá»›p thiá»ƒu sá»‘ cÃ³ thá»ƒ gÃ¢y háº­u quáº£ nghiÃªm trá»ng, nhÆ° cháº©n Ä‘oÃ¡n y táº¿.
			
		### Recall 
			Äo lÆ°á»ng kháº£ nÄƒng phÃ¡t hiá»‡n táº¥t cáº£ cÃ¡c máº«u thuá»™c lá»›p thiá»ƒu sá»‘. Cao khi mÃ´ hÃ¬nh cÃ³ kháº£ nÄƒng phÃ¡t hiá»‡n táº¥t cáº£ cÃ¡c trÆ°á»ng há»£p thuá»™c lá»›p thiá»ƒu sá»‘, dÃ¹ cho cÃ³ nhiá»u dá»± Ä‘oÃ¡n sai.
			
		### F1-score 
			Trung bÃ¬nh hÃ i hÃ²a giá»¯a Precision vÃ  Recall, mang láº¡i má»™t chá»‰ sá»‘ cÃ¢n báº±ng hÆ¡n khi hai chá»‰ sá»‘ nÃ y cÃ³ sá»± chÃªnh lá»‡ch.
			
		### ROC-AUC 
			Chá»‰ sá»‘ nÃ y Ä‘o lÆ°á»ng kháº£ nÄƒng cá»§a mÃ´ hÃ¬nh trong viá»‡c phÃ¢n biá»‡t giá»¯a cÃ¡c lá»›p. ÄÆ°á»ng cong ROC biá»ƒu thá»‹ má»‘i quan há»‡ giá»¯a True Positive Rate (TPR) vÃ  False Positive Rate (FPR), vÃ  AUC thá»ƒ hiá»‡n diá»‡n tÃ­ch dÆ°á»›i Ä‘Æ°á»ng cong nÃ y. GiÃ¡ trá»‹ AUC cÃ ng gáº§n 1, mÃ´ hÃ¬nh cÃ ng tá»‘t.
			
	

	## 4.5. Ká»¹ thuáº­t Ensemble (Ensemble Techniques) 
		CÃ¡c ká»¹ thuáº­t Ensemble káº¿t há»£p nhiá»u mÃ´ hÃ¬nh khÃ¡c nhau Ä‘á»ƒ táº¡o ra má»™t mÃ´ hÃ¬nh máº¡nh máº½ hÆ¡n, cÃ³ thá»ƒ kháº¯c phá»¥c Ä‘Æ°á»£c cÃ¡c nhÆ°á»£c Ä‘iá»ƒm cá»§a tá»«ng mÃ´ hÃ¬nh Ä‘Æ¡n láº». CÃ¡c phÆ°Æ¡ng phÃ¡p Ensemble phá»• biáº¿n bao gá»“m: 
		
		### Bagging 
			Ká»¹ thuáº­t nÃ y bao gá»“m viá»‡c huáº¥n luyá»‡n nhiá»u mÃ´ hÃ¬nh Ä‘á»™c láº­p trÃªn cÃ¡c máº«u khÃ¡c nhau cá»§a táº­p dá»¯ liá»‡u, sau Ä‘Ã³ káº¿t há»£p cÃ¡c káº¿t quáº£ dá»± Ä‘oÃ¡n cá»§a chÃºng. Má»™t vÃ­ dá»¥ ná»•i báº­t lÃ  Random Forest, nÆ¡i nhiá»u cÃ¢y quyáº¿t Ä‘á»‹nh Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn cÃ¡c táº­p con cá»§a dá»¯ liá»‡u vÃ  káº¿t quáº£ cuá»‘i cÃ¹ng Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh dá»±a trÃªn bÃ¬nh chá»n Ä‘a sá»‘.
			
		### Boosting 
			KhÃ¡c vá»›i Bagging, Boosting huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh má»™t cÃ¡ch tuáº§n tá»±, trong Ä‘Ã³ má»—i mÃ´ hÃ¬nh má»›i Ä‘Æ°á»£c huáº¥n luyá»‡n Ä‘á»ƒ kháº¯c phá»¥c lá»—i cá»§a mÃ´ hÃ¬nh trÆ°á»›c Ä‘Ã³. Gradient Boosting vÃ  AdaBoost lÃ  nhá»¯ng vÃ­ dá»¥ Ä‘iá»ƒn hÃ¬nh. Boosting thÆ°á»ng cÃ³ kháº£ nÄƒng tá»‘t hÆ¡n trong viá»‡c phÃ¡t hiá»‡n cÃ¡c máº«u thiá»ƒu sá»‘ do sá»± nháº¥n máº¡nh vÃ o cÃ¡c máº«u khÃ³ phÃ¢n loáº¡i.
			
		### Stacking 
			Trong Stacking, cÃ¡c mÃ´ hÃ¬nh khÃ¡c nhau Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn cÃ¹ng má»™t táº­p dá»¯ liá»‡u, vÃ  cÃ¡c dá»± Ä‘oÃ¡n cá»§a chÃºng Ä‘Æ°á»£c sá»­ dá»¥ng nhÆ° Ä‘áº§u vÃ o cho má»™t mÃ´ hÃ¬nh â€œmeta-modelâ€ Ä‘á»ƒ táº¡o ra dá»± Ä‘oÃ¡n cuá»‘i cÃ¹ng. Ká»¹ thuáº­t nÃ y cÃ³ thá»ƒ táº¡o ra nhá»¯ng káº¿t quáº£ máº¡nh máº½ hÆ¡n báº±ng cÃ¡ch khai thÃ¡c Ä‘iá»ƒm máº¡nh cá»§a tá»«ng mÃ´ hÃ¬nh thÃ nh pháº§n.
			
	## 4.6. Data Augmentation 
		Data Augmentation lÃ  má»™t ká»¹ thuáº­t táº¡o ra cÃ¡c máº«u dá»¯ liá»‡u má»›i tá»« cÃ¡c máº«u dá»¯ liá»‡u hiá»‡n cÃ³ báº±ng cÃ¡ch Ã¡p dá»¥ng cÃ¡c phÃ©p biáº¿n Ä‘á»•i nhÆ° xoay, láº­t, hoáº·c thay Ä‘á»•i Ä‘á»™ sÃ¡ng, Ä‘áº·c biá»‡t phá»• biáº¿n trong cÃ¡c bÃ i toÃ¡n xá»­ lÃ½ áº£nh. PhÆ°Æ¡ng phÃ¡p nÃ y giÃºp tÄƒng sá»‘ lÆ°á»£ng máº«u thuá»™c lá»›p thiá»ƒu sá»‘ mÃ  khÃ´ng cáº§n thu tháº­p thÃªm dá»¯ liá»‡u má»›i, tá»« Ä‘Ã³ giÃºp cÃ¢n báº±ng táº­p dá»¯ liá»‡u vÃ  cáº£i thiá»‡n hiá»‡u suáº¥t cá»§a mÃ´ hÃ¬nh.
		
		VÃ­ dá»¥ vá» Data Augmentation trong PyTorch


# 5. Káº¿t luáº­n
	Imbalanced Dataset lÃ  má»™t thÃ¡ch thá»©c lá»›n trong Machine Learning, nhÆ°ng khÃ´ng pháº£i lÃ  má»™t trá»Ÿ ngáº¡i khÃ´ng thá»ƒ vÆ°á»£t qua. Báº±ng cÃ¡ch Ã¡p dá»¥ng cÃ¡c ká»¹ thuáº­t phÃ¹ há»£p nhÆ° Resampling, thay Ä‘á»•i trá»ng sá»‘, sá»­ dá»¥ng cÃ¡c thuáº­t toÃ¡n nháº¡y cáº£m vá»›i dá»¯ liá»‡u máº¥t cÃ¢n báº±ng, vÃ  káº¿t há»£p nhiá»u phÆ°Æ¡ng phÃ¡p, chÃºng ta cÃ³ thá»ƒ cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ hiá»‡u suáº¥t cá»§a mÃ´ hÃ¬nh.

	Quan trá»ng hÆ¡n cáº£ lÃ  viá»‡c hiá»ƒu rÃµ vá» báº£n cháº¥t cá»§a váº¥n Ä‘á» vÃ  lá»±a chá»n nhá»¯ng phÆ°Æ¡ng phÃ¡p phÃ¹ há»£p vá»›i Ä‘áº·c thÃ¹ cá»§a tá»«ng bÃ i toÃ¡n cá»¥ thá»ƒ. Äiá»u nÃ y sáº½ giÃºp báº¡n xÃ¢y dá»±ng nhá»¯ng mÃ´ hÃ¬nh khÃ´ng chá»‰ chÃ­nh xÃ¡c mÃ  cÃ²n Ä‘á»§ linh hoáº¡t Ä‘á»ƒ xá»­ lÃ½ dá»¯ liá»‡u thá»±c táº¿, nÆ¡i mÃ  dá»¯ liá»‡u máº¥t cÃ¢n báº±ng thÆ°á»ng xuyÃªn xuáº¥t hiá»‡n.


</pre><a id='backBottom' href='../AI-learning-list.html' style='display:none;'>ğŸ”™ Quay láº¡i danh sÃ¡ch</a><br><button onclick='toggleTheme()'>ğŸŒ™ Chuyá»ƒn giao diá»‡n</button></div><script>function toggleTheme() {   let mode = document.body.classList.contains('dark-mode') ? 'light-mode' : 'dark-mode';   document.body.className = mode; localStorage.setItem('theme', mode);   syncTheme();}function applyTheme() {   let savedTheme = localStorage.getItem('theme') || 'dark-mode';   document.body.className = savedTheme;   syncTheme();}function syncTheme() {   let preElement = document.querySelector('pre');   if (document.body.classList.contains('dark-mode')) { preElement.style.background = '#1e1e1e'; preElement.style.color = '#e0e0e0'; }   else { preElement.style.background = '#f5f5f5'; preElement.style.color = '#333333'; }}function checkPageHeight() {   let contentHeight = document.body.scrollHeight;   let windowHeight = window.innerHeight;   if (contentHeight > windowHeight * 1.2) {       document.getElementById('backBottom').style.display = 'block';   } else {       document.getElementById('backBottom').style.display = 'none';   }}</script></body></html>