<html><head><title>Lesson 5 == H·ªìi quy tuy·∫øn t√≠nh ==========//</title><style>body { font-family: Arial, sans-serif; transition: background 0.3s, color 0.3s; }.dark-mode { background-color: #121212; color: #e0e0e0; }.light-mode { background-color: #ffffff; color: #333333; }h1 { text-align: center; color: #73d9f5; }pre { padding: 15px; border-radius: 5px;       white-space: pre-wrap; word-wrap: break-word;       overflow-x: auto; max-width: 100%;       transition: background 0.3s, color 0.3s; }.dark-mode pre { background: #1e1e1e; color: #e0e0e0; }.light-mode pre { background: #f5f5f5; color: #333333; }#backTop, #backBottom {    font-size: 2em; padding: 20px 40px;    background: #bb86fc; color: white; text-decoration: none;    border-radius: 10px; display: inline-block; text-align: center; }#backTop:hover, #backBottom:hover { background: #9b67e2; }button { font-size: 1.5em; padding: 15px 30px;    background: #03dac6; color: #121212; border: none;    cursor: pointer; border-radius: 5px; display: block; margin: 10px auto; }button:hover { background: #02b8a3; }.dark-mode a { color: #03dac6; } .light-mode a { color: #007bff; }</style></head><body onload='applyTheme(); checkPageHeight()'><div class='container'><a id='backTop' href='../AI-learning-list.html'>üîô Quay l·∫°i danh s√°ch</a><br><h1>Lesson 5 -- H·ªìi quy tuy·∫øn t√≠nh -//</h1><pre>
# 1. Gi·ªõi thi·ªáu 
	
		H·ªìi quy tuy·∫øn t√≠nh l√† m·ªôt k·ªπ thu·∫≠t th·ªëng k√™ c∆° b·∫£n ƒë∆∞·ª£c s·ª≠ d·ª•ng r·ªông r√£i trong ph√¢n t√≠ch d·ªØ li·ªáu v√† h·ªçc m√°y. M·ª•c ti√™u c·ªßa h·ªìi quy tuy·∫øn t√≠nh l√† x√¢y d·ª±ng m·ªôt m√¥ h√¨nh to√°n h·ªçc ƒë·ªÉ d·ª± ƒëo√°n gi√° tr·ªã c·ªßa m·ªôt bi·∫øn ph·ª• thu·ªôc (output) d·ª±a tr√™n m·ªôt ho·∫∑c nhi·ªÅu bi·∫øn ƒë·ªôc l·∫≠p(input). M√¥ h√¨nh h·ªìi quy tuy·∫øn t√≠nh gi·∫£ ƒë·ªãnh r·∫±ng m·ªëi quan h·ªá gi·ªØa c√°c bi·∫øn n√†y l√† tuy·∫øn t√≠nh, nghƒ©a l√† c√≥ th·ªÉ bi·ªÉu di·ªÖn d∆∞·ªõi d·∫°ng m·ªôt ƒë∆∞·ªùng th·∫≥ng trong kh√¥ng gian s·ªë li·ªáu. 
		https://aicandy.vn/wp-content/uploads/2024/09/aicandy_hoiquytuyentinh.jpg
		
		SimpleLinearRegression : H·ªìi quy tuy·∫øn t√≠nh ƒë∆°n gi·∫£n 
		Multiple Linear Regression : H·ªìi quy tuy·∫øn t√≠nh ƒëa bi·∫øn 
		
# 2.  Ph√¢n lo·∫°i : 
	## 2.1 H·ªìi quy tuy·∫øn t√≠nh ƒë∆°n gi·∫£n 
		H·ªìi quy tuy·∫øn t√≠nh ƒë∆°n gi·∫£m m√¥ h√¨nh h√≥a m·ªëi quan h·ªá gi·ªØa m·ªôt bi·∫øn ƒë·ªôc l·∫≠p X v√† m·ªôt bi·∫øn ph·ª• thu·ªôc Y . C√¥ng th·ª©c c·ªßa m√¥ h√¨nh h·ªìi quy tuy·∫øn t√≠nh ƒë∆°n gi·∫£n l√† : 
						
						Y = b0 + b1X + e 
		
		Trong ƒë√≥ : 
			- Y l√† bi·∫øn ph·ª• thu·ªôc (output) <mjx-c class="mjx-c1D44C TEX-I"></mjx-c>
			- X l√† bi·∫øn ƒë·ªôc l·∫≠p (input)		<mjx-c class="mjx-c1D44B TEX-I"></mjx-c>
			- B0 l√† h·∫±ng s·ªë (intercept)  <mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D6FD TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c30"></mjx-c></mjx-mn></mjx-script></mjx-msub>
			- B1 l√† h·ªá s·ªë h·ªìi quy (slope) , ƒë·∫°i di·ªán cho m·ª©c thay ƒë·ªïi c·ªßa Y khi X thay ƒë·ªïi m·ªôt ƒë∆°n v·ªã   <mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D6FD TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-script></mjx-msub>
			- e l√† sai s·ªë(error tern), bi·ªÉu th·ªã ph·∫ßn bi·∫øn ƒë·ªông c·ªßa Y kh√¥ng ƒë∆∞·ª£c gi·∫£i th√≠ch b·ªüi X  <mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D716 TEX-I"></mjx-c></mjx-mi></mjx-math>
			
		#### Trong to√°n h·ªçc 
			- e l√† ph·∫ßn ch√™nh l·ªách gi·ªØa gi√° tr·ªã th·ª±c t·∫ø y v√† gi√° tr·ªã d·ª± ƒëo√°n y^ m√† m√¥ h√¨nh t√≠nh ƒë∆∞·ª£c 
			- N√≥ bi·ªÉu th·ªã nh·ªØng y·∫øu t·ªë kh√¥ng ƒë∆∞·ª£c m√¥ h√¨nh h√≥a ho·∫∑c c√°c y·∫øu t·ªë ng·∫´u nhi√™n kh√¥ng n·∫±m trong d·ªØ li·ªáu X 
			
		#### Trong AI 
			- Sai s·ªë e l√† m·ªôt th∆∞·ªõc ƒëo ƒë·ªÉ m√¥ h√¨nh bi·∫øt n√≥ d·ª± ƒëo√°n k√©m ch√≠nh x√°c ·ªü m·ª©c n√†o 
			- M·ª•c ti√™u khi hu·∫•n luy·ªán m·ªôt m√¥ h√¨nh l√† l√†m cho t·ªïng e ho·∫∑c m·ªôt d·∫°ng kh√°c c·ªßa sai s·ªë nh∆∞ b√¨nh ph∆∞∆°ng c·ªßa e c√†ng nh·ªè c√†ng t·ªët, t·ª©c l√† l√†m m√¥ h√¨nh kh·ªõp t·ªët nh·∫•t v·ªõi d·ªØ li·ªáu. 
			
		### Li√™n quan ƒë·∫øn MachineLearning 
			- Trong h·ªçc m√°y, sai s·ªë n√†y ƒë∆∞·ª£c t·ªëi ∆∞u h√≥a qua c√°c ph∆∞∆°ng ph√°p nh∆∞ gradient sescent. M√¥ h√¨nh s·∫Ω ƒëi·ªÅu ch·ªânh c√°c tham s·ªë b0, b1 ƒë·ªÉ gi·∫£m sai s·ªë e 
			- N·∫øu e qu√° l·ªõn ho·∫∑c ph√¢n ph·ªëi kh√¥ng ng·∫´u nhi√™n(V√≠ d·ª• c√≥ m·∫´u h√¨nh l·∫∑p l·∫°i), ƒëi·ªÅu n√†y cho th·∫•y m√¥ h√¨nh ch∆∞a ph√π h·ª£p ho·∫∑c d·ªØ li·ªáu c√≥ v·∫•n ƒë·ªÅ   
	
	## 2.2  H·ªìi quy tuy·∫øn t√≠nh ƒëa bi·∫øn 
		H·ªìi quy tuy·∫øn t√≠nh ƒëa bi·∫øn m·ªü r·ªông m√¥ h√¨nh tr√™n b·∫±ng c√°ch s·ª≠ d·ª•ng nhi·ªÅu bi·∫øn ƒë·ªôc l·∫≠p ƒë·ªÉ d·ª± ƒëo√°n bi·∫øn ph·ª• thu·ªôc. C√¥ng th·ª©c t·ªïng qu√°t c·ªßa m√¥ h√¨nh h·ªìi quy tuy·∫øn t√≠nh ƒëa bi·∫øn l√† : 
			
				Y = B0 + B1.X1 + B2.X2 + ..... + Bn.Xn + e 
				
			- Trong ƒë√≥ : 
				X1 , X2 , .... Xn l√† c√°c bi·∫øn ƒë·ªôc l·∫≠p 
				B1,B2,Bn l√† c√°c h·ªá s·ªë h·ªìi quy t∆∞∆°ng ·ª©ng, bi·ªÉu th·ªã m·ª©c thay ƒë·ªïi c·ªßa Y m·ªói khi Xi thay ƒë·ªïi m·ªôt ƒë∆°n v·ªã 
				b0 : L√† gi√° tr·ªã y khi c·∫£ x1 v√† x2 ƒë·ªÅu b·∫±ng 0 , ƒë√¢y l√† ƒëi·ªÉm kh·ªüi ƒë·∫ßu tr√™n tr·ª•c y 
				b1: l√† h·ªá s·ªë h·ªìi quy c·ªßa x1. N√≥ cho bi·∫øt khi x1 tƒÉng l√™n 1 ƒë∆°n v·ªã, y s·∫Ω thay ƒë·ªïi bao nhi√™u 
				b2 : L√† h·ªá s·ªë h·ªìi quy c·ªßa x2. N√≥ cho bi·∫øt khi x2 tƒÉng l√™n 1 ƒë∆°n v·ªã (v·ªõi x1 gi·ªØ nguy√™n), y s·∫Ω thay ƒë·ªïi bao nhi√™u 
				
			-> M√¥ h√¨nh n√†y c·ªë g·∫Øng t√¨m ra m·ªëi quan h·ªá gi·ªØa 2 bi·∫øn ƒë·∫ßu v√†o (x1,x2)v√† m·ªôt bi·∫øn ƒë·∫ßu ra(y). N√≥ t√≠nh to√°n ƒë·ªÉ t√¨m ra m·ªôt m·∫∑t ph·∫≥ng trong kh√¥ng gian 3d sao cho m·∫∑t ph·∫≥ng n√†y ph√π h·ª£p nh·∫•t v·ªõi c√°c ƒëi·ªÉm d·ªØ li·ªáu  
				
# 3 C√°ch t√≠nh h·ªá s·ªë h·ªìi quy 
	
	H·ªá s·ªë h·ªìi quy B ƒë∆∞·ª£c x√°c ƒë·ªãnh b·∫±ng c√°ch t·ªëi hi·ªÉu h√≥a t·ªïng b√¨nh ph∆∞∆°ng sai s·ªë(Residual Sum of Squares - RSS ) 
	gi·ªØa gi√° tr·ªã d·ª± ƒëo√°n v√† gi√° tr·ªã th·ª±c t·∫ø. Ph∆∞∆°ng ph√°p ph·ªï bi·∫øn nh·∫•t ƒë·ªÉ ∆∞·ªõc l∆∞·ª£ng c√°c h·ªá s·ªë h·ªìi quy l√† ph∆∞∆°ng ph√°p b√¨nh ph∆∞∆°ng t·ªëi thi·ªÉu (Ordinary Least Squares - OLS )
	
	## Ph∆∞∆°ng ph√°p b√¨nh ph∆∞∆°ng t·ªëi thi·ªÉu 
		C√¥ng th·ª©c ƒë·ªÉ t√≠nh h·ªá s·ªë h·ªìi quy trong h·ªìi quy tuy·∫øn t√≠nh ƒë∆°n gi·∫£n : 
		### 1. B1 h·ªá s·ªë g√≥c 
			<MathJax Original Source>
			\hat{\beta_1} = \frac{\sum_{i=1}^{n} (X_i ‚Äì \bar{X})(Y_i ‚Äì \bar{Y})}{\sum_{i=1}^{n} (X_i ‚Äì \bar{X})^2}
			
		- Trong ƒë√≥ : 
			b1  v√† b0 l√† c√°c gi√° tr·ªã ∆∞·ªõc l∆∞·ª£ng c·ªßa B1 v√† B0. 
			Xi  v√† Yi l√† c√°c gi√° tr·ªã bi·∫øn ƒë·ªôc l·∫≠p c·ªßa bi·∫øn ph·ª• thu·ªôc t·∫°i ƒëi·ªÉm d·ªØ li·ªáu th·ª© i 
			X_ v√† Y_ l√† gi√° tr·ªã trung b√¨nh c·ªßa X v√† Y 

	### 2. B0 : H·ªá s·ªë ch·∫∑n  
				b0 = y_ - b1 x_
	
		
#4. M√¥ h√¨nh h·ªìi quy tuy·∫øn t√≠nh v·ªõi Python 
	## 4.1 Th·ª±c hi·ªán h·ªìi quy tuy·∫øn t√≠nh ƒë∆°n gi·∫£n b·∫±ng Python ()
		
		### e.g V√≠ d·ª• v·ªÅ c√°ch th·ª±c hi·ªán h·ªìi quy tuy·∫øn t√≠nh ƒë∆°n gi·∫£n b·∫±ng Python s·ª≠ d·ª•ng th∆∞ vi√™n scikit-learn ƒë·ªÉ d·ª± ƒëo√°n m·ªôt bi·∫øn y d·ª±a tr√™n bi·∫øn x 
					import numpy as np
					import matplotlib.pyplot as plt 
					from sklearn.linear_model import LinearRegression 
					
					''' V√≠ d·ª• v·ªÅ c√°ch th·ª±c hi·ªán h·ªìi quy tuy·∫øn t√≠nh ƒë∆°n gi·∫£n b·∫±ng Python s·ª≠ d·ª•ng th∆∞ vi√™n scikit-learn'''
					
					# D·ªØ li·ªáu m·∫´u 
					X = np.array([1,2,3,4,5]).reshape(-1,1)
					y = np.array([1,3,3,2,5])
					
					# Kh·ªüi t·∫°o m√¥ h√¨nh 
					model = LinearRegression()
					
					# Hu·∫•n luy·ªán m√¥ h√¨nh 
					model.fit(X,y)
					
					# D·ª± ƒëo√°n 
					y_pred =  model.predict(X)
					
					# H·ªá s·ªë h·ªìi quy 
					print(f"H·ªá s·ªë h·ªìi quy : {model.coef_[0]}")
					print(f"H·ªá s·ªë h·ªìi quy : {model.intercept_}")
					
					# V·∫Ω ƒë·ªì th·ªã 
					plt.scatter(X , y,color='blue')
					plt.plot(X , y_pred , color='red')
					plt.xlabel('X')
					plt.ylabel('Y')
					plt.title('H·ªìi quy tuy·∫øn t√≠nh ƒë∆°n gi·∫£n')
					plt.show()

			
		### Gi·∫£i th√≠ch 
			1. Th∆∞ vi·ªán s·ª≠ d·ª•ng 
				numpy : X·ª≠ l√Ω d·ªØ li·ªáu d∆∞·ªõi d·∫°ng m·∫£ng s·ªë h·ªçc 
				matplotlib.pyplot: V·∫Ω ƒë·ªì th·ªã ƒë·ªÉ tr·ª±c quan h√≥a d·ªØ li·ªáu 
				sklearn.linear_model.LinearRegression : Cung c·∫•p m√¥ h√¨nh h·ªìi quy tuy·∫øn t√≠nh 
			
			2. D·ªØ li·ªáu m·∫´u : 
				X = np.array([1,2,3,4,5]).reshape(-1,1): ƒê√¢y l√† bi·∫øn ƒë·ªôc l·∫≠p X, ƒë∆∞·ª£c ƒë·ªãnh d·∫°ng th√†nh m·∫£ng c·ªôt (v√¨ scikit-learn y√™u c·∫ßu ƒë·∫ßu v√†o ph·∫£i c√≥ ƒë·ªãnh d·∫°ng n√†y )
				
				y = np.array([1,3,3,2,5]): L√† bi·∫øn ph·ª• thu·ªôc y m√† b·∫°n mu·ªën d·ª± ƒëo√°n  
			
			3. Kh·ªüi t·∫°o m√¥ h√¨nh hu·∫•n luy·ªán : 
				model = LinearRegression(): Kh·ªüi t·∫°o m·ªôt m√¥ h√¨nh h·ªìi quy tuy·∫øn t√≠nh  
				
				model.fit(X,y) : Hu·∫•n luy·ªán m√¥ h√¨nh b·∫±ng c√°ch t√¨m ra c√°c h·ªá s·ªë h·ªìi quy b0 v√† b1 d·ª±a tr√™n d·ªØ li·ªáu X v√† y 
				
			4. D·ª± ƒëo√°n 
				y_pred = model.predict(X): D·ª± ƒëo√°n gi√° tr·ªã y d·ª±a tr√™n d·ªØ li·ªáu X 
				
			5. In c√°c h·ªá s·ªë h·ªìi quy : 
				model.coef_ : H·ªá s·ªë g√≥c b1, cho bi·∫øt m·ª©c ƒë·ªô thay ƒë·ªïi c·ªßa y khi x thay ƒë·ªïi 
				
				model.intercept_ : H·ªá s·ªë ch·∫∑n b0, gi√° tr·ªã y khi x = 0 
				
			6. V·∫Ω ƒë·ªì th·ªã : 
				plt.scatter(X , y , color = 'blue') : V·∫Ω c√°c ƒëi·ªÉm d·ªØ li·ªáu th·ª±c t·∫ø(X , y).
				
				plt.plot(X , y_pred, color = 'red') : v·∫Ω ƒë∆∞·ªùng h·ªìi quy tuy·∫øn t√≠nh 
				
				plt.xlabel  , plt.ylabel , plt.title : ƒê·∫∑t nh√£n v√† ti√™u ƒë·ªÅ cho ƒë·ªì th·ªã. 
				
			--
			
				
			
			
	# 4.2 Th·ª±c hi·ªán h·ªìi quy tuy·∫øn t√≠nh ƒëa bi·∫øn b·∫±ng Python 
		
			import numpy as np
			from sklearn.linear_model import LinearRegression
			import matplotlib.pyplot as plt
			from mpl_toolkits.mplot3d import Axes3D
			
			# D·ªØ li·ªáu m·∫´u
			X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])
			y = np.dot(X, np.array([1, 2])) + 3
			
			# Kh·ªüi t·∫°o m√¥ h√¨nh
			model = LinearRegression()
			
			# Hu·∫•n luy·ªán m√¥ h√¨nh
			model.fit(X, y)
			
			# D·ª± ƒëo√°n
			y_pred = model.predict(X)
			
			# H·ªá s·ªë h·ªìi quy
			print(f"H·ªá s·ªë h·ªìi quy: {model.coef_}")
			print(f"Intercept: {model.intercept_}")
			
			# V·∫Ω ƒë·ªì th·ªã 3D
			fig = plt.figure()
			ax = fig.add_subplot(111, projection='3d')
			
			# D·ªØ li·ªáu g·ªëc (ch·∫•m xanh)
			ax.scatter(X[:, 0], X[:, 1], y, color='blue', label='D·ªØ li·ªáu th·ª±c t·∫ø')
			
			# D·ªØ li·ªáu d·ª± ƒëo√°n (m·∫∑t ph·∫≥ng h·ªìi quy)
			x1_range = np.linspace(X[:, 0].min(), X[:, 0].max(), 10)
			x2_range = np.linspace(X[:, 1].min(), X[:, 1].max(), 10)
			x1_grid, x2_grid = np.meshgrid(x1_range, x2_range)
			y_grid = model.intercept_ + model.coef_[0] * x1_grid + model.coef_[1] * x2_grid
			ax.plot_surface(x1_grid, x2_grid, y_grid, alpha=0.5, color='red', label='M·∫∑t ph·∫≥ng h·ªìi quy')
			
			# G√°n nh√£n cho c√°c tr·ª•c
			ax.set_xlabel('X1')
			ax.set_ylabel('X2')
			ax.set_zlabel('Y')
			ax.set_title('H·ªìi quy tuy·∫øn t√≠nh ƒëa bi·∫øn')
			ax.legend()
			
			plt.show()
		
		
		
		### Gi·∫£i th√≠ch v·ªÅ c√°c bi·∫øn 
		
		- X : ƒê√¢y l√† d·ªØ li·ªáu ƒë·∫ßu v√†o bi·∫øn ƒë·ªôc l·∫≠p, c√≥ 4 h√†ng v√† 2 c·ªôt : 
			- M·ªói h√†ng ƒë·∫°i di·ªán cho m·ªôt quan s√°t, v√† m·ªói c·ªôt l√† m·ªôt bi·∫øn ƒë·ªôc l·∫≠p(feature)
		
		- y : ƒê√¢y l√† d·ªØ li·ªáu ƒë·∫ßu ra(Bi·∫øn ph·ª• thu·ªôc), ƒë∆∞·ª£c t√≠nh b·∫±ng c√¥ng th·ª©c  y = X .[1,2] + 3
			
		-  np.dot(X , np.array([1,2])) th·ª±c hi·ªán ph√©p ma tr·∫≠n gi·ªØa X v√† [1,2]
		 
		-  Sau ƒë√≥ c·ªông th√™m 3 v√†o t·ª´ng ph·∫ßn t·ª≠, k·∫øt qu·∫£ l√† : 
					y = [6,8,10,13]
		 
	
		- model   = LinearRegression()  : Kh·ªüi t·∫°o m·ªôt m√¥ h√¨nh h·ªìi quy tuy·∫øn t√≠nh 
		
		- model.fit(X , y): Hu·∫•n luy·ªán m√¥ h√¨nh tr√™n d·ªØ li·ªáu X v√† y. 
			+ M√¥ h√¨nh h·ªçc c√°ch t√¨m ra c√°c tham s·ªë b0(intercept) v√† b1,b2(c√°c h·ªá s·ªë h·ªìi quy sao cho):  y = b0 + b1.x1 + b2.x2 
			+ Trong tr∆∞·ªùng h·ª£p n√†y, m√¥ h√¨nh s·∫Ω t√¨m ra b0 = 3 , b1 = 1 v√† b2 = 2  
			
		- y_pred = model.predict(X) : T√≠nh to√°n gi√° tr·ªã y d·ª± ƒëo√°n(gi√° tr·ªã m√¥ h√¨nh ∆∞·ªõc l∆∞·ª£ng) t·ª´ d·ªØ li·ªáu ƒë·∫ßu v√†o X 
			+ K·∫øt qu·∫£ : Ypred = [6,8,10,13]
		
		- M·∫∑t ph·∫≥ng h·ªìi quy cho th·∫•y m√¥ h√¨nh c·ªßa b·∫°n ƒë√£ h·ªçc ƒë∆∞·ª£t quy lu·∫≠t : Khi x1 ho·∫∑c x2 tƒÉng, y c≈©ng tƒÉng theo m·ªôt t·ª∑ l·ªá nh·∫•t ƒë·ªãnh. 

#. 5 H·ªìi quy tuy·∫øn t√≠nh s·ª≠ d·ª•ng PyTorch 
	-> N·∫øu b·∫°n mu·ªën h·ªìi quy tuy·∫øn t√≠nh trong m√¥i tr∆∞·ªùng h·ªçc m√°y n√¢ng cao, b·∫°n c√≥ th·ªÉ s·ª≠ d·ª•ng PyTorch 
	
#5.1 H·ªìi quy tuy·∫øn t√≠nh ƒë∆°n gi·∫£n v·ªõi PyTorch 
	
		import torch
		import torch.nn as nn
		import torch.optim as optim
		import matplotlib.pyplot as plt
		
		# D·ªØ li·ªáu m·∫´u
		X = torch.tensor([[1.0], [2.0], [3.0], [4.0]])
		y = torch.tensor([[2.0], [4.0], [6.0], [8.0]])
		
		# M√¥ h√¨nh h·ªìi quy tuy·∫øn t√≠nh
		model = nn.Linear(1, 1)  # 1 ƒë·∫ßu v√†o, 1 ƒë·∫ßu ra
		
		# H√†m m·∫•t m√°t v√† t·ªëi ∆∞u h√≥a
		criterion = nn.MSELoss()  # Sai s·ªë b√¨nh ph∆∞∆°ng trung b√¨nh
		optimizer = optim.SGD(model.parameters(), lr=0.01)  # Gradient Descent
		
		# Hu·∫•n luy·ªán m√¥ h√¨nh
		losses = []  # Danh s√°ch l∆∞u gi√° tr·ªã loss ƒë·ªÉ v·∫Ω bi·ªÉu ƒë·ªì
		
		for epoch in range(1000):  # Hu·∫•n luy·ªán trong 1000 epoch
			model.train()
		
			# D·ª± ƒëo√°n
			y_pred = model(X)
		
			# T√≠nh to√°n m·∫•t m√°t
			loss = criterion(y_pred, y)
			losses.append(loss.item())  # L∆∞u gi√° tr·ªã loss
		
			# T·ªëi ∆∞u h√≥a
			optimizer.zero_grad()  # X√≥a gradient c≈©
			loss.backward()  # T√≠nh to√°n gradient
			optimizer.step()  # C·∫≠p nh·∫≠t tr·ªçng s·ªë
		
			# In th√¥ng tin m·ªói 100 epoch
			if (epoch + 1) % 100 == 0:
				print(f'Epoch: {epoch + 1}, Loss: {loss.item()}')
		
		# H·ªá s·ªë h·ªìi quy v√† intercept
		print(f'H·ªá s·ªë h·ªìi quy (slope): {model.weight.item()}')
		print(f'Intercept (ƒë·ªô d·ªùi): {model.bias.item()}')
		
		# Tr·ª±c quan h√≥a d·ªØ li·ªáu v√† ƒë∆∞·ªùng h·ªìi quy
		plt.figure(figsize=(10, 5))
		
		# V·∫Ω d·ªØ li·ªáu g·ªëc
		plt.scatter(X.numpy(), y.numpy(), color='blue', label='D·ªØ li·ªáu g·ªëc')
		
		# V·∫Ω ƒë∆∞·ªùng h·ªìi quy
		x_line = torch.linspace(0, 5, 100).reshape(-1, 1)  # T·∫°o c√°c ƒëi·ªÉm x
		y_line = model(x_line).detach().numpy()  # D·ª± ƒëo√°n gi√° tr·ªã y
		plt.plot(x_line.numpy(), y_line, color='red', label='ƒê∆∞·ªùng h·ªìi quy')
		
		plt.title('H·ªìi quy tuy·∫øn t√≠nh ƒë∆°n gi·∫£n')
		plt.xlabel('X')
		plt.ylabel('y')
		plt.legend()
		plt.grid(True)
		plt.show()
		
		# V·∫Ω bi·ªÉu ƒë·ªì m·∫•t m√°t (loss) theo epoch
		plt.figure(figsize=(10, 5))
		plt.plot(range(1, 1001), losses, label='Loss')
		plt.title('Gi√° tr·ªã Loss theo Epoch')
		plt.xlabel('Epoch')
		plt.ylabel('Loss')
		plt.grid(True)
		plt.legend()
		plt.show()


	

	# Gi·∫£i th√≠ch code 
	1. D·ªØ li·ªáu m·∫´u : 
		T·∫°o m·ªôt t·∫≠p d·ªØ li·ªáu ƒë·∫ßu v√†o l√† c√°c m·∫£ng ƒëa chi·ªÅu(tensor) m√¥ ph·ªèng m·ªëi quan h·ªá tuy·∫øn t√≠nh y = 2x 
		
	2. S·ª≠ d·ª•ng m√¥ h√¨nh h·ªìi quy tuy·∫øn t√≠nh : 
		S·ª≠ d·ª•ng nn.Linear(1,1) ƒë·ªÉ t·∫°o m·ªôt m√¥ h√¨nh h·ªìi quy tuy·∫øn t√≠nh ƒë∆°n gi·∫£n v·ªõi : 
			1 ƒë·∫ßu v√†o(input feature)
			1 ƒë·∫ßu ra (output feature)
	3. H√†m m·∫•t m√°t : 
		nn.MSELoss() d√πng ƒë·ªÉ t√≠nh to√°n sai s·ªë b√¨nh ph∆∞∆°ng tr√¨nh b√¨nh MSE gi·ªØa gi√° tr·ªã d·ª± ƒëo√°n v√† th·ª±c t·∫ø. 
	
	4. Thu·∫≠t to√°n t·ªëi ∆∞u : 
		optim.SGD(Stochastic Gradient Descent ) ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ c·∫≠p nh·∫≠t tr·ªçng s·ªë d·ª±a tr√™n gradient 
		
	5. V√≤ng l·∫∑p hu·∫•n luy·ªán : 
		
		### Trong m·ªói epoch 
			- D·ª± ƒëo√°n gi√° tr·ªã y_pred = model(X).
			- T√≠nh to√°n m·∫•t m√°t loss = criterion(y_pred , y)
			- T·ªëi ∆∞u h√≥a tr·ªçng s·ªë 
							optimizer.zero_grad()     //  X√≥a gradient c≈© 
							loss.backward()           // T√≠nh gradient m·ªõi 
							optimizer.step()          // C·∫≠p nh·∫≠t tr·ªçng s·ªë 
			
	6. K·∫øt qu·∫£ : 
		Sau khi hu·∫•n luy·ªán m√¥ h√¨nh s·∫Ω tr·∫£ v·ªÅ h·ªá s·ªë h·ªìi quy(slope) v√† intercept ƒë·ªô d·ªùi: 


# 5.1 H·ªìi quy tuy·∫øn t√≠nh ƒëa bi·∫øn v·ªõi PyTorch
	
			import torch 
			import torch.nn as nn
			import torch.optim as optim 
			import matplotlib.pyplot as plt 
			
			#D·ªØ li·ªáu m·∫´u 
			# Gi·∫£ s·ª≠ c√≥ 2 bi·∫øn ƒë·ªôc l·∫≠p v√† m·ªôt bi·∫øn ph·ª• thuojc 
			X = torch.tensor([ [1.0 , 2.0] , [2.0,3.0] , [3.0,4.0] , [4.0,5.0] ]) 
			y = torch.tensor( [ [3.0] , [5.0] , [7.0] , [9.0] ])
			
			# M√¥ h√¨nh h·ªìi quy tuy·∫øn t√≠nh ƒëa bi·∫øn 
			model = nn.Linear(2,1)
			
			# H√†m m·∫•t m√°t v√† t·ªëi ∆∞u h√≥a 
			criterion = nn.MSELoss()
			optimizer = optim.SGD(model.parameters() , lr = 0.01)
			
			#L∆∞u tr·ªØ gi√° tr·ªã m·∫•t m√°t ƒë·ªÉ v·∫Ω ƒë·ªì th·ªã 
			losses = []
			
			#Hu·∫•n luy·ªán m√¥ h√¨nh 
			for epoch in range(1000):
				model.train()
			
				#D·ª± ƒëo√°n 
				y_pred = model(X)
				#T√≠nh to√°n m·∫•t m√°t 
				loss = criterion(y_pred , y)
				losses.append(loss.item())
				# T·ªëi ∆∞u h√≥a
				optimizer.zero_grad()
				loss.backward()
				optimizer.step()
			
				if(epoch + 1) % 100 == 0:
					print(f'Epoch {epoch+1}, Loss: {loss.item()}')
			
			#H·ªá s·ªë h·ªìi quy v√† intercept
			print(f'H·ªá s·ªë h·ªìi quy: {model.weight.data}')
			print(f'Intercept: {model.bias.data}')
			
			# V·∫Ω ƒë·ªì th·ªã m·∫•t m√°t qua c√°c epoch (log scale cho tr·ª•c Y)
			plt.plot(losses)
			plt.title('Qu√° tr√¨nh gi·∫£m m·∫•t m√°t (Log Scale)')
			plt.xlabel('Epoch')
			plt.ylabel('Loss')
			plt.yscale('log')  # Chuy·ªÉn tr·ª•c Y sang d·∫°ng logarit
			plt.grid(True, which="both", linestyle="--", linewidth=0.5)
			plt.show()
			#Ki·ªÉm tra d·ª± ƒëo√°n 
			with torch.no_grad():
				y_test = model(X)
				print(f'D·ª± ƒëo√°n: {y_test}')


		### Gi·∫£i th√≠ch : 
			- S·ª≠ d·ª•ng torch.tensor ƒë·ªÉ t·∫°o d·ªØ li·ªáu ƒë·∫ßu v√†o X v√† ƒë·∫ßu ra y 
			- M√¥ h√¨nh h·ªìi quy tuy·∫øn t√≠nh ƒëa bi·∫øn ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a v·ªõi 2 bi·∫øn ƒë·∫ßu v√†o v√† 1 bi·∫øn ƒë·∫ßu ra b·∫±ng c√°ch s·ª≠ d·ª•ng l·ªõp nn.Linear 
			- Ch√∫ng ta s·ª≠ d·ª•ng h√†m m·∫•t m√°t nn.MSELoss ƒë·ªÉ t√≠nh to√°n sai s·ªë b√¨nh ph∆∞∆°ng trung b√¨nh v√† t·ªëi ∆∞u h√≥a m√¥ h√¨nh b·∫±ng optim.SGD 
			- Ch√∫ng ta hu·∫•n luy·ªán m√¥ h√¨nh qua nhi·ªÅu epoch v√† in ra c√°c h·ªá s·ªë h·ªìi quy v√† intercept sau khi hu·∫•n luy·ªán 


#6. ·ª®ng d·ª•ng c·ªßa h·ªìi quy tuy·∫øn t√≠nh trong h·ªçc m√°y 
H·ªìi quy tuy·∫øn t√≠nh c√≥ r·∫•t nhi·ªÅu ·ª©ng d·ª•ng trong c√°c lƒ©nh v·ª±c kh√°c nhau, t·ª´ kinh t·∫ø, t√†i ch√≠nh ƒë·∫øn y h·ªçc, k·ªπ thu·∫≠t v√† h∆°n th·∫ø n·ªØa. D∆∞·ªõi ƒë√¢y l√† m·ªôt s·ªë v√≠ d·ª• ƒëi·ªÉn h√¨nh : 


	##6.1 D·ª± ƒëo√°n gi√° nh√† 
		H·ªìi quy tuy·∫øn t√≠nh c√≥ th·ªÉ s·ª≠ d·ª•ng ƒë·ªÉ d·ª± ƒëo√°n gi√° nh√† d·ª±a tr√™n c√°c y·∫øu t·ªë nh∆∞ di·ªán t√≠ch, s·ªë ph√≤ng ng·ªß,v√† v·ªã tr√≠. B·∫°n c√≥ th·ªÉ x√¢y d·ª±ng m√¥ h√¨nh h·ªìi quy tuy·∫øn t√≠nh ƒë·ªÉ d·ª± ƒëo√°n gi√° nh√† d·ª±a tr√™n c√°c ƒë·∫∑c ƒëi·ªÉm n√†y 
		
	## 6.2  Ph√¢n t√≠ch m·ªói quan h·ªá gi·ªØa c√°c bi·∫øn kinh t·∫ø 
		Trong kinh t·∫ø h·ªçc, h·ªìi quy tuy·∫øn t√≠nh c√≥ th·ªÉ ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ ph√¢n t√≠ch m·ªëi quan h·ªá gi·ªØa c√°c ch·ªâ s·ªë kinh t·∫ø nh∆∞ GDP, t·ª∑ l·ªá th·∫•t nghi·ªáp v√† l·∫°m ph√°t. V√≠ d·ª•, b·∫°n c√≥ th·ªÉ nghi√™n c·ª©u ·∫£nh h∆∞·ªüng c·ªßa t·ª∑ l·ªá th·∫•t nghi·ªáp ƒë·∫øn tƒÉng tr∆∞·ªüng GDP 
		
	## 6.3 D·ª± ƒëo√°n nguy c∆° b·ªánh t·∫≠t 
		H·ªìi quy tuy·∫øn t√≠nh c≈©ng c√≥ th·ªÉ ƒë∆∞·ª£c s·ª≠ d·ª•ng trong y h·ªçc ƒë·ªÉ d·ª± ƒëo√°n nguy c∆° m·∫Øc b·ªánh d·ª±a tr√™n c√°c ch·ªâ s·ªë s·ª©c kh·ªèe. V√≠ d·ª•, m√¥ h√¨nh h·ªìi quy tuy·∫øn t√≠nh c√≥ th·ªÉ ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ ph√¢n t√≠ch m·ªëi quan h·ªá gi·ªØa huy·∫øt √°p v√† nguy c∆° m·∫Øc b·ªánh tim m·∫°ch 
		

# K·∫øt lu·∫≠n  
	H·ªìi quy tuy·∫øn t√≠nh l√† m·ªôt k·ªπ thu·∫≠t c∆° b·∫£n nh∆∞ng c·ª±c k·ª≥ m·∫°nh m·∫Ω trong ph√¢n t√≠ch d·ªØ li·ªáu v√† h·ªçc m√°y. N√≥ cung c·∫•p m·ªôt n·ªÅn t·∫£ng quan tr·ªçng cho c√°c m√¥ h√¨nh ph·ª©c t·∫°p h∆°n v√† c√≥ nhi·ªÅu ·ª©ng d·ª•ng th·ª±c ti·ªÖn trong c√°c lƒ©nh v·ª±c kh√°c nhau. Vi·ªác hi·ªÉu r√µ c√°ch th·ª©c ho·∫°t ƒë·ªông v√† ·ª©ng d·ª•ng c·ªßa h·ªìi quy tuy·∫øn t√≠nh kh√¥ng ch·ªâ gi√∫p b·∫°n ph√¢n t√≠ch d·ªØ li·ªáu t·ªët h∆°n m√† c√≤n m·ªü ra nhi·ªÅu c∆° h·ªôi trong ph√°t tri·ªÉn c√°c m√¥ h√¨nh h·ªçc m√°y hi·ªáu qu·∫£. 




</pre><a id='backBottom' href='../AI-learning-list.html' style='display:none;'>üîô Quay l·∫°i danh s√°ch</a><br><button onclick='toggleTheme()'>üåô Chuy·ªÉn giao di·ªán</button></div><script>function toggleTheme() {   let mode = document.body.classList.contains('dark-mode') ? 'light-mode' : 'dark-mode';   document.body.className = mode; localStorage.setItem('theme', mode);   syncTheme();}function applyTheme() {   let savedTheme = localStorage.getItem('theme') || 'dark-mode';   document.body.className = savedTheme;   syncTheme();}function syncTheme() {   let preElement = document.querySelector('pre');   if (document.body.classList.contains('dark-mode')) { preElement.style.background = '#1e1e1e'; preElement.style.color = '#e0e0e0'; }   else { preElement.style.background = '#f5f5f5'; preElement.style.color = '#333333'; }}function checkPageHeight() {   let contentHeight = document.body.scrollHeight;   let windowHeight = window.innerHeight;   if (contentHeight > windowHeight * 1.2) {       document.getElementById('backBottom').style.display = 'block';   } else {       document.getElementById('backBottom').style.display = 'none';   }}</script></body></html>