<html><head><title>Untitled</title><style>body { font-family: Arial, sans-serif; transition: background 0.3s, color 0.3s; }.dark-mode { background-color: #121212; color: #e0e0e0; }.light-mode { background-color: #ffffff; color: #333333; }h1 { text-align: center; color: #73d9f5; }pre { padding: 15px; border-radius: 5px;       white-space: pre-wrap; word-wrap: break-word;       overflow-x: auto; max-width: 100%;       transition: background 0.3s, color 0.3s; }.dark-mode pre { background: #1e1e1e; color: #e0e0e0; }.light-mode pre { background: #f5f5f5; color: #333333; }#backTop, #backBottom {    font-size: 2em; padding: 20px 40px;    background: #bb86fc; color: white; text-decoration: none;    border-radius: 10px; display: inline-block; text-align: center; }#backTop:hover, #backBottom:hover { background: #9b67e2; }button { font-size: 1.5em; padding: 15px 30px;    background: #03dac6; color: #121212; border: none;    cursor: pointer; border-radius: 5px; display: block; margin: 10px auto; }button:hover { background: #02b8a3; }.dark-mode a { color: #03dac6; } .light-mode a { color: #007bff; }</style></head><body onload='applyTheme(); checkPageHeight()'><div class='container'><a id='backTop' href='../AI-math-learning-list.html'>üîô Quay l·∫°i danh s√°ch</a><br><h1>Untitled</h1><pre>//==========Lesson 1 == Lession 1 == Lession 1 == C√°c kh√°i ni·ªám to√°n h·ªçc ƒë∆∞·ª£c s·ª≠ d·ª•ng ==========//

# 1 ====== MSE (Mean Squared Error ) Sai s·ªë b√¨nh ph∆∞∆°ng trung b√¨nh 
 ## Kh√°i ni·ªám :
	Sai s·ªë b√¨nh ph∆∞∆°ng trung b√¨nh(MSE) l√† m·ªôt c√°ch ƒë·ªÉ ƒëo xem d·ª± ƒëo√°n c·ªßa m√¥ h√¨nh l·ªách so v·ªõi th·ª±c t·∫ø l√† bao nhi√™u :
		### C√°ch t√≠nh 
			V·ªõi m·ªói c·∫∑p gi√° tr·ªã d·ª± ƒëo√°n v√† gi√° tr·ªã th·ª±c, ta t√≠nh ƒë·ªô l·ªách sai s·ªë 
						Sai s·ªë = d·ª± ƒëo√°n  - th·ª±c t·∫ø 
			L·∫•y b√¨nh ph∆∞∆°ng ƒë·ªô l·ªách n√†y ƒë·ªÉ ƒë·∫£m b·∫£o sai s·ªë lu√¥n d∆∞∆°ng 
			
			Cu·ªëi c√πng t√≠nh trung b√¨nh c·ªßa t·∫•t c·∫£ c√°c sai s·ªë b√¨nh ph∆∞∆°ng 
			
			
 ## √ù nghƒ©a : 
	MSE cho ta bi·∫øt trung b√¨nh sai s·ªë c·ªßa m√¥ h√¨nh l√† bao nhi√™u. 
		+ MSE c√†ng nh·ªè th√¨ m√¥ h√¨nh c√†ng ch√≠nh x√°c 
		+ N·∫øu MSE l·ªõn, nghƒ©a l√† m√¥ h√¨nh d·ª± ƒëo√°n r·∫•t kh√°c so v·ªõi th·ª±c t·∫ø . 
		
# 2. ===== SGD (Stochastic Gradient Descent )
 ## Kh√°i ni·ªám : -
	SGD l√† m·ªôt c√°ch ƒë·ªÉ h·ªçc v√† c·∫£i thi·ªán m√¥ h√¨nh AI, b·∫±ng c√°ch li√™n t·ª•c ƒëi·ªÅu ch·ªânh c√°c th√¥ng s·ªë c·ªßa n√≥(tr·ªçng s·ªë)
		
		### H√¨nh dung : 
			T∆∞·ªüng t∆∞·ª£ng b·∫°n ƒëang leo xu·ªëng m·ªôt ng·ªçn n√∫i ƒë·ªÉ t√¨m ƒëi·ªÉm th·∫•p nh·∫•t. B·∫°n kh√¥ng bi·∫øt r√µ b·∫£n ƒë·ªì, nh∆∞ng b·∫°n c√≥ th·ªÉ ∆∞·ªõc l∆∞·ª£ng h∆∞·ªõng n√†o l√† d·ªëc nh·∫•t ƒë·ªÉ ƒëi xu·ªëng 
				+ M√¥ h√¨nh AI gi·ªëng nh∆∞ ng·ªçn n√∫i 
				+ ƒêi·ªÉm th·∫•p nh·∫•t(ƒë·ªânh thung l≈©ng) l√† n∆°i m√† sai s·ªë MSE nh·ªè nh·∫•t 
				+ SGD gi·ªëng nh∆∞ b·∫°n ƒëi t·ª´ng b∆∞·ªõc nh·ªè xu·ªëng n√∫i, d·ª±a v√†o h∆∞·ªõng d·ªëc . 
				
		### Qu√° tr√¨nh : 
			1. B·∫Øt ƒë·∫ßu : M√¥ h√¨nh AI d·ª± ƒëo√°n ng·∫´u nhi√™n(gi·ªëng nh∆∞ b·∫°n ƒë·ª©ng tr√™n n√∫i)
			2. T√≠nh sai s·ªë : Xem d·ª± ƒëo√°n kh√°c th·ª±c t·∫ø bao nhi√™u(d·ª±a tr√™n MSE) 
			3. ƒêi·ªÅu ch·ªânh : SGD t√≠nh to√°n h∆∞·ªõng v√† thay ƒë·ªïi tr·ªçng s·ªë ƒë·ªÉ gi·∫£m sai s·ªë. 
			4. L·∫∑p l·∫°i: Qu√° tr√¨nh n√†y ƒë∆∞·ª£c l·∫∑p l·∫°i cho ƒë·∫øn khi sai s·ªë ƒë·∫°t m·ª©c nh·ªè nh·∫•t. 
			
	T·∫°i sao g·ªçi l√† Stochastic(ng·∫´u nhi√™n)? 
		Thay v√¨ t√≠nh to√°n tr√™n to√†n b·ªô d·ªØ li·ªáu, SGD ch·ªâ ch·ªçn ng·∫´u nhi√™n m·ªôt v√†i m·∫´u d·ªØ li·ªáu ƒë·ªÉ h·ªçc, gi√∫p qu√° tr√¨nh nhanh h∆°n  
		
		
# 3. ====== Slope(H·ªá s·ªë h·ªìi quy) v√† Intercept(ƒê·ªô d·ªùi )
	## Kh√°i ni·ªám : 
		H·ªá s·ªë h·ªìi quy v√† ƒë·ªô d·ªùi l√† hai th√†nh ph·∫ßn ch√≠nh c·ªßa ph∆∞∆°ng tr√¨nh ƒë∆∞·ªùng th·∫≥ng  
		
	   ### C√¥ng th·ª©c : 
		M·ªôt ƒë∆∞·ªùng th·∫≥ng trong kh√¥ng gian 2D ƒë∆∞·ª£c bi·ªÉu di·ªÖn  
					y = mx + b 
				m : l√† h·ªá s·ªë h·ªìi quy(slope) : N√≥ cho bi·∫øt ƒë·ªô d·ªëc c·ªßa ƒë∆∞·ªùng th·∫≥ng 
				b : Intercept(ƒë·ªô d·ªùi) : N√≥ l√† ƒëi·ªÉm m√† ƒë∆∞·ªùng th·∫≥ng c·∫Øt tr·ª•c y khi x = 0 
				
	## √ù nghƒ©a : 
		H·ªá s·ªë h·ªìi quy(m) :
			- Cho bi·∫øt m·ªëi quan h·ªá gi·ªØa x v√† y 
				+ N·∫øu m > 0 : Khi x tƒÉng, y c≈©ng tƒÉng  
				+ N·∫øu m < 0 : Khi x tƒÉng, y gi·∫£m 
				+ Gi√° tr·ªã l·ªõn c·ªßa m nghƒ©a l√† y thay ƒë·ªïi nhanh khi x thay ƒë·ªïi  
		Intercept(b):
			- L√† ƒëi·ªÉm kh·ªüi ƒë·∫ßu c·ªßa ƒë∆∞·ªùng th·∫≥ng 
				+ n·∫øu b  = 2 : Khi x = 0 th√¨ y = 2 
				
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
//==========Lesson 2 == Lession 2 == Lession 2 == Vector ==========//
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////


	# Kh√°i ni·ªám : Vector l√† m·ªôt ƒë·ªëi t∆∞·ª£ng to√°n h·ªçc c√≥ ƒë·ªô l·ªõn v√† h∆∞·ªõng. Trong ƒë·∫°i s·ªë tuy·∫øn t√≠nh, vector ƒë∆∞·ª£c bi·ªÉu di·ªÖn d∆∞·ªõi d·∫°ng danh s√°ch c√°c s·ªë, th∆∞·ªùng trong m·ªôt c·ªôt ho·∫∑c h√†ng  (ƒë∆∞·ª£c g·ªçi l√† th√†nh ph·∫ßn c·ªßa vector )
	
		V√≠ d·ª• trong kh√¥ng gian 2 chi·ªÅu , vector v = (3,4) c√≥ th√†nh ph·∫ßn x = 3 v√† y  = 4

	# ·ª®ng d·ª•ng : 
		M√¥ t·∫£ d·ªØ li·ªáu trong AI(v√≠ d·ª• m·ªôt h√¨nh ·∫£nh c√≥ th·ªÉ l√† m·ªôt vector c√°c gi√° tr·ªã m√†u s·∫Øc ) 
		ƒê·∫°i di·ªán cho ƒëi·ªÉm ho·∫∑c h∆∞·ªõng trong kh√¥ng gian 
				
//--------------------------------------- Kh√¥ng gian vector(Vector Space)
# Kh√°i ni·ªám : T·∫≠p h·ª£p c√°c vector c√≥ th·ªÉ ƒë∆∞·ª£c c·ªông l·∫°i v·ªõi nhau ho·∫∑c nh√¢n v·ªõi m·ªôt s·ªë(scaling) m√† k·∫øt qu·∫£ v·∫´n n·∫±m trong t·∫≠p ƒë√≥ , v·∫´n n·∫±m trong kh√¥ng gian ƒë√≥ 
		V√≠ d·ª• : t·∫≠p t·∫•t c·∫£ c√°c vector [x,y] v·ªõi x,y l√† s·ªë th·ª±c 
		
		## M·ªôt kh√¥ng gian vector c·∫ßn th·ªèa m√£n c√°c ƒëi·ªÅu ki·ªán sau  
			### C·ªông vector : N·∫øu v , w l√† hai vector trong kh√¥ng gian vector, th√¨ v + w c≈©ng ph·∫£i thu·ªôc kh√¥ng gian vector ƒë√≥  
			
			### Nh√¢n vector v·ªõi scalar : N·∫øu v l√† m·ªôt vector v√† c l√† m·ªôt s·ªë(scalar) th√¨ c x v c≈©ng ph·∫£i thu·ªôc kh√¥ng gian vector ƒë√≥ 
			
			### C√°c t√≠nh ch·∫•t kh√°c :  C√°c ph√©p to√°n c·ªông vector v√† nh√¢n v·ªõi scalar ph·∫£i th·ªèa m√£n m·ªôt s·ªë t√≠nh ch·∫•t nh∆∞ k·∫øt h·ª£p, ph√¢n ph·ªëi, c√≥ ph·∫ßn t·ª≠ kh√¥ng thay ƒë·ªïi(vector 0)
		
		
		## Trong m·ªói h∆∞·ªõng c√≥ t·ªìn t·∫°i v√¥ h·∫°n vector v√¨ th·∫ø ch√∫ng ta c√≥ th·ªÉ nh√¢n ch√∫ng v·ªõi b·∫•t k·ª≥ s·ªë n√†o c≈©ng ra h∆∞·ªõng ƒë√≥ 
		
		
		
# ·ª®ng d·ª•ng 
		+ L√† c∆° s·ªü ƒë·ªÉ hi·ªÉu c√°c kh√°i ni·ªám nh∆∞ h·ªá ph∆∞∆°ng tr√¨nh tuy·∫øn t√≠nh ho·∫∑c h·ªçc m√°y 
		
		

# V√≠ d·ª• v·ªÅ kh√¥ng gian vector 2 chi·ªÅu v√† 3 chi·ªÅu : 
	## 1. Kh√¥ng gian vector 2 chi·ªÅu : 
		+> T·∫≠p h·ª£p t·∫•t c·∫£ c√°c vector c√≥ d·∫°ng(x,y) l√† kh√¥ng gian vector 2 chi·ªÅu trong m·∫∑t ph·∫≥ng()
							V√≠ d·ª• : 
								(3,4) , (-1,2) , (0,0)
		
		+> C√°c vector n√†y c√≥ th·ªÉ c·ªông l·∫°i v√† nh√¢n v·ªõi scalar 

	## 2 Kh√¥ng gian vector 3 chi·ªÅu :
		+> T·∫≠p h·ª£p t·∫•t c·∫£ c√°c vector c√≥ d·∫°ng (x,y,z) l√† kh√¥ng gian vector 3 chi·ªÅu(trong kh√¥ng gian 3 chi·ªÅu )
							V√≠ d·ª•: 
								
								(1,2,3) , (4,-5,6)
		
		+> T∆∞∆°ng t·ª± nh∆∞ kh√¥ng gian 2 chi·ªÅu, b·∫°n c√≥ th·ªÉ th·ª±c hi·ªán c·ªông v√† nh√¢n vector v·ªõi scalar
				


# C√°c ph√©p to√°n c∆° b·∫£n v·ªÅ kh√¥ng gian vector 	
	## C·ªông vector 
		Khi c·ªông hai vector, b·∫°n c·ªông t·ª´ng th√†nh ph·∫ßn t∆∞∆°ng ·ª©ng c·ªßa ch√∫ng 
		
				v√≠ d·ª• : 
				v = (2,3) , w = (4 , -1) 
				v + w = ((2+4) , (3+(-1)) ) = (6,2)
				
	## Nh√¢n vector v·ªõi scalar 
		Nh√¢n v·ªõi Scalar t·ª©c l√† ta s·∫Ω nh√¢n t·ª´ng th√†nh ph·∫ßn c·ªßa vector ƒë√≥ v·ªõi scalar 
			V√≠ d·ª•  
				v = (2,3)  Scalar = 3 
				v . 3 = 3 . (2,3) = (6,9)
				
	
	## ƒê·ªô d√†i c·ªßa vector (Norm of a vector )
		ƒê·ªô d√†i(hay c√≤n g·ªçi l√† norm)  c·ªßa m·ªôt vector v = (x , y) l√† cƒÉn b·∫≠c 2 c·ªßa t·ªïng b√¨nh ph∆∞∆°ng c√°c th√†nh ph·∫ßn 
			||v|| = sqrt(x^2 + y^2)		
			
		V√≠ d·ª• : 
			v = (3,4)  =>  ||v|| = 5


//---------------------------- T√≠ch v√¥ h∆∞·ªõng(Dot product)		
	# Kh√°i ni·ªám : l√† m·ªôt ph√©p to√°n gi·ªØa 2 vector, cho k·∫øt qu·∫£ l√† m·ªôt s·ªë (scalar)
		C√¥ng th·ª©c : u.v = u1v1 + u2v2 + ... + unvn
		
	# ·ª®ng d·ª•ng :
		T√≠nh ƒë·ªô t∆∞∆°ng t·ª± gi·ªØa 2 vector(v√≠ d·ª• trong t√¨m ki·∫øm vƒÉn b·∫£n )
		T√≠nh g√≥c gi·ªØa 2 vector 
		
		
	# C√¥ng th·ª©c 
		 vector A.B = |A|.|B|.cos0  
			- |A| l√† ƒë·ªô d√†i c·ªßa vector A 
			- cos0 : gi√° tr·ªã cosine c·ªßa g√≥c 0 gi·ªØa 2 vector 
			
			-> Khi 	cos0 = 1 	: Hai vector c√πng h∆∞·ªõng 
					cos0 = 0 	: Hai vector vu√¥ng g√≥c 
					cos0 = -1 	: Hai vector ng∆∞·ª£c h∆∞·ªõng  
					

		V√≠ d·ª• : 
			v = (1,2) , w = (3,4) 
			v .  w = 1x3 + 2x4 = 3+8 = 11
			
		V√≠ d·ª• g√≥c gi·ªØa 2 vector 	 A (1,0) , B = (0,1)
				
				T√≠ch v√¥ h∆∞·ªõng :
					A . B = 1.0 + 0 . 1 = 0 
					T√≠ch v√¥ h∆∞·ªõng b·∫±ng 0 nghƒ©a l√† g√≥c gi·ªØa 2 vector vu√¥ng g√≥c 
		
	# ·ª®ng d·ª•ng t√≠ch v√¥ h∆∞·ªõng trong h·ªçc m√°y AI 
		Trong machine learning, t√≠ch v√¥ h∆∞·ªõng th∆∞·ªùng ƒë∆∞·ª£c d√πng ƒë·ªÉ : 
			- T√≠nh ƒë·ªô d√†i gi·ªØa 2 vector
			
			## V√≠ d·ª• : 
				N·∫øu hai vector ƒë·∫°i di·ªán cho 2 vƒÉn b·∫£n, t√≠ch v√¥ h∆∞·ªõng s·∫Ω gi√∫p cho bi·∫øt li·ªáu ch√∫ng c√≥ li√™n quan hay kh√¥ng 
				A = (1,2,3) , B = (4,5,6)
				
				A.B  = 1.4 + 2.5 + 3.6 = 32   // N·∫øu t√≠ch v√¥ h∆∞·ªõng l·ªõn, ƒëi·ªÅu ƒë√≥ c√≥ th·ªÉ cho th·∫•y hai vector r·∫•t gi·ªëng nhau .
				

//--------------------  C√°c kh√°i ni·ªám kh√¥ng gian vector ph·ª©c t·∫°p h∆°n  : 

# ƒê·ªãnh nghƒ©a kh√¥ng gian vector ph·ª©c 
	Kh√¥ng gian vector ph·ª©c(C^n) l√† m·ªôt t·∫≠p h·ª£p c√°c vector, trong ƒë√≥: 
		- C√°c ph·∫ßn t·ª≠ (th√†nh ph·∫ßn) c·ªßa vector thu·ªôc tr∆∞·ªùng s·ªë ph·ª©c C
		- C√°c ph√©p c·ªông vector v√† nh√¢n v·ªõi s·ªë v√¥ h∆∞·ªõng (scalar) tu√¢n theo quy t·∫Øc s·ªë ph·ª©c  
		
	V√≠ d·ª• : M·ªôt kh√¥ng gian vector ph·ª©c trong C^2 g·ªìm c√°c vector d·∫°ng  
			v = (z1,z2) , v·ªõi z1, z2 thu·ªôc t·∫≠p C 
			
# Quy t·∫Øc c·ªßa kh√¥ng gian vector ph·ª©c 
	
	Gi·ªëng nh∆∞ kh√¥ng gian vector th·ª±c(R^n), kh√¥ng gian vector ph·ª©c c≈©ng th·ªèa m√£n c√°c ti·ªÅn ƒë·ªÅ c∆° b·∫£n 
		
	## Ph√©p c·ªông vector  
		ƒê√≥ng vai tr√≤ c·ªông t·ª´ng th√†nh ph·∫ßn  
			(1 + i , 2 - i) + (3 - i , 4 + i) =  (  (1+i) + (3 - i) , (2 - i) + (4+i)   ) = (4,6)		// C·ªông g·ªôp v√†o s·∫Ω lo·∫°i ƒë∆∞·ª£c i ra 
			
	## Ph√©p nh√¢n v·ªõi s·ªë v√¥ h∆∞·ªõng : Nh√¢n t·ª´ng th√†nh ph·∫ßn v·ªõi m·ªôt s·ªë ph·ª©c  
		
		V√≠ d·ª• : 
			(1 + i).(2 - i , -1 + i) = ( (1+i)(2-i) , (1+i)(-1+i) )

	## Tu√¢n th·ªß c√°c t√≠nh ch·∫•t giao ho√°n, ph√¢n ph·ªëi , k·∫øt h·ª£p 
	
		

#√ù nghƒ©a h√¨nh h·ªçc kh√¥ng gian vector ph·ª©c 
	- Trong kh√¥ng gian th·ª±c, vector ƒë∆∞·ª£c xem l√† m≈©i t√™n trong kh√¥ng gian n-chi·ªÅu 
	- Trong kh√¥ng gian ph·ª©c, m·ªói ph·∫ßn t·ª≠ l√† m·ªôt s·ªë ph·ª©c, t·ª©c l√† vector s·∫Ω c√≥ ph·∫ßn th·ª±c v√† ·∫£o. ƒêi·ªÅu n√†y gi√∫p bi·ªÉu di·ªÖn c√°c h·ªá th·ªëng ph·ª©c t·∫°p h∆°n, ch·∫≥ng h·∫°n trong ƒëi·ªán t·ª≠, v·∫≠t l√Ω l∆∞·ª£ng t·ª≠ ho·∫∑c h·ªçc m√°y. 
	
	V√≠ d·ª• :  M·ªôt vector trong C^3 c√≥ th·ªÉ ƒë∆∞·ª£c bi·ªÉu di·ªÖn nh∆∞: 
		
			v = (1 + i , 2 - i , -i )
			

# ƒê·ªô d√†i v√† t√≠ch v√¥ h∆∞·ªõng trong kh√¥ng gian vector ph·ª©c 	

	## ƒê·ªô d√†i (Norm)
		ƒê·ªô d√†i (norm) c·ªßa m·ªôt vector v = (z1, z2 , .... zn) ƒë∆∞·ª£c t√≠nh b·∫±ng 
			||v|| = sqrt(|z1|^2 + |z2|^2 + ... + |zn|^2)

	## T√≠ch v√¥ h∆∞·ªõng 
		T√≠ch v√¥ h∆∞·ªõng trong kh√¥ng gian vector ph·ª©c ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a kh√°c m·ªôt ch√∫t so v·ªõi kh√¥ng gian th·ª±c 
			<u,v> = _u1v1 + _u2v2 + ... + _unvn 
			
			_u1 l√† li√™n h·ª£p ph·ª©c c·ªßa u1 
			
		

# C√°c ·ª©ng d·ª•ng th·ª±c t·∫ø :
		1. X·ª≠ l√Ω t√≠n hi·ªáu v√† √¢m thanh: M√¥ h√¨nh h√≥a s√≥ng √¢m v√† t√≠n hi·ªáu b·∫±ng s·ªë ph·ª©c 
		2. V·∫≠t l√Ω l∆∞·ª£ng t·ª≠: Bi·ªÉu di·ªÖn tr·∫°ng th√°i l∆∞·ª£ng t·ª≠ b·∫±ng c√°c vector ph·ª©c 
		3. H·ªçc m√°y : S·ª≠ d·ª•ng kh√¥ng gian ph·ª©c trong m·∫°ng n∆°-ron ho·∫∑c d·ªØ li·ªáu c√≥ li√™n quan ƒë·∫øn t·∫ßn s·ªë 
		4. H·ªá th·ªëng ƒëi·ªán : D√πng ƒë·ªÉ ph·∫ßn t√≠ch ƒëi·ªán √°p v√† d√≤ng ƒëi·ªán d·∫°ng s√≥ng  


//------------------ Vector ƒë·∫∑c tr∆∞ng(Eigenvector) trong to√°n h·ªçc 

# Kh√°i ni·ªám Eigenvector (Vector ƒë·∫∑c tr∆∞ng trong to√°n h·ªçc )
	Vector ƒë·∫∑c tr∆∞ng l√† m·ªôt vector ƒë·∫∑c bi·ªát, khi b·∫°n nh√¢n n√≥ v·ªõi m·ªôt matrix, k·∫øt qu·∫£ s·∫Ω l√† vector ƒë√≥ nh√¢n v·ªõi m·ªôt s·ªë(scalar )
	
	C·ª• th·ªÉ, n·∫øu g·ªçi vector ƒë·∫∑c tr∆∞ng l√† v v√† s·ªë ƒë√≥ l√† ÂÖ•(lambda), th√¨: 
				A.v = ÂÖ•.v
				
			- A : l√† ma tr·∫≠n vu√¥ng(s·ªë h√†ng =  s·ªë c·ªôt )
			- v : l√† vector ƒë·∫∑c tr∆∞ng (eigenvector)
			- ÂÖ• : l√† gi√° tr·ªã ƒë·∫∑c tr∆∞ng(eigenvalue)

# Nghƒ©a l√† g√¨ trong th·ª±c t·∫ø 

	H√¨nh dung m·ªôt ma tr·∫≠n A l√† m·ªôt h·ªá th·ªëng n√†o ƒë√≥, c√≤n v l√† m·ªôt h∆∞·ªõng ƒë·∫∑c bi·ªát 
		- Khi b·∫°n √°p d·ª•ng ma tr·∫≠n A l√™n vector v, th√¨ vector v ch·ªâ b·ªã c√≥ d√£n ho·∫∑c ph√≥ng to theo h∆∞·ªõng c·ªßa n√≥(kh√¥ng thay ƒë·ªïi h∆∞·ªõng )
		- ÂÖ• cho bi·∫øt m·ª©c ƒë·ªô co d√£n ph√≥ng to 
		
	## V√≠ d·ª• d·ªÖ hi·ªÉu 
		B·∫°n v·∫Ω m·ªôt h√¨nh tr√≤n v√† k√©o d√£n n√≥ th√†nh h√¨nh elip b·∫±ng c√°ch √°p d·ª•ng ma tr·∫≠n A. Sau khi k√©o, m·ªôt v√†i vector ban ƒë·∫ßu kh√¥ng ƒë·ªïi h∆∞·ªõng, ch·ªâ thay ƒë·ªïi ƒë·ªô d√†i. Nh·ªØng vector n√†y ch√≠nh l√† vector ƒë·∫∑c tr∆∞ng, v√† m·ª©c ƒë·ªô k√©o d√£n/ ph√≥ng to c·ªßa ch√∫ng ƒë∆∞·ª£c x√°c ƒë·ªãnh b·ªüi ÂÖ• 
		
		
	## S·ªë l∆∞·ª£ng vector c√≥ th·ªÉ x·∫£y ra 
		Ph∆∞∆°ng tr√¨nh ƒë·∫∑c tr∆∞ng 
							det(A - ÂÖ•I) = 0 
			- L√† m·ªôt ph∆∞∆°ng tr√¨nh ƒëa th·ª©c, c·∫•p b·∫≠c c·ªßa n√≥ b·∫±ng v·ªõi k√≠ch th∆∞·ªõc ma tr·∫≠n A 
			- N·∫øu A l√† ma tr·∫≠n n x n, ph∆∞∆°ng tr√¨nh s·∫Ω c√≥ b·∫≠c n 
			- ƒêi·ªÅu n√†y nghƒ©a l√† b·∫°n c√≥ t·ªëi ƒëa n gi√° tr·ªã ÂÖ•(Eigenvalue) t∆∞∆°ng ·ª©ng v·ªõi n Eigenvector ƒë·ªôc l·∫≠p 
			
	

	## I l√† ma tr·∫≠n ƒë∆°n v·ªã(identity matrix) : Ma tr·∫≠n vu√¥ng m√† c√°c ph·∫ßn t·ª≠ l√† ƒë∆∞·ªùng ch√©o ch√≠nh l√† 1, c√°c ph·∫ßn c√≤n l·∫°i l√† 0 
			V√≠ d·ª• 	
					A =  [ [1,0,0] , [0,1,0] , [0,0,1]  ]
					
			M·ª•c ƒë√≠ch c·ªßa I l√† ƒë·∫£m b·∫£o r·∫±ng ÂÖ• c√≥ th·ªÉ ƒë∆∞·ª£c tr·ª´ tr·ª±c ti·∫øp kh·ªèi ma tr·∫≠n A, v√¨ b·∫°n kh√¥ng th·ªÉ tr·ª´ s·ªë ÂÖ• t·ª´ ma tr·∫≠n A m√† kh√¥ng nh√¢n n√≥ v·ªõi ma tr·∫≠n ƒë∆°n v·ªã , T·ª©c l√† n√≥ t·ªìn t·∫°i ƒë·ªÉ gi·ªØ cho ph√©p to√°n h·ª£p l·ªá v√¨ k th·ªÉ tr·ª´ m·ªôt ma tr·∫≠n cho m·ªôt bi·∫øn X ƒë∆∞·ª£c m√† ph·∫£i nh√¢n n√≥ v·ªõi m·ªôt ma tr·∫≠n ƒë∆°n v·ªã 

	
		
		
# ·ª®ng d·ª•ng v√†o th·ª±c t·∫ø : 
	Vector ƒë·∫∑c tr∆∞ng ƒë∆∞·ª£c d√πng r·ªông r√£i v√¨ t√≠nh ch·∫•t "Gi·ªØ h∆∞·ªõng" n√†y, gi√∫p ƒë∆°n gi·∫£n h√≥a nhi·ªÅu v·∫•n ƒë·ªÅ. M·ªôt s·ªë ·ª©ng d·ª•ng quan tr·ªçng: 
		- Ph√¢n t√≠ch d·ªØ li·ªáu (PCA) : Gi·∫£m s·ªë chi·ªÅu d·ªØ li·ªáu m√† v·∫´n gi·ªØ ƒë∆∞·ª£c th√¥ng tin quan tr·ªçng. 
		- Computer Vision : Ph√°t hi·ªán c√°c khu√¥n m·∫´u ch·ªß y·∫øu trong h√¨nh ·∫£nh 
		- Google Search(Page Rank) : X·∫øp h·∫°ng trang web b·∫±ng c√°ch t√≠nh c√°c vector ƒë·∫∑c tr∆∞ng 
		

# L√†m sao ƒë·ªÉ t√≠nh ƒë∆∞·ª£c Vector ƒë·∫∑c tr∆∞ng 
	ƒê·ªÉ t√¨m vector ƒë·∫∑c tr∆∞ng v v√† gi√° tr·ªã ƒë·∫∑c tr∆∞ng ÂÖ•, b·∫°n gi·∫£i ph∆∞∆°ng tr√¨nh 
		
			det(A - ÂÖ•I) = 0 
			
		// Trong ƒë√≥: 
			-  I : Ma tr·∫≠n ƒë∆°n v·ªã 
			-  det : ƒë·ªãnh th·ª©c c·ªßa ma tr·∫≠n  
		
		Gi·∫£i ph∆∞∆°ng tr√¨nh tr√™n, b·∫°n t√¨m ra ÂÖ•(gi√° tr·ªã ƒë·∫∑c tr∆∞ng). Sau ƒë√≥, thay t·ª´ng ÂÖ• v√†o ph∆∞∆°ng tr√¨nh 
							(A - ÂÖ•I)v = 0 
						ƒê·ªÉ t√¨m c√°c vector v t∆∞∆°ng ·ª©ng  
						


# V√≠ d·ª• s·ªë : 
	
	## Cho ma tr·∫≠n   
			A =  [ [2,1] , [1,2] ]
			
		1. T√¨m vector ƒë·∫∑c tr∆∞ng  
			
			det(A - ÂÖ•I) = det(  [2-ÂÖ• , 1] , [1 , 2-ÂÖ•]  )
			-> ÂÖ•^2 -4ÂÖ• + 3 = 0 
			-> ÂÖ•1 = 3 ,  ÂÖ•2 = 1  			// 2 nghi·ªám ph√¢n bi·ªát 
			
			

		2. T√¨m vector ƒë·∫∑c tr∆∞ng : 
			-  V·ªõi ÂÖ• = 3 : 
					(A - 3I)v = 0 => [ [-1 , 1] , [ 1 , -1 ] ] . [x,y] = 0
					=> v1 = [1,1]
					
			
			- V·ªõi ÂÖ• = 1 
					(A - I) = 0 => [[1,1] , [1,1]] . [x,y] = 0 
					=> v2 = [-1 , 1]
					
	
# T√≥m t·∫Øt √Ω nghƒ©a  
	Vector ƒë·∫∑c tr∆∞ng cho bi·∫øt c√°c h∆∞·ªõng ch√≠nh c·ªßa ma tr·∫≠n ·∫£nh h∆∞·ªüng 
	Gi√° tr·ªã ƒë·∫∑c tr∆∞ng cho bi·∫øt m·ª©c ƒë·ªô ·∫£nh h∆∞·ªüng(co d√£n, ph√≥ng to )
	
	
//------------------ Vector ƒë·∫∑c tr∆∞ng(Feature Vector) Trong Machine learning 
# Kh√°i ni·ªám : 
	- Feature Vector(Vector ƒë·∫∑c tr∆∞ng) l√† m·ªôt t·∫≠p h·ª£p c√°c thu·ªôc t√≠nh(feature) c·ªßa m·ªôt ƒë·ªëi t∆∞·ª£ng, ƒë∆∞·ª£c bi·ªÉu di·ªÖn d∆∞·ªõi d·∫°ng m·ªôt vector 
	- M·ª•c ƒë√≠ch :  Feature vector d√πng ƒë·ªÉ m√¥ t·∫£ ƒë·ªëi t∆∞·ª£ng d∆∞·ªõi d·∫°ng s·ªë li·ªáu, ƒë·ªÉ m√°y t√≠nh c√≥ th·ªÉ hi·ªÉu v√† x·ª≠ l√Ω. 
	
	N√≥i c√°ch kh√°c, n√≥ l√† c√°ch ch√∫ng ta chuy·ªÉn ƒë·ªïi th√¥ng tin t·ª´ th·∫ø gi·ªõi th·ª±c nh∆∞ h√¨nh ·∫£nh, vƒÉn b·∫£n, √¢m thanh th√†nh c√°c con s·ªë m√† m√°y t√≠nh c√≥ th·ªÉ ph√¢n t√≠ch. 
	
	## T·∫°i sao c·∫ßn Feature Vector trong Machine Learning ? 
		M√°y t√≠nh kh√¥ng hi·ªÉu c√°c d·ªØ li·ªáu th√¥ nh∆∞ con ng∆∞·ªùi. V√¨ v·∫≠y, ch√∫ng ta c·∫ßn bi·ªÉu di·ªÖn d·ªØ li·ªáu d∆∞·ªõi d·∫°ng vector s·ªë, gi√∫p c√°c thu·∫≠t to√°n h·ªçc m√°y(machine learning) c√≥ th·ªÉ : 
			1. So s√°nh gi·ªØa c√°c ƒë·ªëi t∆∞·ª£ng 
			2. T√¨m ra m·ªëi quan h·ªá gi·ªØa c√°c ƒë·∫∑c tr∆∞ng 
			3. D·ª± ƒëo√°n, ph√¢n lo·∫°i, ho·∫∑c th·ª±c hi·ªán c√°c nhi·ªám v·ª• h·ªçc m√°y kh√°c. 

# V√≠ d·ª• d·ªÖ hi·ªÉu v·ªÅ Feature Vector 
	
	## V√≠ d·ª• 1: Ph√¢n lo·∫°i tr√°i c√¢y 
		Gi·∫£ s·ª≠ b·∫°n mu·ªën hu·∫•n luy·ªán m·ªôt m√¥ h√¨nh ƒë·ªÉ ph√¢n lo·∫°i tr√°i c√¢y(t√°o, cam, chu·ªëi). M·ªói tr√°i c√¢y c√≥ c√°c ƒë·∫∑c tr∆∞ng sau : 
			- Kh·ªëi l∆∞·ª£ng (gram) : 150g, 200g, 120g
			- M√†u s·∫Øc : ƒê·ªè, v√†ng , cam 
			- ƒê∆∞·ªùng k√≠nh(cm): 6cm , 7cm... 
			
		ƒê·ªÉ bi·ªÉu di·ªÖn tr√°i c√¢y d∆∞·ªõi d·∫°ng vector: 
			- Chuy·ªÉn c√°c ƒë·∫∑c tr∆∞ng th√†nh s·ªë 
				+ Kh·ªëi l∆∞·ª£ng : gi·ªØ nguy√™n 150,200 
				+ M√†u s·∫Øc : M√£ h√≥a th√†nh s·ªë(ƒë·ªè = 0 , cam = 1 , v√†ng  = 2)
				+ ƒê∆∞·ªùng k√≠nh : gi·ªØ nguy√™n 6, 7 
				
			Khi ƒë√≥ m·ªói tr√°i c√¢y ƒë∆∞·ª£c bi·ªÉu di·ªÖn nh∆∞ sau (Feature Vector)
			T√°o = [150 , 0 , 6] , Cam [200 , 1 , 7] , Chu·ªëi [120 , 2 , 5]
			
			
	# V√≠ d·ª• 2: Ph√¢n lo·∫°i email th√†nh spam hay kh√¥ng spam
			M·ªôt email c√≥ th·ªÉ ƒë∆∞·ª£c bi·ªÉu di·ªÖn b·∫±ng c√°c ƒë·∫∑c tr∆∞ng sau:
			
			S·ªë l∆∞·ª£ng t·ª´ kh√≥a "mi·ªÖn ph√≠" xu·∫•t hi·ªán: 3, 0, 1...
			S·ªë li√™n k·∫øt trong email: 5, 2, 0...
			C√≥ t·ªáp ƒë√≠nh k√®m kh√¥ng? (C√≥ = 1, Kh√¥ng = 0).
			M·ªói email s·∫Ω c√≥ Feature Vector:
					Email¬†1=[3,5,1],
					Email¬†2=[0,2,0]

  
‚Äã# ƒê·∫∑c ƒëi·ªÉm ch√≠nh c·ªßa Feature Vector: 

	1. K√≠ch th∆∞·ªõc c·ªë ƒë·ªãnh : Feature Vector ph·∫£i c√≥ s·ªë l∆∞·ª£ng ph·∫ßn t·ª≠ c·ªë ƒë·ªãnh. V√≠ d·ª•, n·∫øu c√≥ 3 ƒë·∫∑c tr∆∞ng(kh·ªëi l∆∞·ª£ng, m√†u s·∫Øc, ƒë∆∞·ªùng k√≠nh), th√¨ m·ªçi vector ƒë·ªÅu c√≥ 3 ph·∫ßn t·ª≠ 
	2. S·ªë li·ªáu h√≥a: m·ªçi ƒë·∫∑c tr∆∞ng ƒë·ªÅu ƒë∆∞·ª£c chuy·ªÉn th√†nh s·ªë(numeric) ƒë·ªÉ m√°y t√≠nh x·ª≠ l√Ω d·ªÖ d√†ng 
	3. ƒêa chi·ªÅu : M·ªôt Feature Vector c√≥ th·ªÉ l√† : 
		Vector 1 chi·ªÅu : [x1 , x2 , xn]
		
		ho·∫∑c nhi·ªÅu chi·ªÅu h∆°n (trong h√¨nh ·∫£nh, √¢m thanh )
		
Feature Vector trong c√°c lo·∫°i d·ªØ li·ªáu kh√°c nhau 
	## 1. H√¨nh ·∫£nh : 
		- M·ªói b·ª©c ·∫£nh c√≥ th·ªÉ ƒë∆∞·ª£c bi·ªÉu di·ªÖn d∆∞·ªõi d·∫°ng ma tr·∫≠n s·ªë(c√°c pixcel)
		- Feature Vector l√† c√°ch bi·ªÉu di·ªÖn c√°c th√¥ng tin ch√≠nh(m√†u s·∫Øc, h√¨nh d·∫°ng, c·∫°nh) d∆∞·ªõi d·∫°ng vector 
	
		### V√≠ d·ª• m·ªôt b·ª©c ·∫£nh 28x28 px c√≥ th·ªÉ ƒë∆∞·ª£c chuy·ªÉn th√†nh vector 784 ph·∫ßn t·ª≠ 
					Feature¬†Vector=[ 0,255,128,‚Ä¶,64]
	
	
	
	## 2. √Çm thanh  
		- D·ªØ li·ªáu √¢m thanh ƒë∆∞·ª£c bi·ªÉu di·ªÖn b·∫±ng c√°c ƒë·∫∑c tr∆∞ng nh∆∞ t·∫ßn s·ªë, bi√™n ƒë·ªô... 
		- V√≠ d·ª• : M·ªôt ƒëo·∫°n √¢m thanh ƒë∆∞·ª£c bi·ªÉu di·ªÖn b·∫±ng m·ªôt Vector nh∆∞: 
				Feature Vector = [440,100,80,0.5]
					T·∫ßn s·ªë 440Hz , Bi√™n ƒë·ªô  = 100 .... 
					
	## VƒÉn b·∫£n : 
		- VƒÉn b·∫£n ƒë∆∞·ª£c chuy·ªÉn th√†nh vector b·∫±ng c√°ch ƒë·∫øm s·ªë t·ª´ ho·∫∑c d√πng c√°c k·ªπ thu·∫≠t nh∆∞ World Embedding 
		- V√≠ d·ª• : VƒÉn b·∫£n I love AI c√≥ th·ªÉ tr·ªü th√†nh  
			Feature¬†Vector = [1,2,0,0]
			(S·ªë l∆∞·ª£ng t·ª´ "I", "love", "AI",...)


# L√†m th·∫ø n√†o ƒë·ªÉ ch·ªçn Feature t·ªët ? 

	M·ªôt Feature t·ªët c·∫ßn ph·∫£i 
		1. M√¥ t·∫£ ch√≠nh x√°c ƒë·ªëi t∆∞·ª£ng : Feature ph·∫£i ch·ª©a th√¥ng tin li√™n quan ƒë·∫øn b√†i to√°n . 
			- V√≠ d·ª• : trong b√†i to√°n ph√¢n lo·∫°i tr√°i c√¢y, m√†u s·∫Øc l√† quan tr·ªçng , nh∆∞ng s·ªë l·∫ßn tr√°i c√¢y ƒë∆∞·ª£c mua trong tu·∫ßn c√≥ th·ªÉ kh√¥ng quan tr·ªçng 
			
		2. Kh√¥ng d∆∞ th·ª´a : tr√°nh c√°c ƒë·∫∑c tr∆∞ng kh√¥ng li√™n quan ho·∫∑c g√¢y nhi·ªÅu(noise)
				-> V√≠ d·ª• trong b√†i to√°n spam email : "k√≠ch th∆∞·ªõc ph√¥ng ch·ªØ" th∆∞·ªùng kh√¥ng li√™n quan 

		3. ƒê·ªôc l·∫≠p : C√°c ƒë·∫∑c tr∆∞ng kh√¥ng n√™n ph·ª• thu·ªôc qu√° nhi·ªÅu v√†o nhau. 
		

# T√≥m l·∫°i 
	Feature vector l√† c√°ch ch√∫ng ta bi·ªÉu di·ªÖn d·ªØ li·ªáu d∆∞·ªõi d·∫°ng s·ªë ƒë·ªÉ m√°y t√≠nh c√≥ th·ªÉ hi·ªÉu v√† ph√¢n t√≠ch. N√≥ l√† m·ªôt ph·∫ßn c·ª±c k·ª≥ quan tr·ªçng trong Machine Learning v√¨ ch·∫•t l∆∞·ª£ng c·ªßa Feature vector quy·∫øt ƒë·ªãnh hi·ªáu su·∫•t c·ªßa m√¥ h√¨nh  
	
	


	
# 4. Vector t·ªëi ∆∞u h∆°n JSON/Object ·ªü ƒëi·ªÉm n√†o?
	Ti√™u ch√≠				Vector														JSON/Object
	T·ªëc ƒë·ªô x·ª≠ l√Ω			Nhanh, nh·ªù th∆∞ vi·ªán to√°n h·ªçc v√† ph·∫ßn c·ª©ng t·ªëi ∆∞u.			Ch·∫≠m h∆°n do c·∫ßn gi·∫£i m√£ c·∫•u tr√∫c v√† x·ª≠ l√Ω d·ªØ li·ªáu.
	Dung l∆∞·ª£ng l∆∞u tr·ªØ		√çt t·ªën b·ªô nh·ªõ (ch·ªâ l∆∞u s·ªë).									T·ªën b·ªô nh·ªõ do ph·∫£i l∆∞u c·∫£ c·∫•u tr√∫c (key-value).
	Thao t√°c to√°n h·ªçc		D·ªÖ d√†ng th·ª±c hi·ªán (c·ªông, nh√¢n,...)							Kh√¥ng h·ªó tr·ª£ to√°n h·ªçc tr·ª±c ti·∫øp.
	ƒê·ªìng nh·∫•t d·ªØ li·ªáu		Ph·∫£i c√≥ k√≠ch th∆∞·ªõc c·ªë ƒë·ªãnh, lo·∫°i d·ªØ li·ªáu gi·ªëng nhau.		C√≥ th·ªÉ kh√°c nhau gi·ªØa c√°c ƒë·ªëi t∆∞·ª£ng.
	Linh ho·∫°t c·∫•u tr√∫c		H·∫°n ch·∫ø (ch·ªâ s·ªë).											Linh ho·∫°t, ch·ª©a c·∫£ text, object l·ªìng nhau,...
	
//-------------------- Vector trong kh√¥ng gian chi·ªÅu cao(Hight-Dimensional Space)

# Kh√°i ni·ªám :
	Khi n√≥i ƒë·∫øn vector trong kh√¥ng gian chi·ªÅu cao (nD), h√£y t∆∞·ªüng t∆∞·ª£ng nh∆∞ ta ƒëang m√¥ ta m·ªôt ƒë·ªëi t∆∞·ª£ng b·∫±ng r·∫•t nhi·ªÅu th√¥ng tin  
		
	## V√≠ d·ª• , n·∫øu ta m√¥ ta m·ªôt ng∆∞·ªùi ta c√≥ th·ªÉ d√πng m·ªôt vector nh∆∞ sau : 
		(Tu·ªïi, C√¢n n·∫∑ng, Chi·ªÅu cao, IQ, Thu nh·∫≠p, S·ªë gi·ªù ng·ªß/ng√†y, S·ªë b∆∞·ªõc ƒëi/ng√†y, ...)
		(25, 70, 175, 110, 2000, 7, 10000, ...)	
		-> M·ªói chi·ªÅu c·ªßa vector t∆∞∆°ng ·ª©ng v·ªõi m·ªôt ƒë·∫∑c ƒëi·ªÉm c·ªßa ƒë·ªëi t∆∞·ª£ng  
		
# Vector trong to√°n h·ªçc(Kh√¥ng gian vector, ph√©p to√°n)
	
	## Bi·ªÉu di·ªÖn vector b·∫±ng ma tr·∫≠n  
		M·ªôt vector c√≥ th·ªÉ vi·∫øt d∆∞·ªõi d·∫°ng ma tr·∫≠n c·ªôt ho·∫∑c ma tr·∫≠n h√†ng : 
			Vector c·ªôt : 
				v = [3 , 4 , 5	]			//vi·∫øt theo chi·ªÅu d·ªçc 

			Vector h√†ng : 
				v = [3 , 4 , 5	]			//vi·∫øt theo chi·ªÅu ngang 
				
				
	## C√°c ph√©p to√°n v·ªõi vector 
		## C·ªông vector 
		
		## Nh√¢n vector v·ªõi s·ªë v√¥ h∆∞·ªõng 
					2 x (3,4,5) = (6,8,10)
					
		## T√≠ch v√¥ h∆∞·ªõng (dot product) - ƒëo ƒë·ªô t∆∞∆°ng ƒë·ªìng gi·ªØa 2 vector 
			
				v1.v2 = x1.x2 + y1y2 + z1z2 
							(1,2,3).(4,5,6) = 1x4 + 2x5 + 3x6 = 32 
							
# Vector trong AI 							
			


//----------------------- H·ªá ph∆∞∆°ng tr√¨nh tuy·∫øn t√≠nh 
# Trong to√°n h·ªçc :
	H·ªá ph∆∞∆°ng tr√¨nh tuy·∫øn t√≠nh l√† m·ªôt t·∫≠p h·ª£p c√°c ph∆∞∆°ng tr√¨nh tuy·∫øn t√≠nh c√≥ d·∫°ng: 
		a1x1 + a2x2 + ... + anxn = b 
		
	## Bi·ªÉu di·ªÖn b·∫±ng ma tr·∫≠n  
		Ax = b 
		
		A = [ [2,3] , [4,-1] ] 
		x = [x,y] 
		b = [8,5]
		
		
	## Gi·∫£i h·ªá ph∆∞∆°ng tr√¨nh tuy·∫øn t√≠nh  
		- Ph∆∞∆°ng ph√°p th·∫ø 
		- Ph∆∞∆°ng ph√°p c·ªông ƒë·∫°i s·ªë 
		- Ph∆∞∆°ng ph√°p ma tr·∫≠n  
			+) N·∫øu A l√† ma tr·∫≠n vu√¥ng v√† kh·∫£ ngh·ªãch(det(A) != 0) ta c√≥ th·ªÉ t√¨m nghi·ªám b·∫±ng 
					x = A^1 . b
						
						
			+) N·∫øu h·ªá c√≥ v√¥ s·ªë nghi·ªám ho·∫∑c v√¥ nghi·ªám, ph·∫£i d√πng ph∆∞∆°ng ph√°p kh√°c nh∆∞ kh·ª≠ Gauss 

			

# Trong AI 
	## ·ª®ng d·ª•ng trong Machine Learning 
		H·ªá ph∆∞∆°ng tr√¨nh tuy·∫øn t√≠nh xu·∫•t hi·ªán nhi·ªÅu trong c√°c b√†i to√°n AI, v√≠ d·ª•: 
			- H·ªìi quy tuy·∫øn t√≠nh (Linear Regression)
			- M·∫°ng n∆°-ron nh√¢n t·∫°o(Neural Networks)
			- Ph√¢n r√£ gi√° tr·ªã k·ª≥ d·ªã (SVD) trong x·ª≠ l√Ω ·∫£nh 


//------------------- !!!!!!!!!!!!!!!!!!!!!!!!!!!!!--------------------------
// CH√ö √ù : h√£y h·ªçc th√™m chi ti·∫øt c·ª• th·ªÉ v·ªÅ ph√¢n t√≠ch gi√° tr·ªã k·ª≥ d·ªã trong to√°n h·ªçc, v√† trong AI, m·ªõi ch·ªâ h·ªçc v·ªÅ kh√°i ni·ªám th√¥i ch∆∞a hi·ªÉu l·∫Øm  


//----------------------- Ph√¢n t√≠ch gi√° tr·ªã k·ª≥ d·ªã (SVD Singular Value Decomposition) trong to√°n h·ªçc 
# Kh√°i ni·ªám SVD trong to√°n h·ªçc 
	Ph√¢n r√£ gi√° tr·ªã k·ª≥ d·ªã(SVD) l√† m·ªôt c√°ch ph√¢n t√°ch m·ªôt ma tr·∫≠n th√†nh ba ma tr·∫≠n con ƒë·∫∑c bi·ªát : 
			A = U Z_ V^T    // Z_ l√† x√≠ch ma 
			
			- A : l√† ma tr·∫≠n g·ªëc(c√≥ th·ªÉ l√† b·∫•t k·ª≥ ma tr·∫≠n vu√¥ng ho·∫∑c ch·ªØ nh·∫≠t n√†o)
			- U l√† ma tr·∫≠n tr·ª±c giao(c·ªôt c·ªßa n√≥ l√† vector ƒë·∫∑c tr∆∞ng tr√°i c·ªßa A)
			- Z_ : L√† ma tr·∫≠n ƒë∆∞·ªùng ch√©o ch·ª©a c√°c gi√° tr·ªã k·ª≥ d·ªã (s·∫Øp x·∫øp theo th·ª© t·ª± gi·∫£m d·∫ßn).
			- V^T : l√† ma tr·∫≠n tr·ª±c giao(h√†ng c·ªßa n√≥ l√† vector ƒë·∫∑c tr∆∞ng ph·∫£i c·ªßa A )
	
	## √ù nghƒ©a 
		Gi·∫£m s·ªë chi·ªÅu d·ªØ li·ªáu : Ch·ªâ gi·ªØ l·∫°i c√°c gi√° tr·ªã k·ª≥ d·ªã l·ªõn nh·∫•t ƒë·ªÉ lo·∫°i b·ªè nhi·ªÖu 
		T√¨m nhanh c√°c th√†nh ph·∫ßn quan tr·ªçng nh·∫•t trong d·ªØ li·ªáu(d√πng trong PCA - Principal Component Analysis )
		X·∫•p x·ªâ ma tr·∫≠n : N·∫øu ta ch·ªâ gi·ªØ v√†i gi√° tr·ªã k·ª≥ d·ªã l·ªõn nh·∫•t, ta c√≥ th·ªÉ x·∫•p x·ªâ ma tr·∫≠n g·ªëc v·ªõi ma tr·∫≠n nh·ªè h∆°n 
		
		V√≠ d·ª•, n·∫øu A l√† ma tr·∫≠n 1000x1000 nh∆∞ng ch·ªâ c√≥ 10 gi√° tr·ªã k·ª≥ d·ªã l·ªõn, ta c√≥ th·ªÉ d√πng ma tr·∫≠n 1000x10 thay v√¨ 1000x1000 gi√∫p ti·∫øt ki·ªám kh√¥ng gian l∆∞u tr·ªØ v√† t√≠nh to√°n  
	
	
# H√¨nh dung tr·ª±c quan 
		### V√≠ d·ª• : H√¨nh dung m·ªôt ma tr·∫≠n l√†m bi·∫øn d·∫°ng ·∫£nh 
		
		H√£y t∆∞·ªüng t∆∞·ª£ng b·∫°n c√≥ m·ªôt t·∫•m ·∫£nh v√† mu·ªën n√©n ·∫£nh m√† v·∫´n gi·ªØ l·∫°i n·ªôi dung quan tr·ªçng 
			- B∆∞·ªõc 1 : Chuy·ªÉn ·∫£nh th√†nh m·ªôt ma tr·∫≠n A( m·ªói √¥ l√† gi√° tr·ªã pixel)
			- B∆∞·ªõc 2 : D√πng SVD ƒë·ªÉ ph√¢n r√£ A th√†nh U, Z_ , V^T 
			- B∆∞·ªõc 3 :  Gi·ªØ l·∫°i m·ªôt s·ªë √≠t gi√° tr·ªã trong Z_, b·ªè b·ªõt nh·ªØng gi√° tr·ªã nh·ªè h∆°n 
			- B∆∞·ªõc 4 : Nh√¢n l·∫°i c√°c ma tr·∫≠n, thu ƒë∆∞·ª£c h√¨nh ·∫£nh ƒë√£ gi·∫£m dung l∆∞·ª£ng nh∆∞ng v·∫´n gi·ªØ ƒë∆∞·ª£c n·ªôi dung ch√≠nh  
			
		-> ƒêi·ªÅu n√†y gi·ªëng nh∆∞ b·∫°n ƒëang chuy·ªÉn ƒë·ªïi m·ªôt h√¨nh ·∫£nh th√†nh c√°c l·ªõp th√¥ng tin ch√≠nh, gi·ªëng nh∆∞ v·∫Ω ph√°c th·∫£o thay v√¨ v·∫Ω to√†n b·ªô chi ti·∫øt nh·ªè 
		H√¨nh ·∫£nh sau khi x·ª≠ l√Ω tr√¥ng c√≥ v·∫ª gi·ªëng ·∫£nh g·ªëc nh∆∞ng th·ª±c t·∫ø ƒë√£ b·ªã gi·∫£m s·ªë chi·ªÅu d·ªØ li·ªáu, gi√∫p x·ª≠ l√Ω nhanh h∆°n trong AI 
		
# V√≠ d·ª• to√°n h·ªçc 
	
	
		
//----------------------- Ph√¢n t√≠ch gi√° tr·ªã k·ª≥ d·ªã (SVD Singular Value Decomposition) trong machine learning 
			
			
//------------------------ Vector ƒë·∫∑c tr∆∞ng ph·∫£i , Vector ƒë·∫∑c tr∆∞ng tr√°i 
# Vector ƒë·∫∑c tr∆∞ng tr√°i (U)
	## ƒê·ªãnh nghƒ©a : 
		Vector ƒë·∫∑c tr∆∞ng tr√°i l√† c√°c c·ªôt c·ªßa ma tr·∫≠n U 
	
	## Vai tr√≤ : 
		Ch√∫ng th·ªÉ hi·ªán c√°ch d·ªØ li·ªáu g·ªëc b·ªã k√©o d√†i ho·∫∑c n√©n theo h√†ng c·ªßa ma tr·∫≠n A 
		
	## √ù nghƒ©a : 
		N·∫øu ta coi ma tr·∫≠n A l√† m·ªôt h·ªá th·ªëng bi·∫øn ƒë·ªïi d·ªØ li·ªáu, th√¨ vector ƒë·∫∑c tr∆∞ng tr√°i cho ta bi·∫øt h∆∞·ªõng ch√≠nh c·ªßa d·ªØ li·ªáu trong kh√¥ng gian h√†ng  . 
		
	## V√≠ d·ª• d·ªÖ h√¨nh dung  	
		H√£y t∆∞·ªüng t∆∞·ª£ng b·∫°n c√≥ m·ªôt b·ªô s∆∞u t·∫≠p h√¨nh ·∫£nh. M·ªói h√¨nh l√† m·ªôt h√†ng c·ªßa ma tr·∫≠n A. C√°c vector ƒë·∫∑c tr∆∞ng tr√°i s·∫Ω cho ta bi·∫øt c√°c m·∫´u ch√≠nh c·ªßa h√¨nh ·∫£nh ƒë√≥, t·ª©c l√† nh·ªØng ƒë·∫∑c ƒëi·ªÉm quan tr·ªçng nh·∫•t trong t·ª´ng h√†ng. 
		
		·ª®ng d·ª•ng trong AI: Vector ƒë·∫∑c tr∆∞ng tr√°i th∆∞·ªùng ƒë∆∞·ª£c d√πng trong gi·∫£m s·ªë chi·ªÅu d·ªØ li·ªáu (PCA,LSA trong NLP, x·ª≠ l√Ω ·∫£nh , vv)
		
		
# Vector ƒë·∫∑c tr∆∞ng ph·∫£i 
	## ƒê·ªãnh nghƒ©a : 
		Vector ƒë·∫∑c tr∆∞ng ph·∫£i l√† c√°c c·ªôt c·ªßa ma tr·∫≠n V(ho·∫∑c c√°c h√†ng c·ªßa V^T)
		
	## Vai tr√≤ : 
		Ch√∫ng th·ªÉ hi·ªán c√°ch d·ªØ li·ªáu g·ªëc b·ªã k√©o d√†i ho·∫∑c n√©n theo c·ªôt c·ªßa ma tr·∫≠n A 
		
	## √ù nghƒ©a :  
		N·∫øu ta coi ma tr·∫≠n A l√† m·ªôt ph√©p bi·∫øn ƒë·ªïi, th√¨ vector ƒë·∫∑c tr∆∞ng ph·∫£i cho ta bi·∫øt h∆∞·ªõng ch√≠nh c·ªßa d·ªØ li·ªáu trong kh√¥ng gian c·ªôt 
		
	## V√≠ d·ª• h√¨nh dung 
		N·∫øu ta coi ma tr·∫≠n A ch·ª©a d·ªØ li·ªáu v·ªÅ ng∆∞·ªùi d√πng v√† phim, trong ƒë√≥: 
			- H√†ng l√† ng∆∞·ªùi d√πng 
			- C·ªôt l√† phim 
			
		Khi √°p d·ª•ng SVD, c√°c vector ƒë·∫∑c tr∆∞ng ph·∫£i cho ta bi·∫øt m·ªëi quan h·ªá gi·ªØa c√°c b·ªô phim t·ª©c l√† c√°c phim c√≥ ƒë·∫∑c ƒëi·ªÉm gi·ªëng nhau s·∫Ω c√≥ vector ƒë·∫∑c tr∆∞ng ph·∫£i t∆∞∆°ng t·ª± nhau . 
		·ª®ng d·ª•ng trong AI : Vector ƒë·∫∑c tr∆∞ng ph·∫£i r·∫•t quan tr·ªçng trong h·ªá th·ªëng g·ª£i √Ω (Recommendation Systems), gi√∫p ƒë·ªÅ xu·∫•t phim, nh·∫°c, s·∫£n ph·∫©m cho ng∆∞·ªùi d√πng. 
		
	
# So s√°nh Vector ƒë·∫∑c tr∆∞ng tr√°i v√† Vector ƒë·∫∑c tr∆∞ng ph·∫£i 

Ti√™u ch√≠ 						Vector ƒë·∫∑c tr∆∞ng tr√°i(U) 							Vector ƒë·∫∑c tr∆∞ng ph·∫£i(V)
	
V·ªã tr√≠ trong SVD					C·ªôt c·ªßa U										C·ªôt c·ªßa V (ho·∫∑c h√†ng c·ªßa V^T)
Bi·ªÉu di·ªÖn d·ªØ li·ªáu theo				H√†ng c·ªßa ma tr·∫≠n A								C·ªôt c·ªßa ma tr·∫≠n A 
√ù nghƒ©a								ƒê·∫∑c tr∆∞ng ch√≠nh c·ªßa h√†ng d·ªØ li·ªáu				ƒê·∫∑c tr∆∞ng ch√≠nh c·ªßa c·ªôt d·ªØ li·ªáu
·ª®ng d·ª•ng							Gi·∫£m s·ªë chi·ªÅu d·ªØ li·ªáu (PCA, x·ª≠ l√Ω ·∫£nh, NLP)		H·ªá th·ªëng g·ª£i √Ω, ph√¢n c·ª•m d·ªØ li·ªáu


# H√¨nh dung tr·ª±c quan 

		H√£y t∆∞·ªüng t∆∞·ª£ng m·ªôt b·ª©c ·∫£nh l√† m·ªôt ma tr·∫≠n A
		üì∑ B·ª©c ·∫£nh = Ma tr·∫≠n A (g·ªìm c√°c pixel)
		
		Vector ƒë·∫∑c tr∆∞ng tr√°i gi·ªëng nh∆∞ b·∫£n ph√°c th·∫£o c·ªßa b·ª©c ·∫£nh theo t·ª´ng h√†ng, gi√∫p gi·ªØ l·∫°i nh·ªØng n√©t ch√≠nh c·ªßa b·ª©c ·∫£nh.
		Vector ƒë·∫∑c tr∆∞ng ph·∫£i gi·ªëng nh∆∞ b·∫£n ph√°c th·∫£o c·ªßa b·ª©c ·∫£nh theo t·ª´ng c·ªôt, gi√∫p gi·ªØ l·∫°i th√¥ng tin theo chi·ªÅu d·ªçc.
		Khi b·∫°n gi·∫£m s·ªë chi·ªÅu d·ªØ li·ªáu b·∫±ng SVD, b·∫°n th·ª±c ch·∫•t ƒëang gi·ªØ l·∫°i nh·ªØng th√†nh ph·∫ßn quan tr·ªçng nh·∫•t c·ªßa b·ª©c ·∫£nh.

# K·∫øt lu·∫≠n 
		
	## Vector ƒë·∫∑c tr∆∞ng tr√°i(U)  gi√∫p hi·ªÉu v·ªÅ d·ªØ li·ªáu theo h∆∞·ªõng h√†ng(ƒë·ªëi t∆∞·ª£ng ch√≠nh trong t·∫≠p d·ªØ li·ªáu)
	
	## Vector ƒë·∫∑c tr∆∞ng ph·∫£i(V) gi√∫p hi·ªÉu v·ªÅ d·ªØ li·ªáu theo h∆∞·ªõng c·ªôt(m·ªëi quan h·ªá gi·ªØa c√°c thu·ªôc t√≠nh d·ªØ li·ªáu)
	
	## SVD l√† c√¥ng c·ª• c·ª±c k·ª≥ m·∫°nh trong AI, d√πng ƒë·ªÉ gi·∫£m s·ªë chi·ªÅu, t√¨m ƒë·∫∑c tr∆∞ng quan tr·ªçng, v√† ph√°t hi·ªán quan h·ªá gi·ªØa c√°c th√†nh ph·∫ßn d·ªØ li·ªáu 
	
	Gi·∫£m chi·ªÅu d·ªØ li·ªáu c√≥ th·ªÉ h√¨nh dung gi·ªëng nh∆∞ ·∫£nh to√†n k√Ω (hologram),
	
	## H√¨nh dung v·ªÅ ·∫¢nh To√†n K√Ω vs. Gi·∫£m Chi·ªÅu D·ªØ Li·ªáu
			### ·∫¢nh to√†n k√Ω (hologram):
		
				Khi nh√¨n t·ª´ nhi·ªÅu g√≥c ƒë·ªô kh√°c nhau, b·∫°n v·∫´n th·∫•y to√†n b·ªô v·∫≠t th·ªÉ, d√π m·ªói g√≥c nh√¨n ch·ªâ ch·ª©a m·ªôt ph·∫ßn th√¥ng tin.
				T√≠nh ch·∫•t: M·∫•t m·ªôt ph·∫ßn d·ªØ li·ªáu nh∆∞ng v·∫´n c√≥ th·ªÉ t√°i t·∫°o h√¨nh ·∫£nh g·∫ßn ƒë√∫ng c·ªßa v·∫≠t th·ªÉ.
			### Gi·∫£m chi·ªÅu d·ªØ li·ªáu (Dimensionality Reduction):
		
				B·∫°n n√©n to√†n b·ªô d·ªØ li·ªáu g·ªëc th√†nh m·ªôt d·∫°ng ƒë∆°n gi·∫£n h∆°n, nh∆∞ng v·∫´n gi·ªØ ƒë∆∞·ª£c nh·ªØng ƒë·∫∑c ƒëi·ªÉm quan tr·ªçng nh·∫•t.
				T√≠nh ch·∫•t: Lo·∫°i b·ªè th√¥ng tin d∆∞ th·ª´a ho·∫∑c nhi·ªÖu, nh∆∞ng v·∫´n gi·ªØ ƒë∆∞·ª£c c·∫•u tr√∫c ch√≠nh.
				
				
					üí° ƒêi·ªÉm t∆∞∆°ng ƒë·ªìng:
					C·∫£ hai ƒë·ªÅu c·ªë g·∫Øng gi·ªØ l·∫°i th√¥ng tin quan tr·ªçng nh·∫•t v·ªõi d·ªØ li·ªáu √≠t h∆°n.
					Khi b·∫°n gi·∫£m s·ªë chi·ªÅu c·ªßa d·ªØ li·ªáu AI, n√≥ v·∫´n gi·ªØ ƒë∆∞·ª£c ph·∫ßn l·ªõn √Ω nghƒ©a c·ªßa d·ªØ li·ªáu g·ªëc, gi·ªëng nh∆∞ c√°ch m·ªôt ·∫£nh to√†n k√Ω v·∫´n hi·ªÉn th·ªã ƒë∆∞·ª£c v·∫≠t th·ªÉ t·ª´ nhi·ªÅu g√≥c ƒë·ªô.
					
					
					üí° ƒêi·ªÉm kh√°c bi·ªát:
					·∫¢nh to√†n k√Ω l∆∞u th√¥ng tin theo d·∫°ng t·∫ßn s·ªë √°nh s√°ng v√† v·∫´n ch·ª©a g·∫ßn nh∆∞ to√†n b·ªô d·ªØ li·ªáu, ch·ªâ l√† b·ªã ph√¢n b·ªë kh√°c ƒëi.
					Gi·∫£m chi·ªÅu d·ªØ li·ªáu trong AI (nh∆∞ PCA, SVD) th·ª±c s·ª± lo·∫°i b·ªè d·ªØ li·ªáu √≠t quan tr·ªçng, ch·ª© kh√¥ng ch·ªâ ph√¢n b·ªë l·∫°i n√≥.
					
					
					üìå V√≠ d·ª• Tr·ª±c Quan
					üñº Gi·∫£ s·ª≠ b·∫°n c√≥ m·ªôt ·∫£nh ƒëen tr·∫Øng 1000x1000 pixel (1 tri·ªáu ƒëi·ªÉm ·∫£nh).
					
						N·∫øu ch·ª•p ·∫£nh to√†n k√Ω:
							B·ª©c ·∫£nh c√≥ th·ªÉ ƒë∆∞·ª£c l∆∞u tr·ªØ d∆∞·ªõi d·∫°ng s√≥ng √°nh s√°ng v√† t√°i t·∫°o t·ª´ nhi·ªÅu g√≥c nh√¨n, nh∆∞ng th√¥ng tin kh√¥ng b·ªã m·∫•t ƒëi nhi·ªÅu.
							
						N·∫øu d√πng gi·∫£m chi·ªÅu d·ªØ li·ªáu:
							Gi·∫£ s·ª≠ b·∫°n √°p d·ª•ng SVD v√† gi·ªØ l·∫°i ch·ªâ 100 th√†nh ph·∫ßn quan tr·ªçng nh·∫•t, b·ª©c ·∫£nh c√≥ th·ªÉ gi·∫£m xu·ªëng ch·ªâ c√≤n 100x100 pixel, nh∆∞ng v·∫´n ƒë·ªß r√µ ƒë·ªÉ nh·∫≠n di·ªán n·ªôi dung ch√≠nh.
					
					üìå ·ª®ng d·ª•ng th·ª±c t·∫ø:
						X·ª≠ l√Ω ·∫£nh: Khi gi·∫£m chi·ªÅu d·ªØ li·ªáu, AI c√≥ th·ªÉ l√†m m·ªù c√°c chi ti·∫øt nh·ªè kh√¥ng quan tr·ªçng nh∆∞ng v·∫´n gi·ªØ ƒë∆∞·ª£c n·ªôi dung ch√≠nh.
						Nh·∫≠n d·∫°ng khu√¥n m·∫∑t: AI c√≥ th·ªÉ l∆∞u tr·ªØ ch·ªâ nh·ªØng ƒë·∫∑c ƒëi·ªÉm quan tr·ªçng nh·∫•t, thay v√¨ ghi nh·ªõ to√†n b·ªô h√¨nh ·∫£nh chi ti·∫øt.
						
					ü§î K·∫øt Lu·∫≠n
						‚úî Gi·∫£m chi·ªÅu d·ªØ li·ªáu c√≥ th·ªÉ ƒë∆∞·ª£c so s√°nh v·ªõi ·∫£nh to√†n k√Ω ·ªü kh√≠a c·∫°nh c·ªë g·∫Øng gi·ªØ l·∫°i th√¥ng tin quan tr·ªçng b·∫±ng c√°ch gi·∫£m l∆∞·ª£ng d·ªØ li·ªáu c·∫ßn l∆∞u tr·ªØ.
						‚úî Nh∆∞ng AI th·ª±c s·ª± lo·∫°i b·ªè d·ªØ li·ªáu √≠t quan tr·ªçng, trong khi hologram ph√¢n ph·ªëi l·∫°i th√¥ng tin theo m·ªôt c√°ch kh√°c.
						‚úî Hi·ªÉu c√°ch n√†y s·∫Ω gi√∫p b·∫°n d·ªÖ d√†ng n·∫Øm b·∫Øt vai tr√≤ c·ªßa gi·∫£m s·ªë chi·ªÅu trong AI! 	
			



//------------------------ Kh√¥ng gian vector trong h·ªçc m√°y  			



	 ## . T·∫°i sao to√†n b·ªô AI ƒë·ªÅu s·ª≠ d·ª•ng Vector?
		
		Nh√¢n vector v·ªõi tr·ªçng s·ªë.
		T√≠nh kho·∫£ng c√°ch (Euclidean, cosine).
		T·ªëi ∆∞u h√†m m·∫•t m√°t (loss function).


//---------------------------- √Ånh x·∫° tuy·∫øn t√≠nh (Linear Mapping)

# ƒê·ªãnh nghƒ©a 
	√Ånh x·∫° tuy·∫øn t√≠nh l√† m·ªôt ph√©p bi·∫øn ƒë·ªïi t·ª´ m·ªôt kh√¥ng gian vector sang m·ªôt kh√¥ng gian vector kh√°c m√† v·∫´n gi·ªØ nguy√™n c·∫•u tr√∫c tuy·∫øn t√≠nh, N√≥i ƒë∆°n gi·∫£n n√≥ l√† m·ªôt h√†m bi·∫øn ƒë·ªïi vector nh∆∞ng v·∫´n b·∫£o to√†n ph√©p c·ªông v√† nh√¢n v√¥ h∆∞·ªõng 
	
	N·∫øu c√≥ m·ªôt √°nh x·∫° T t·ª´ kh√¥ng gian vector V sang W, th√¨ T l√† √°nh x·∫° tuy·∫øn t√≠nh n·∫øu th·ªèa m√£n  
	
	1. B·∫£o to√†n ph√©p c·ªông  
			T (u + v) = T(u) + T(v)
			
	2. B·∫£o to√†n ph√©p nh√¢n v√¥ h∆∞·ªõng  
			T(cv) = cT(v) (v·ªõi m·ªçi s·ªë th·ª±c C )
			
			
			
# H√¨nh dung tr·ª±c quan 
	H√£y t∆∞·ªüng t∆∞·ª£ng √°nh x·∫° tuy·∫øn t√≠nh c≈©ng gi·ªëng nh∆∞ m·ªôt b·ªô l·ªçc ·∫£nh. Khi b·∫°n √°p d·ª•ng b·ªô l·ªçc l√™n ·∫£nh, ·∫£nh s·∫Ω b·ªã thay ƒë·ªïi m√†u s·∫Øc, ƒë·ªô s√°ng nh∆∞ng c√°c ƒë∆∞·ªùng n√©t v·∫´n gi·ªØ nguy√™n 
	
	V√≠ d·ª• : 
		M·ªôt √°nh x·∫° tuy·∫øn t√≠nh c√≥ th·ªÉ ph√≥ng to ho·∫∑c thu nh·ªè vector, nh∆∞ng kh√¥ng l√†m cong n√≥ 
		
		M·ªôt ph√©p quay(rotation) c≈©ng l√† m·ªôt √°nh x·∫° tuy·∫øn t√≠nh 
	

	## Li√™n h·ªá v·ªõi Java 
	Trong Java, ta c√≥ th·ªÉ bi·ªÉu di·ªÖn √°nh x·∫° tuy·∫øn t√≠nh nh∆∞ m·ªôt ph∆∞∆°ng th·ª©c nh·∫≠n v√†o m·ªôt vector v√† tr·∫£ v·ªÅ m·ªôt vector m·ªõi. V√≠ d·ª•:
			public class LinearTransformation {
			private double[][] matrix; // Ma tr·∫≠n √°nh x·∫° tuy·∫øn t√≠nh
		
			public LinearTransformation(double[][] matrix) {
				this.matrix = matrix;
			}
		
			public double[] apply(double[] vector) {
				if (vector.length != matrix[0].length) {
					throw new IllegalArgumentException("Vector size must match matrix columns");
				}
				double[] result = new double[matrix.length];
				for (int i = 0; i < matrix.length; i++) {
					for (int j = 0; j < vector.length; j++) {
						result[i] += matrix[i][j] * vector[j];
					}
				}
				return result;
			}
		
			public static void main(String[] args) {
				double[][] transformationMatrix = {
					{2, 0},
					{0, 3}
				};
				LinearTransformation transformation = new LinearTransformation(transformationMatrix);
		
				double[] vector = {1, 1};
				double[] transformedVector = transformation.apply(vector);
		
				System.out.println("Transformed Vector: [" + transformedVector[0] + ", " + transformedVector[1] + "]");
			}
		}


//----------------------- C∆° s·ªü (Basis )
# Kh√°i ni·ªám 

	C∆° s·ªü c·ªßa m·ªôt kh√¥ng gian vector l√† m·ªôt t·∫≠p h·ª£p c√°c vector ƒë·ªôc l·∫≠p tuy·∫øn t√≠nh c√≥ th·ªÉ bi·ªÉu di·ªÖn t·∫•t c·∫£ c√°c vector trong kh√¥ng gian ƒë√≥. 
	N√≥i c√°ch kh√°c, c∆° s·ªü l√† m·ªôt "b·ªô khung" gi√∫p x√¢y d·ª±ng to√†n b·ªô kh√¥ng gian vector  

	N√≥ ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ chuy·ªÉn ƒë·ªïi gi·ªØa c√°c h·ªá t·ªça ƒë·ªô kh√°c nhau 

# H√¨nh dung tr·ª±c quan  

	H√£y t∆∞·ªüng t∆∞·ª£ng b·∫°n ƒëang x√¢y m·ªôt ng√¥i nh√† b·∫±ng g·∫°ch. M·ªói vi√™n g·∫°ch l√† m·ªôt vector c∆° s·ªü, v√† b·∫°n c√≥ th·ªÉ k·∫øt h·ª£p c√°c vi√™n g·∫°ch theo nhi·ªÅu c√°ch kh√°c nhau ƒë·ªÉ t·∫°o ra c√°c b·ª©c t∆∞·ªùng kh√°c nhau  

	## V√≠ d·ª• trong kh√¥ng gian 2D, c√°c vector c∆° s·ªü ph·ªï bi·∫øn nh·∫•t l√† : 
			e1 = (1,0)  , e2 =(0,1)
			
	## M·ªçi vector trong kh√¥ng gian 2D c√≥ th·ªÉ ƒë∆∞·ª£c bi·ªÉu di·ªÖn b·∫±ng t·ªï h·ª£p tuy·∫øn t√≠nh c·ªßa hai vector n√†y 
			
			(3,4) = 3(1,0) + 4(0,1)


# Li√™n h·ªá v·ªõi Java l·∫≠p tr√¨nh 
	Trong l·∫≠p tr√¨nh, m·ªôt t·∫≠p h·ª£p vector c∆° s·ªü gi·ªëng nh∆∞ m·ªôt t·∫≠p c√°c class tr·ª´u t∆∞·ª£ng(abstract class). M·ªçi vector trong kh√¥ng gian c√≥ th·ªÉ coi l√† instance c·ªßa nh·ªØng class n√†y  
	
	
	
//------------------------------ H·ªá s·ªë ma tr·∫≠n(Matrix Coefficients)

# ƒê·ªãnh nghƒ©a 
	H·ªá s·ªë ma tr·∫≠n ch√≠nh l√† c√°c gi√° tr·ªã trong ma tr·∫≠n, quy·∫øt ƒë·ªãnh c√°ch √°nh x·∫° tuy·∫øn t√≠nh bi·∫øn ƒë·ªïi vector 
		
		V√≠ d·ª• v·ªõi ma tr·∫≠n 
					A = [ [2 , -1 ] , [0 , 3]]
					
				S·ªë 2 l√† h·ªá s·ªë t·∫°i h√†ng 1 c·ªôt 1 
				S·ªë -1 l√† h·ªá s·ªë t·∫°i h√†ng 1 c·ªôt 2 
				
	=> H·ªá s·ªë ma tr·∫≠n c√≥ th·ªÉ hi·ªÉu l√† tr·ªçng s·ªë c·ªßa √°nh x·∫° tuy·∫øn t√≠nh 
	
	
//------------------------------ Ma tr·∫≠n chuy·ªÉn ƒë·ªïi(Change of Basis Matrix)	

# ƒê·ªãnh nghƒ©a 
	Ma tr·∫≠n chuy·ªÉn ƒë·ªïi l√† ma tr·∫≠n gi√∫p chuy·ªÉn t·ª´ h·ªá t·ªça ƒë·ªô n√†y sang h·ªá t·ªça ƒë·ªô kh√°c 
	N·∫øu ta c√≥ m·ªôt kh√¥ng gian v·ªõi c∆° s·ªü B = {b1, b2} v√† m·ªôt kh√¥ng gian kh√°c v·ªõi c∆° s·ªü C = {c1 , c2}, ta c√≥ th·ªÉ d√πng ma tr·∫≠n chuy·ªÉn ƒë·ªïi P ƒë·ªÉ ƒë·ªïi t·ª´ h·ªá n√†y sang h·ªá kia : 
			vc = Pvb 
			
		trong ƒë√≥ 
			vb : l√† vector trong h·ªá c∆° s·ªü B 
			P : l√† ma tr·∫≠n chuy·ªÉn ƒë·ªïi 
			vc : L√† vector trong h·ªá c∆° s·ªü C  

# ·ª®ng d·ª•ng 
	Chuy·ªÉn ƒë·ªïi gi·ªØa c√°c h·ªá t·ªça ƒë·ªô kh√°c nhau 
	√Åp d·ª•ng trong ƒë·ªì h·ªça m√°y t√≠nh v√† AI  
	
#T√≥m l·∫°i
	√Ånh x·∫° tuy·∫øn t√≠nh gi√∫p bi·∫øn ƒë·ªïi vector nh∆∞ng v·∫´n gi·ªØ t√≠nh ch·∫•t tuy·∫øn t√≠nh.
	C∆° s·ªü l√† "khung x∆∞∆°ng" c·ªßa kh√¥ng gian vector.
	H·ªá s·ªë ma tr·∫≠n l√† c√°c s·ªë quy·∫øt ƒë·ªãnh c√°ch √°nh x·∫° ho·∫°t ƒë·ªông.
	Ma tr·∫≠n chuy·ªÉn ƒë·ªïi gi√∫p ƒë·ªïi t·ª´ h·ªá c∆° s·ªü n√†y sang h·ªá c∆° s·ªü kh√°c.


‚úÖ Gi·∫£m chi·ªÅu d·ªØ li·ªáu & PCA (Principal Component Analysis): Hi·ªÉu PCA s·∫Ω gi√∫p b·∫°n th·∫•y ·ª©ng d·ª•ng th·ª±c t·∫ø c·ªßa SVD d·ªÖ d√†ng h∆°n.

üìå Sau khi n·∫Øm ch·∫Øc c√°c kh√°i ni·ªám tr√™n, vi·ªác h·ªçc SVD s·∫Ω tr·ªü n√™n d·ªÖ d√†ng h∆°n v√¨ b·∫°n ƒë√£ c√≥ n·ªÅn t·∫£ng v·ªØng ch·∫Øc.


		

//----------------------- Gi·∫£m chi·ªÅu d·ªØ li·ªáu (Dimensionality Redution)
# Kh√°i ni·ªám  
	Gi·∫£m chi·ªÅu d·ªØ li·ªáu l√† qu√° tr√¨nh gi·∫£m s·ªë l∆∞·ª£ng bi·∫øn(features) trong m·ªôt t·∫≠p h·ª£p d·ªØ li·ªáu m√† v·∫´n gi·ªØ l·∫°i c√†ng nhi·ªÅu th√¥ng tin quan tr·ªçng c√†ng t·ªët. Vi·ªác n√†y gi√∫p 
		- Gi·∫£m ƒë·ªô ph·ª©c t·∫°p t√≠nh to√°n 
		- Gi·∫£m nguy c∆° overfitting 
		- D·ªÖ tr·ª±c quan h√≥a d·ªØ li·ªáu h∆°n (nh·∫•t l√† khi d·ªØ li·ªáu >3 chi·ªÅu )
		- C·∫£i thi·ªán hi·ªáu su·∫•t m√¥ h√¨nh AI 
		
//-------- Gi·∫£m chi·ªÅu trong to√°n cao c·∫•p 
# 2.1 Ma tr·∫≠n v√† √°nh x·∫° tuy·∫øn t√≠nh trong gi·∫£m chi·ªÅu 
	M·ªôt t·∫≠p d·ªØ li·ªáu c√≥ th·ªÉ ƒë∆∞·ª£c bi·ªÉu di·ªÖn d∆∞·ªõi d·∫°ng ma tr·∫≠n X thu·ªôc R^(m.n), trong ƒë√≥ 
		m : l√† s·ªë m·∫´u (s·ªë h√†ng)
		n : l√† s·ªë chi·ªÅu(s·ªë c·ªôt =  s·ªë feature)
		
	B·∫£n ch·∫•t c·ªßa gi·∫£m chi·ªÅu l√† t√¨m m·ªôt √°nh x·∫° tuy·∫øn t√≠nh T : R^n -> R^k v·ªõi k < n sao cho gi·ªØ l·∫°i c√†ng nhi·ªÅu th√¥ng tin c√†ng t·ªët . 
	
	ƒêi·ªÅu n√†y li√™n quan ƒë·∫øn kh√¥ng gian con(subspace) trong kh√¥ng gian vector : 
	Thay v√¨ l√†m vi·ªác v·ªõi kh√¥ng gian g·ªëc R^n , ta t√¨m m·ªôt kh√¥ng gian con R^k c√≥ th·ªÉ m√¥ t·∫£ d·ªØ li·ªáu t·ªët nh·∫•t 
	
# 2.2 C√°c ph∆∞∆°ng ph√°p to√°n h·ªçc ch√≠nh  
	
	## PCA  - Ph√¢n t√≠ch th√†nh ph·∫ßn ch√≠nh(Principal Component Analysis )
		
		### √ù t∆∞·ªüng : PCA t√¨m c√°c tr·ª•c ch√≠nh(principal components) sao cho ƒë·∫øn khi chi·∫øu d·ªØ li·ªáu l√™n ch√∫ng, ta gi·ªØ ƒë∆∞·ª£c ph·∫ßn l·ªõn ph∆∞∆°ng sai d·ªØ c·ªßa d·ªØ li·ªáu .
		
		### C√°ch l√†m :	 
			1. Chu·∫©n h√≥a d·ªØ li·ªáu(mean = 0)
			2. T√≠nh ma tr·∫≠n hi·ªáp ph∆∞∆°ng sai S = 1/m . X^T X 								<* C·∫ßn t√¨m hi·ªÉu th√™m >
			3. T√≠nh vector ri√™ng(eigenvectors) v√† gi√° tr·ªã ri√™ng(eigenvalues) c·ªßa S  		<* C·∫ßn t√¨m hi·ªÉu th√™m >
			4. Ch·ªçn k vector ri√™ng c√≥ gi√° tr·ªã ri√™ng l·ªõn nh·∫•t ƒë·ªÉ t·∫°o kh√¥ng gian m·ªõi 
			
		C√¥ng th·ª©c √°nh x·∫° d·ªØ li·ªáu sang kh√¥ng gian gi·∫£m chi·ªÅu : 
			Z = X.Wk  
			
			X : L√† d·ªØ li·ªáu g·ªëc 
			Wk : l√† ma tr·∫≠n ch·ª©a k vector ri√™ng l·ªõn nh·∫•t 
			
			
	## SVD  - Ph√¢n t√≠ch gi√° tr·ªã k·ª≥ d·ªã(Singular Value Decomposition)
		SVD ph√¢n r√£ m·ªôt ma tr·∫≠n th√†nh ba ma tr·∫≠n kh√°c : 
			X = U. Z_ . V^t 
			
			Y v√† V l√† ma tr·∫≠n tr·ª±c giao 
			Z_ ch·ª©a c√°c gi√° tr·ªã k·ª≥ d·ªã s·∫Øp x·∫øp gi·∫£m d·∫ßn 
			
		-> Gi·ªØ l·∫°i k gi√° tr·ªã k·ª≥ d·ªã l·ªõn nh·∫•t v√† lo·∫°i b·ªè ph·∫ßn c√≤n l·∫°i gi√∫p gi·∫£m chi·ªÅu d·ªØ li·ªáu. 	
		-> PCA th·ª±c ch·∫•t l√† m·ªôt tr∆∞·ªùng h·ª£p ƒë·∫∑c bi·ªát c·ªßa SVD 
	
			
			
	## T-SNE & UMAP -  Gi·∫£m chi·ªÅu phi tuy·∫øn  
	
		PCA v√† SVD l√† tuy·∫øn t√≠nh. nh∆∞ng c√≥ nh·ªØng k·ªπ thu·∫≠t gi·∫£m chi·ªÅu phi tuy·∫øn m·∫°nh h∆°n nh∆∞ : 
			- T-SNE (t-Distributed Stochastic Neighbor Embedding): D√πng ph√¢n b·ªë x√°c su·∫•t ƒë·ªÉ b·∫£o to√†n c·∫•u tr√∫c d·ªØ li·ªáu 
			
			- UMAP (Uniform Manifold Approximation and Projection): Hi·ªáu qu·∫£ h∆°n T-SINE khi l√†m vi·ªác v·ªõi d·ªØ li·ªáu l·ªõn  
			

//--------------------- Gi·∫£m chi·ªÅu trong l·∫≠p tr√¨nh AI 
# S·ª≠ d·ª•ng th∆∞ vi·ªán Python h·ªó tr·ª£ 

	- Scikit-learn: PCA, TruncatedSVD ,  TSNE 
	- TensorFlow/Pytorch : D√πng SVD ho·∫∑c Autoencoder 
	
# Code minh h·ªça PCA trong Python 
		Gi·∫£ s·ª≠ ta c√≥ d·ªØ li·ªáu ng·∫´u nhi√™n t·ª´ 3 chi·ªÅu mu·ªën gi·∫£m xu·ªëng 2 chi·ªÅu  
		

Ph∆∞∆°ng ph√°p					Tuy·∫øn t√≠nh / Phi tuy·∫øn					Khi n√†o d√πng?
PCA							Tuy·∫øn t√≠nh							Khi d·ªØ li·ªáu c√≥ c·∫•u tr√∫c tuy·∫øn t√≠nh, d·ªÖ t√≠nh to√°n
SVD							Tuy·∫øn t√≠nh							T∆∞∆°ng t·ª± PCA, d√πng trong NLP (LSA)
T-SNE						Phi tuy·∫øn							Khi d·ªØ li·ªáu ph·ª©c t·∫°p, d√πng ƒë·ªÉ tr·ª±c quan h√≥a
UMAP						Phi tuy·∫øn							Khi c·∫ßn gi·∫£m chi·ªÅu nhanh h∆°n T-SNE


			
5. K·∫øt lu·∫≠n
Gi·∫£m chi·ªÅu d·ªØ li·ªáu l√† m·ªôt k·ªπ thu·∫≠t quan tr·ªçng trong to√°n cao c·∫•p v√† AI, gi√∫p t·ªëi ∆∞u h√≥a t√≠nh to√°n v√† l√†m m√¥ h√¨nh hi·ªáu qu·∫£ h∆°n.

N·∫øu mu·ªën gi·∫£m chi·ªÅu tuy·∫øn t√≠nh ‚Üí PCA, SVD
N·∫øu d·ªØ li·ªáu ph·ª©c t·∫°p ‚Üí T-SNE, UMAP
N·∫øu l√†m v·ªõi ·∫£nh, NLP ‚Üí SVD, Autoencoder		
//////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////	
//==========Lesson 3 == Lession 3 == Lession 3 == Ma tr·∫≠n ==========//
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////


	# Kh√°i ni·ªám : Ma tr·∫≠n l√† m·ªôt b·∫£ng g·ªìm c√°c s·ªë ƒë∆∞·ª£c s·∫Øp x·∫øp th√†nh h√†ng v√† c·ªôt. N√≥ c√≥ th·ªÉ xem nh∆∞ m·ªü r·ªông c·ªßa vector 
		
	# ·ª®ng d·ª•ng :
		- Bi·ªÉu di·ªÖn c√°c ph√©p bi·∫øn ƒë·ªïi (xoay, co d√£n trong ƒë·ªì h·ªça m√°y t√≠nh)
		- L∆∞u tr·ªØ d·ªØ li·ªáu l·ªõn(nh∆∞ ma tr·∫≠n t∆∞∆°ng ƒë·ªìng trong AI )
	
	# ƒê∆∞·ªùng ch√©o ch√≠nh 
		ƒê∆∞·ªùng ch√©o ch√≠nh c·ªßa m·ªôt ma tr·∫≠n l√† t·∫≠p h·ª£p c√°c ph·∫ßn t·ª≠ c√≥ ch·ªâ s·ªë h√†ng v√† ch·ªâ s·ªë c·ªôt b·∫±ng nhau. V√≠ d·ª• A[1][1] , A[2][2] , A[3][3],... A[n][n] 
		
		
	//----------------------------- Ma tr·∫≠n chuy·ªÉn v·ªã  
		# Kh√°i ni·ªám : 
			Gi·∫£ s·ª≠ c√≥ m·ªôt ma tr·∫≠n A k√≠ch th∆∞·ªõc m x n, nghƒ©a l√† m h√†ng v√† n c·ªôt, m√† tr·∫≠n chuy·ªÉn v·ªã c·ªßa A, k√Ω hi·ªáu l√† A^T (A m≈© T )
			ƒë∆∞·ª£c t·∫°o ra b·∫±ng c√°ch ho√°n ƒë·ªïi h√†ng v√† c·ªôt c·ªßa A 
				+ Ph·∫ßn t·ª≠ ·ªü v·ªã tr√≠ i, c·ªôt j trong A s·∫Ω tr·ªü th√†nh ph·∫ßn t·ª≠ v·ªã tr√≠ h√†ng j, c·ªôt i trong A^T(A m≈© T)
					... 
		# C√°ch hi·ªÉu ƒë∆°n gi·∫£n : 
			B·∫°n l·∫≠t ma tr·∫≠n qua ƒë∆∞·ªùng ch√©o ch√≠nh(ƒë∆∞·ªùng ch√©o t·ª´ g√≥c tr√™n b√™n tr√°i xu·ªëng g√≥c d∆∞·ªõi b√™n ph·∫£i  )
			H√†ng c·ªßa ma tr·∫≠n ban ƒë·∫ßu tr·ªü th√†nh c·ªôt c·ªßa ma tr·∫≠n chuy·ªÉn v·ªã v√† ng∆∞·ª£c l·∫°i 
			
			gi·ªëng nh∆∞ 1 t·ªù gi·∫•y, xoay 1 g√≥c 90 ƒë·ªô  sang b√™n tr√°i v√† l·∫≠t theo chi·ªÅu d·ªçc 180 ƒë·ªô 
			
		# K√≠ch th∆∞·ªõc c·ªßa ma tr·∫≠n chuy·ªÉn v·ªã  
			N·∫øu ma tr·∫≠n A c√≥ k√≠ch th∆∞·ªõc mxn th√¨ chuy·ªÉn v·ªã c·ªßa A (A^T)  s·∫Ω c√≥ k√≠ch th∆∞·ªõc n x m  
			
		# M·ªôt s·ªë t√≠nh ch·∫•t c·ªßa ma tr·∫≠n chuy·ªÉn v·ªã : 
			- Chuy·ªÉn v·ªã 2 l·∫ßn s·∫Ω tr·∫£ v·ªÅ ma tr·∫≠n ban ƒë·∫ßu 
			- Chuy·ªÉn v·ªã c·ªßa t·ªïng 2 ma tr·∫≠n b·∫±ng t·ªïng chuy·ªÉn v·ªã c·ªßa t·ª´ng ma tr·∫≠n  
			- Chuy·ªÉn v·ªã c·ªßa t√≠ch 2 ma tr·∫≠n b·∫±ng t√≠ch c·ªßa chuy·ªÉn v·ªã t·ª´ng ma tr·∫≠n , nh∆∞ng th·ª© t·ª± b·ªã ƒë·∫£o ng∆∞·ª£c 
			- N·∫øu A l√† ma tr·∫≠n vu√¥ng : N·∫øu A ƒë·ªëi x·ª©ng(A = A^T) th√¨ A ƒë∆∞·ª£c g·ªçi l√† ma tr·∫≠n ƒë·ªëi x·ª©ng 

		# ·ª®ng d·ª•ng : 
			- Chuy·ªÉn b·ªã gi√∫p t·ªï ch·ª©c l·∫°i d·ªØ li·ªáu, v√≠ d·ª•: bi·∫øn m·ªôt danh s√°ch gi√° tr·ªã t·ª´ h√†ng th√†nh c·ªôt ho·∫∑c ng∆∞·ª£c l·∫°i 
			- D√πng chuy·ªÉn v·ªã ƒë·ªÉ t√≠nh nghi·ªám trong c√°c b√†i to√°n li√™n quan ƒë·∫øn ma tr·∫≠n hi·ªáp ph∆∞∆°ng sai ho·∫∑c h·ªìi quy tuy·∫øn t√≠nh 

	//---------------------- ƒê·ªãnh th·ª©c c·ªßa ma tr·∫≠n  
	# Kh√°i ni·ªám : 
			ƒê·ªãnh th·ª©c l√† m·ªôt gi√° tr·ªã s·ªë ƒë∆∞·ª£c t√≠nh t·ª´ m·ªôt ma tr·∫≠n vu√¥ng(s·ªë h√†ng  =  s·ªë c·ªôt) v√† ƒë√≥ng vai tr√≤ quan tr·ªçng trong ƒë·∫°i s·ªë tuy·∫øn t√≠nh. ƒê·ªãnh th·ª©c gi√∫p x√°c ƒë·ªãnh c√°c t√≠nh ch·∫•t quan tr·ªçng c·ªßa ma tr·∫≠n ch·∫≥ng h·∫°n nh∆∞ : 
				Ma tr·∫≠n c√≥ kh·∫£ ngh·ªãch hay kh√¥ng 
				Ma tr·∫≠n c√≥ th·ªÉ ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ gi·∫£i h·ªá ph∆∞∆°ng tr√¨nh tuy·∫øn t√≠nh hay kh√¥ng  
				
			V√≠ d·ª• : Cho m·ªôt ma tr·∫≠n vu√¥ng A k√≠ch th∆∞·ªõc nxm. ƒê·ªãnh th·ª©c c·ªßa ma tr·∫≠n k√Ω hi·ªáu l√† det(A) ho·∫∑c |A|, l√† m·ªôt s·ªë ƒë∆∞·ª£c t√≠nh d·ª±a tr√™n c√°c ph·∫ßn t·ª≠ c·ªßa A 
			
	# C√°ch t√≠nh ƒë·ªãnh th·ª©c 
			## Ma tr·∫≠n 1x1 
				N·∫øu A = |a| th√¨ : 
					det(A) = a 
				V√≠ d·ª• : 
					A = |5| => det(A) = 5 
					
			## Ma tr·∫≠n 2x2 
				matrix A = [[a,b] , [c,d]]
				det(A) = ad - bc 
				
				V√≠ d·ª• : 
					A = [ [3,2] , [1,4]]
					det(A) = (3.4) - (2.1) = 12 - 2 = 10
					
					
			## Ma tr·∫≠n 3x3 
				cho ma tr·∫≠n  A = [ [a , b ,c] , [d , e , f] , [g , h , i] ]
				-> det(A) = a(ei-fh) -b(di-fg) + c(dh - eg)
				
				V√≠ d·ª• 
					A = [[1,2,3] , [4,5,6] , [7,8,9]]
					det(A) = 1.(5.9-6.8) - 2(4.9 - 6.7) + 3.(4.8 - 5.7) = 0 // ƒê·ªãnh th·ª©c b·∫±ng 0 cho n√™n ma tr·∫≠n n√†y KH√îNG KH·∫¢ NGH·ªäCH 
			
			
			## ƒê·ªãnh th·ª©c c·ªßa ma tr·∫≠n l·ªõn h∆°n 3x3 
			
			### S·ª≠ d·ª•ng ph∆∞∆°ng ph√°p Laplace 
				S·ª≠ d·ª•ng khai tri·ªÉn Laplace(Laplace expansion) ho·∫∑c c√°c thu·∫≠t to√°n kh√°c nh∆∞ gi·∫£m Gauss ƒë·ªÉ t√≠nh ƒë·ªãnh th·ª©c 
				Khai tri·ªÉn Laplace cho ph√©p t√≠nh ƒë·ªãnh th·ª©c c·ªßa m·ªôt ma tr·∫≠n vu√¥ng n x n b·∫±ng c√°ch chia nh·ªè th√†nh ƒë·ªãnh th·ª©c c·ªßa c√°c ma tr·∫≠n con (n - 1) x (n - 1)
					
					# C√¥ng th·ª©c : 
									... 
									
				Quy tr√¨nh th·ª±c hi·ªán : 
					1. Ch·ªçn m·ªôt h√†ng ho·∫∑c c·ªôt ƒë·ªÉ khai tri·ªÉn(th∆∞·ªùng ch·ªçn h√†ng ho·∫∑c c·ªôt c√≥ nhi·ªÅu ph·∫ßn t·ª≠ 0 ƒë·ªÉ gi·∫£m s·ªë ph√©p t√≠nh  )
					2. V·ªõi m·ªói ph·∫ßn t·ª≠ Aij trong h√†ng/ c·ªôt ƒë√£ ch·ªçn : 
						 - T√≠nh ma tr·∫≠n con Mij 
						 - T√≠nh ƒë·ªãnh th·ª©c c·ªßa Mij (b·∫±ng c√°ch ti·∫øp t·ª•c khai tri·ªÉn n·∫øu c·∫ßn )
					3. C·ªông ho·∫∑c tr·ª´ c√°c k·∫øt qu·∫£ theo c√¥ng th·ª©c
			
			
			### S·ª≠ d·ª•ng ph∆∞∆°ng ph√°p Gauss(Gi·∫£m Gauss)
				Ph∆∞∆°ng ph√°p Gauss gi·∫£m ma tr·∫≠n v·ªÅ d·∫°ng tam gi√°c tr√™n(Upper Triangular Matrix). Khi ma tr·∫≠n ·ªü d·∫°ng tam gi√°c tr√™n, ƒë·ªãnh th·ª©c b·∫±ng t√≠ch c·ªßa c√°c ph·∫ßn t·ª≠ tr√™n ƒë∆∞·ªùng ch√©o ch√≠nh  
				S·ª≠ d·ª•ng ph√©p bi·∫øn ƒë·ªïi c∆° s·ªü c·∫•p tr√™n h√†ng c·ªßa ma tr·∫≠n  
					
					Quy tr√¨nh th·ª±c hi·ªán 
						1. Bi·∫øn ƒë·ªïi h√†ng :  D√πng ph√©p bi·∫øn ƒë·ªïi h√†ng ƒë·ªÉ lo·∫°i b·ªè c√°c ph·∫ßn t·ª≠ d∆∞·ªõi ƒë∆∞·ªùng ch√©o ch√≠nh, ƒë∆∞a ma tr·∫≠n v·ªÅ d·∫°ng tam gi√°c tr√™n 
						
						2. T√≠nh ƒë·ªãnh th·ª©c : 
							ƒê·ªãnh th·ª©c b·∫±ng t√≠ch c√°c ph·∫ßn t·ª≠ tr√™n ƒë∆∞·ªùng ch√©o ch√≠nh, nh√¢n v·ªõi h·ªá s·ªë d·∫•u c·ªßa c√°c ph√©p ƒë·ªïi ch·ªó h√†ng n·∫øu c√≥ 
			
	# √ù nghƒ©a c·ªßa ƒë·ªãnh th·ª©c 
		## 1. X√°c ƒë·ªãnh ma tr·∫≠n c√≥ kh·∫£ ngh·ªãch hay kh√¥ng 
			N·∫øu det(A) != 0 : Ma tr·∫≠n kh·∫£ ngh·ªãch 
			n·∫øu det(A) == 0 : Ma tr·∫≠n KH√îNG kh·∫£ ngh·ªãch 
			
		## Gi·∫£i h·ªá ph∆∞∆°ng tr√¨nh tuy·∫øn t√≠nh : 
			N·∫øu ƒë·ªãnh th·ª©c c·ªßa ma tr·∫≠n h·ªá s·ªë != 0 , h·ªá ph∆∞∆°ng tr√¨nh c√≥ nghi·ªám duy nh·∫•t  
		## ·ª®ng d·ª•ng trong h√¨nh h·ªçc 	
			ƒê·ªãnh th·ª©c cho bi·∫øt di·ªán t√≠ch 2d ho·∫∑c th·ªÉ t√≠ch 3d c·ªßa c√°c h√¨nh do vector c·ªôt c·ªßa ma tr·∫≠n t·∫°o th√†nh 
			
			
			
			
	//------------------------ Ph√©p bi·∫øn ƒë·ªïi s∆° c·∫•p, trung c·∫•p, cao c·∫•p tr√™n h√†ng c·ªßa ma tr·∫≠n 	
		Trong to√°n h·ªçc, ph√©p bi·∫øn ƒë·ªïi s∆° c·∫•p, trung c·∫•p v√† cao c·∫•p tr√™n h√†ng c·ªßa ma tr·∫≠n l√† thao t√°c c∆° b·∫£n ƒë∆∞·ª£c th·ª±c hi·ªán ƒë·ªÉ bi·∫øn ƒë·ªïi m·ªôt ma tr·∫≠n v·ªÅ d·∫°ng ƒë∆°n gi·∫£n h∆°n m√† v·∫´n gi·ªØ ƒë∆∞·ª£c c√°c t√≠nh ch·∫•t quan tr·ªçng c·ªßa ma tr·∫≠n. ƒê√¢y l√† nh·ªØng ph√©p bi·∫øn ƒë·ªïi th∆∞·ªùng ƒë∆∞·ª£c s·ª≠ d·ª•ng khi gi·∫£i quy·∫øt c√°c b√†i to√°n v·ªÅ h·ªá ph∆∞∆°ng tr√¨nh tuy·∫øn t√≠nh, ƒë·∫∑c bi·ªát l√† khi s·ª≠ d·ª•ng ph∆∞∆°ng ph√°p kh·ª≠ Gauss(Gauss Emilination) ƒë·ªÉ t√¨m nghi·ªám 
		
# 1. Ph√©p bi·∫øn ƒë·ªïi s∆° c·∫•p tr√™n h√†ng c·ªßa ma tr·∫≠n 
	Ph√©p bi·∫øn ƒë·ªïi s∆° c·∫•p l√† c√°c thao t√°c ƒë∆°n gi·∫£n nh·∫•t v√† c∆° b·∫£n nh·∫•t m√† ta c√≥ th·ªÉ th·ª±c hi·ªán tr√™n c√°c h√†ng c·ªßa ma tr·∫≠n  
	
		## Thao t√°c 1: Ho√°n ƒë·ªïi 2 h√†ng  
		
		
		## Thao t√°c 2 : Nh√¢n ƒë√¥i m·ªôt h√†ng v·ªõi m·ªôt s·ªë kh√°c 0 
		
		
		## Thao t√°c 3 : C·ªông m·ªôt h√†ng v·ªõi m·ªôt b·ªôi s·ªë c·ªßa h√†ng kh√°c, th∆∞·ªùng ƒë∆∞·ª£c s·ª≠ d·ª•ng trong ph∆∞∆°ng ph√°p kh·ª≠ Gauss ƒë·ªÉ lo·∫°i b·ªè c√°c ph·∫ßn t·ª≠ d∆∞·ªõi ƒë∆∞·ªùng ch√©o ch√≠nh c·ªßa ma tr·∫≠n 
		-> Ph√©p bi·∫øn ƒë·ªïi n√†y kh√¥ng l√†m thay ƒë·ªïi gi√° tr·ªã c·ªßa h·ªá ph∆∞∆°ng tr√¨nh t∆∞∆°ng ·ª©ng v·ªõi ma tr·∫≠n, v√¨ n√≥ ch·ªâ l√†m thay ƒë·ªïi c√°ch bi·ªÉu di·ªÖn c·ªßa c√°c ph∆∞∆°ng tr√¨nh m√† kh√¥ng l√†m thay ƒë·ªïi nghi·ªám c·ªßa h·ªá . N√≥ th∆∞·ªùng ƒë∆∞·ª£c d√πng ƒë·ªÉ l√†m cho ma tr·∫≠n tr·ªü n√™n d·ªÖ d√†ng h∆°n ƒë·ªÉ gi·∫£i, v√≠ d·ª•, chuy·ªÉn ma tr·∫≠n v·ªÅ d·∫°ng b·∫≠c thang r√∫t g·ªçn  
			###	C√¥ng th·ª©c :  Ri -> Ri + k.Rj   
						Ri l√† h√†ng th·ª© i 
						Rj l√† h√†ng th·ª© j 
						k l√† m·ªôt h·∫±ng s·ªë(b·ªôi s·ªë  )
			

		### 	V√≠ d·ª• :
				A = [ [1 , 2, 3] , [4,5,6] , [7,8,9]]	// ma tr·∫≠n 3x3 
				
				V√≠ d·ª• ch√∫ng ta c·ªông -7 l·∫ßn h√†ng 1 v√†o h√†ng 3 ta c√≥ 
				A' = [ [1,2,3] , [4,5,6] , [0,-6,-12]]
		
		
# 2. Ph√©p bi·∫øn ƒë·ªïi trung c·∫•p  
	Ph√©p bi·∫øn ƒë·ªïi trung c·∫•p th∆∞·ªùng ƒë∆∞·ª£c s·ª≠ d·ª•ng khi b·∫°n mu·ªën chu·∫©n h√≥a m·ªôt h√†ng ho·∫∑c lo·∫°i b·ªè c√°c ph·∫ßn t·ª≠ d∆∞·ªõi ho·∫∑c tr√™n ƒë∆∞·ªùng ch√©o ch√≠nh c·ªßa ma tr·∫≠n 
	
	## a > Chia m·ªôt h√†ng cho m·ªôt s·ªë kh√°c 0 
		Khi b·∫°n chia m·ªôt h√†ng cho m·ªôt s·ªë kh√°c 0 , b·∫°n chu·∫©n h√≥a h√†ng ƒë√≥ sao cho ph·∫ßn t·ª≠ ƒë·∫ßu ti√™n(ho·∫∑c b·∫•t k·ª≥ ph·∫ßn t·ª≠ n√†o trong h√†ng) c√≥ gi√° tr·ªã b·∫±ng 1, gi√∫p ƒê∆†N GI·∫¢N H√ìA m√° tr·∫≠n  
				
				### V√≠ d·ª•  
					A = [ [1,2,3] , [4,5,6] , [7,8,9] ]
					
					-> Ta chia h√†ng 2 cho 5, ta c√≥ : 
						A ' = [ [1,2,3] , [0.8,1,1.2]  , [7,8,9] ]
		
		
	##  b >  C·ªông m·ªôt b·ªôi s·ªë c·ªßa m·ªôt h√†ng v√†o m·ªôt h√†ng kh√°c 
		C≈©ng gi·ªëng nh∆∞ ph√©p bi·∫øn ƒë·ªïi s∆° c·∫•p, nh∆∞ng ph√©p bi·∫øn ƒë·ªïi n√†y c√≥ th·ªÉ ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ ƒë·∫°t ƒë∆∞·ª£c c√°c m·ª•c ti√™u c·ª• th·ªÉ, ch·∫≥ng h·∫°n nh∆∞ l√†m cho c√°c ph·∫ßn t·ª≠ d∆∞·ªõi ƒë∆∞·ªùng ch√©o ch√≠nh b·∫±ng 0 trong ph∆∞∆°ng ph√°p kh·ª≠ Gauss 
				### 	V√≠ d·ª• :
				A = [ [1 , 2, 3] , [4,5,6] , [7,8,9]]	// ma tr·∫≠n 3x3 
				
				V√≠ d·ª• ch√∫ng ta c·ªông -7 l·∫ßn h√†ng 1 v√†o h√†ng 3 ta c√≥ 
				A' = [ [1,2,3] , [4,5,6] , [0,-6,-12]]
		

# 3. Ph√©p bi·∫øn ƒë·ªïi cao c·∫•p 
		Ph√©p bi·∫øn ƒë·ªïi cao c·∫•p gi√∫p b·∫°n ƒë∆∞a ma tr·∫≠n v·ªÅ d·∫°ng ƒë·∫∑c bi·ªát, nh∆∞ d·∫°ng tam gi√°c tr√™n ho·∫∑c d·∫°ng chu·∫©n d√≤ng  
		
	## a> Chuy·ªÉn ƒë·ªïi ma tr·∫≠n v·ªÅ d·∫°ng tam gi√°c tr√™n  
		Khi b·∫°n ƒë∆∞a ma tr·∫≠n v·ªÅ d·∫°ng tam gi√°c tr√™n, b·∫°n l√†m cho t·∫•t c·∫£ c√°c ph·∫ßn t·ª≠ d∆∞·ªõi ƒë∆∞·ªùng ch√©o ch√≠nh b·∫±ng 0 . ƒê√¢y l√† b∆∞·ªõc quan tr·ªçng trong vi·ªác gi·∫£i h·ªá ph∆∞∆°ng tr√¨nh tuy·∫øn t√≠nh, gi√∫p b·∫°n d·ªÖ d√†ng t√≠nh nghi·ªám b·∫±ng ph∆∞∆°ng ph√°p kh·ª≠ Gauss 
			
			### V√≠ d·ª• 
				A = [ [1,2,3] , [4,5,6] , [7,8,9] ]
				
				// C·ªông -4 l·∫ßn h√†ng 1 v√†o h√†ng 2 : 
					A' = [ [1,2,3] , [0,-3,-6], [7,8,9] ]
					
				// C·ªông -7 l·∫ßn h√†ng 1 v√†o h√†ng 3: 
					A' = [ [1,2,3] , [0,-3,-6], [ 0,-6,-12] ]
					
				// Chia h√†ng 2 cho -3 
					A' = [[1,2,3] , [0,1,2] , [0,-6,-12]]
					
				// C·ªông 6 l·∫ßn h√†ng 2 v√†o h√†ng 3 : 
					A' = [ [1,2,3] , [0,1,2]  , [0,0,0] ]
				
			L√∫c n√†y, ma tr·∫≠n ƒë√£ ·ªü d·∫°ng tam gi√°c tr√™n (t·∫•t c·∫£ ph·∫ßn t·ª≠ d∆∞·ªõi ƒë∆∞·ªùng ch√©o ch√≠nh ƒë·ªÅu l√† 0  )	
	


	##b> Chuy·ªÉn ƒë·ªïi ma tr·∫≠n v·ªÅ d·∫°ng chu·∫©n d√≤ng 
		D·∫°ng chu·∫©n d√≤ng l√† khi m·ªói h√†ng c·ªßa ma tr·∫≠n b·∫Øt ƒë·∫ßu b·∫±ng m·ªôt ph·∫ßn t·ª≠ 1(g·ªçi l√† ƒë·∫ßu d√≤ng) v√† c√°c ph·∫ßn t·ª≠ kh√°c trong c·ªôt n√†y l√† 0 ([ [1,x] , [0,y] , [0,z] ])
			
			## V√≠ d·ª• ma tr·∫≠n chu·∫©n d√≤ng : 
					A = [ [1,2,3] , [0,1,2] , [0,0,1] ]
			
	//---------------------- Ma tr·∫≠n ngh·ªãch ƒë·∫£o 
		## Kh√°i ni·ªám : 
			Ma tr·∫≠n A c√≥ k√≠ch th∆∞·ªõc n x n (ma tr·∫≠n vu√¥ng) ƒë∆∞·ª£c g·ªçi l√† kh·∫£ ngh·ªãch n·∫øu t·ªìn t·∫°i m·ªôt ma tr·∫≠n A-1 sao cho 
							A. A-1 = A-1 . A = I  (·ªü ƒë√¢y A-1 l√† a m≈© tr·ª´ 1 )
			
			Trong ƒë√≥, I l√† ma tr·∫≠n ƒë∆°n v·ªã(ma tr·∫≠n c√≥ gi√° tr·ªã tr√™n 1 ƒë∆∞·ªùng ch√©o ch√≠nh v√† 0 ·ªü c√°c v·ªã tr√≠ kh√°c). 
			
		## ƒêi·ªÅu ki·ªán t·ªìn t·∫°i : 
			- Ma tr·∫≠n A ph·∫£i l√† ma tr·∫≠n vu√¥ng 
			- ƒê·ªãnh th·ª©c det(A) c·ªßa ma tr·∫≠n A kh√°c 0 
			
		## ·ª®ng d·ª•ng : 
			T√¨m nghi·ªám h·ªá ph∆∞∆°ng tr√¨nh tuy·∫øn t√≠nh : N·∫øu A.x = b , nghi·ªám x c√≥ th·ªÉ t√≠nh b·∫±ng 
				x = A‚àí1 .b 
			
			T√≠nh to√°n kho·∫£ng c√°ch v√† ƒë·ªô t∆∞∆°ng ƒë·ªìng : Trong h·ªçc m√°y, ma tr·∫≠n ngh·ªãch ƒë·∫£o gi√∫p x·ª≠ l√Ω d·ªØ li·ªáu trong kh√¥ng gian nhi·ªÅu chi·ªÅu  
			
	
	//-------------------- Gi·∫£ ngh·ªãch ƒë·∫£o (Moore-Penrose Pseudoinverse Matrix)
	
	## Kh√°i ni·ªám : 
		Gi·∫£ ngh·ªãch ƒë·∫£o l√† m·ªôt kh√°i ni·ªám m·ªü r·ªông c·ªßa ma tr·∫≠n ngh·ªãch ƒë·∫£o, ƒë∆∞·ª£c s·ª≠ d·ª•ng khi ma tr·∫≠n kh√¥ng c√≥ ngh·ªãch ƒë·∫£o. ƒêi·ªÅu n√†y th∆∞·ªùng x·∫£y ra khi ma tr·∫≠n kh√¥ng ph·∫£i l√† ma tr·∫≠n vu√¥ng ho·∫∑c c√≥ ƒë·ªãnh th·ª©c b·∫±ng 0. 
		Gi·∫£ ngh·ªãch ƒë·∫£o c·ªßa m·ªôt ma tr·∫≠n A(g·ªçi l√† A+) c√≥ th·ªÉ ƒë∆∞·ª£c t√≠nh ngay c·∫£ khi ma tr·∫≠n kh√¥ng c√≥ ngh·ªãch ƒë·∫£o. M·ª•c ti√™u c·ªßa gi·∫£ ngh·ªãch ƒë·∫£o l√† t√¨m m·ªôt ma tr·∫≠n m√† khi nh√¢n v·ªõi ma tr·∫≠n A s·∫Ω "Gi·∫£i quy·∫øt" m·ªôt s·ªë v·∫•n ƒë·ªÅ trong vi·ªác gi·∫£i h·ªá ph∆∞∆°ng trinh ho·∫∑c t√¨m nghi·ªám g·∫ßn ƒë√∫ng .
		
	
		- D√πng ƒë·ªÉ x·ª≠ l√Ω c√°c ma tr·∫≠n kh√¥ng vu√¥ng ho·∫∑c kh√¥ng kh·∫£ ngh·ªãch 
		- Ma tr·∫≠n gi·∫£ ngh·ªãch ƒë·∫£o A+ l√† phi√™n b·∫£n m·ªü r·ªông c·ªßa ma tr·∫≠n ngh·ªãch ƒë·∫£o, th∆∞·ªùng ƒë∆∞·ª£c t√≠nh b·∫±ng ph∆∞∆°ng ph√°p ph√¢n t√≠ch gi√° tr·ªã k·ª≥ d·ªã SVD 
	
	## ·ª®ng d·ª•ng : 
		Gi·∫£i b√†i to√°n t·ªëi ∆∞u khi ma tr·∫≠n kh√¥ng vu√¥ng 
		Gi·∫£ ngh·ªãch ƒë·∫£o c√≥ r·∫•t nhi·ªÅu ·ª©ng d·ª•ng trong to√°n h·ªçc, ƒë·∫∑c bi·ªát trong c√°c b√†i to√°n li√™n quan ƒë·∫øn h·ªá ph∆∞∆°ng tr√¨nh tuy·∫øn t√≠nh, h·ªçc m√°y, v√† gi·∫£i quy·∫øt c√°c b√†i to√°n t·ªëi ∆∞u. N·∫øu b·∫°n kh√¥ng th·ªÉ gi·∫£i m·ªôt h·ªá ph∆∞∆°ng tr√¨nh v·ªõi ma tr·∫≠n b√¨nh th∆∞·ªùng(v√¨ ma tr·∫≠n kh√¥ng c√≥ ngh·ªãch ƒë·∫£o), b·∫°n c√≥ th·ªÉ s·ª≠ d·ª•ng gi·∫£ ngh·ªãch ƒë·∫£o ƒë·ªÉ t√¨m nghi·ªám g·∫ßn ƒë√∫ng. 
		
		
	## C√°ch t√≠nh gi·∫£ ngh·ªãch ƒë·∫£o(Moore-Penrose Pseudoinverse)
		Gi·∫£ ngh·ªãch ƒë·∫£o c·ªßa m·ªôt ma tr·∫≠n A c√≥ th·ªÉ ƒë∆∞·ª£c t√≠nh b·∫±ng c√°c c√¥ng th·ª©c ph·ª©c t·∫°p, t√πy thu·ªôc v√†o t√≠nh ch·∫•t c·ªßa ma tr·∫≠n ƒë√≥. M·ªôt trong nh·ªØng c√°ch ph·ªï bi·∫øn ƒë·ªÉ t√≠nh gi·∫£ ngh·ªãch ƒë·∫£o l√† th√¥ng qua ph√¢n t√≠ch gi√° tr·ªã k·ª≥ d·ªã(Singular Value Decomposition - SVD). Tuy nhi√™n, vi·ªác t√≠nh to√°n c·ª• th·ªÉ th∆∞·ªùng ƒë∆∞·ª£c th·ª±c hi·ªán b·∫±ng ph·∫ßn m·ªÅm t√≠nh to√°n nh∆∞ MATLAB ho·∫∑c Python v·ªõi th∆∞ vi·ªán NumPy
		
	## T√≥m t·∫Øt 
		- Ma tr·∫≠n ngh·ªãch ƒë·∫£o l√† ma tr·∫≠n A-1 sao cho A x A-1 = I v√† ch·ªâ t·ªìn t·∫°i khi ƒë·ªãnh th·ª©c c·ªßa ma tr·∫≠n A kh√°c 0 (t·ª©c l√† A nh√¢n gi·∫£ ngh·ªãch ƒë·∫£o s·∫Ω t·∫°o ra ma tr·∫≠n ƒë∆°n v·ªã)
		
		- Gi·∫£ ngh·ªãch ƒë·∫£o ƒë∆∞·ª£c s·ª≠ d·ª•ng khi ma tr·∫≠n kh√¥ng c√≥ ngh·ªãch ƒë·∫£o, gi√∫p t√≠nh to√°n g·∫ßn ƒë√∫ng trong c√°c b√†o to√°n t·ªëi ∆∞u v√† h·ªçc m√°y. 
		
		- ·ª®ng d·ª•ng c·ªßa gi·∫£ ngh·ªãch ƒë·∫£o r·∫•t quan tr·ªçng trong vi·ªác gi·∫£i c√°c b√†i to√°n c√≥ ma tr·∫≠n kh√¥ng kh·∫£ ngh·ªãch, v√≠ d·ª• nh∆∞ trong h·ªçc m√°y khi c·∫ßn t√¨m c√°ch t·ªët nh·∫•t ƒë·ªÉ d·ª± ƒëo√°n ho·∫∑c ƒëi·ªÅu ch·ªânh m√¥ h√¨nh.
		
	# V√≠ d·ª• s·ª≠ d·ª•ng th∆∞ vi·ªán numpy ƒë·ªÉ t√≠nh to√°n gi·∫£ ng·ªãch ƒë·∫£o  

			import numpy as np
			# Ma tr·∫≠n A
			A = np.array([[1, 2], [3, 4]])
			
			# T√≠nh gi·∫£ ngh·ªãch ƒë·∫£o c·ªßa A
			A_plus = np.linalg.pinv(A)
			
			print(A_plus)
		
//------------------- Ma tr·∫≠n ƒë∆°n v·ªã (Identity Matrix), k√Ω hi·ªáu l√† I 
		## Kh√°i ni·ªám : 
			I L√† m·ªôt ma tr·∫≠n vu√¥ng v·ªõi t·∫•t c·∫£ c√°c ph·∫ßn t·ª≠ tr√™n ƒë∆∞·ªùng ch√©o ch√≠nh b·∫±ng 1 v√† c√°c ph·∫ßn t·ª≠ c√≤n l·∫°i b·∫±ng 0 , v√≠ d·ª• m√† tr·∫≠n 2x2 , ma tr·∫≠n ƒë∆°n v·ªã l√† 
				I = [[1,0], [0,1] ]
		ƒêi·ªÅu n√†y c√≥ nghƒ©a l√† n·∫øu b·∫°n nh√¢n m·ªôt ma tr·∫≠n A v·ªõi ngh·ªãch ƒë·∫£o c·ªßa n√≥, b·∫°n s·∫Ω thu ƒë∆∞·ª£c ma tr·∫≠n ƒë∆°n v·ªã, t∆∞∆°ng t·ª± nh∆∞ vi·ªác nh√¢n m·ªôt s·ªë v·ªõi 1/x ƒë·ªÉ ƒë∆∞·ª£c 1 
		
		Tuy nhi√™n, kh√¥ng ph·∫£i t·∫•t c·∫£ c√°c ma tr·∫≠n ƒë·ªÅu c√≥ ngh·ªãch ƒë·∫£o. M·ªôt ma tr·∫≠n ch·ªâ c√≥ ngh·ªãch ƒë·∫£o khi ƒë·ªãnh th·ª©c c·ªßa n√≥ kh√°c 0 . ƒê·ªãnh th·ª©c l√† m·ªôt gi√° tr·ªã li√™n quan ƒë·∫øn ma tr·∫≠n v√† n·∫øu ƒë·ªãnh th·ª©c b·∫±ng 0, ma tr·∫≠n ƒë√≥ kh√¥ng c√≥ ngh·ªãch ƒë·∫£o .
	
			
			
		
	
		
//==========Lesson 4 == Lession 4 == Lession 4 == T√≠ch ma tr·∫≠n(Matrix multiplication) ==========//
	# Kh√°i ni·ªám : Ph√©p nh√¢n gi·ªØa hai ma tr·∫≠n ho·∫∑c gi·ªØa ma tr·∫≠n v√† vector, t·∫°o ra m·ªôt ma tr·∫≠n ho·∫∑c vector m·ªõi 
	
	# ·ª®ng d·ª•ng : 
		Bi·∫øn ƒë·ªïi d·ªØ li·ªáu v√† x√¢y d·ª±ng m√¥ h√¨nh AI 
		
		
//==========Lesson 5 == Lession 5 == Lession 5 == ƒê·ªãnh th·ª©c (Determinant) ==========//
	# Kh√°i ni·ªám : M·ªôt s·ªë ƒë·∫∑c tr∆∞ng cho ma tr·∫≠n vu√¥ng (M x N , M == N), cho bi·∫øt ma tr·∫≠n c√≥ ngh·ªãch ƒë·∫£o hay kh√¥ng , n·∫øu ma tr·∫≠n vu√¥ng th√¨ s·∫Ω c√≥ ngh·ªãch ƒë·∫£o 
		V√≠ d·ª• : V·ªõi A = [ [1 , 2] , [3 , 4]  ], ƒë·ªãnh th·ª©c det(A) = 1.4 - 2.3 = -2 
		
	# ·ª®ng d·ª•ng : 
		Ki·ªÉm tra ma tr·∫≠n c√≥ kh·∫£ nƒÉng ngh·ªãch hay kh√¥ng, ho·∫∑c trong gi·∫£i ph∆∞∆°ng tr√¨nh tuy·∫øn t√≠nh  
		
//==========Lesson 6 == Lession 6 == Lession 6 == Gi√° tr·ªã ri√™ng v√† Vector ri√™ng(Eigenvalues & Eigenvector) ==========//
	# Kh√°i ni·ªám : 
		Gi√° tr·ªã ri√™ng : C√°c s·ªë ƒë·∫∑c bi·ªát li√™n quan ƒë·∫øn ma tr·∫≠n khi n√≥ bi·∫øn ƒë·ªïi kh√¥ng gian vector 
		Vector ri√™ng : C√°c vector kh√¥ng thay ƒë·ªïi h∆∞·ªõng khi b·ªã bi·∫øn ƒë·ªïi b·ªüi ma tr·∫≠n 
		
	# ·ª®ng d·ª•ng : 
		Gi·∫£m k√≠ch th∆∞·ªõc d·ªØ li·ªáu trong AI(nh∆∞ PCA -  Ph√¢n t√≠ch th√†nh ph·∫ßn ch√≠nh )
		
T·∫°i sao h·ªçc ƒë·∫°i s·ªë tuy·∫øn t√≠nh l·∫°i quan tr·ªçng trong AI ? 
	- D·ªØ li·ªáu : H·∫ßu h·∫øt d·ªØ li·ªáu AI ƒë∆∞·ª£c bi·ªÉu di·ªÖn b·∫±ng vector ho·∫∑c ma tr·∫≠n . 
	- M√¥ h√¨nh : C√°c m√¥ h√¨nh h·ªçc m√°y ƒë·ªÅu d·ª±a tr√™n c√°c ph√©p tuy·∫øn t√≠nh(nh∆∞ ma tr·∫≠n tr·ªçng s·ªë trong m·∫°ng neural )
	- Gi·∫£i quy·∫øt b√†i to√°n l·ªõn :  ƒê·∫°i s·ªë tuy·∫øn t√≠nh gi√∫p t·ªëi ∆∞u h√≥a v√† ph√¢n t√≠ch d·ªØ li·ªáu nhanh ch√≥ng.
	
	
		
		
		
//==========Lesson 7 == Lession 7 == Lession 7 == T√≠nh ma tr·∫≠n hi·ªáp ph∆∞∆°ng sai ==========//
	
	# Kh√°i ni·ªám 
		Ma tr·∫≠n hi·ªáp ph∆∞∆°ng sai (Convariance Matrix) l√† m·ªôt c·∫•u tr√∫c to√°n h·ªçc gi√∫p ƒëo l∆∞·ªùng m·ª©c ƒë·ªô li√™n h·ªá tuy·∫øn t√≠nh gi·ªØa nhi·ªÅu bi·∫øn s·ªë trong m·ªôt t·∫≠p d·ªØ li·ªáu. N√≥ m·ªü r·ªông kh√°i ni·ªám ph∆∞∆°ng sai c·ªßa m·ªôt bi·∫øn ƒë∆°n sang kh√¥ng gian nhi·ªÅu chi·ªÅu. 
			- N·∫øu c√≥ n bi·∫øn s·ªë(ƒë·∫∑c tr∆∞ng) trong m·ªôt t·∫≠p d·ªØ li·ªáu, ma tr·∫≠n hi·ªáp ph∆∞∆°ng sai s·∫Ω c√≥ k√≠ch th∆∞·ªõc n x n 
			- M·ªói ph·∫ßn t·ª≠ Cij trong ma tr·∫≠n bi·ªÉu th·ªã m·ªëi quan h·ªá gi·ªØa b·∫øn Xi v√† Xj 
			
	# C√¥ng th·ª©c t√≠nh to√°n  
		Gi·∫£ s·ª≠ c√≥ m·ªôt t·∫≠p d·ªØ li·ªáu v·ªõi m m·∫´u, m·ªói m·∫´u c√≥ n ƒë·∫∑c tr∆∞ng : 
			X = [  [X11 , X12 , ... X1n] , [X21 , X22 , ... , X2n] , [...] , [Xm1 , Xm2 , ...,Xmn ] ]
			
			M·ªói c·ªôt Xj l√† m·ªôt bi·∫øn s·ªë ng·∫´u nhi√™n. Ta t√≠nh gi√° tr·ªã trung b√¨nh c·ªßa t·ª´ng bi·∫øn: 
				... Œºj‚Äã\=m1‚Äãi\=1‚àëm‚ÄãXij 
				
			Hi·ªáp ph∆∞∆°ng sai gi·ªØa hai bi·∫øn Xi v√† Xj ƒë∆∞·ª£c t√≠nh b·∫±ng 	
					Cij‚Äã\=m‚àí11‚Äãk\=1‚àëm‚Äã(Xki‚Äã‚àíŒºi‚Äã)(Xkj‚Äã‚àíŒºj‚Äã) 
					
			D·∫°ng ma tr·∫≠n 
					C\=m‚àí11‚Äã(X‚àíŒº)T(X‚àíŒº) 
					
			v·ªõi Œº l√† ma tr·∫≠n trung b√¨nh 
			
	
	# ƒê·∫∑c ƒëi·ªÉm quan tr·ªçng c·ªßa ma tr·∫≠n hi·ªáp ph∆∞∆°ng sai 
		- ƒê·ªëi x·ª©ng  Cij‚Äã = Cji‚Äã  nghƒ©a l√† quan h·ªá gi·ªØa hai bi·∫øn kh√¥ng ph·ª• thu·ªôc v√†o th·ª© t·ª± 
		- ƒê∆∞·ªùng ch√©o l√† ph∆∞∆°ng sai : Cij ch√≠nh l√† ph∆∞∆°ng sai c·ªßa bi·∫øn Xi 
		- C√°c gi√° tr·ªã ngo√†i ƒë∆∞·ªùng ch√©o th·ªÉ hi·ªán m·ª©c ƒë·ªô li√™n h·ªá: 
					
					N·∫øu Cij > 0  : hai bi·∫øn tƒÉng c√πng nhau 
					N·∫øu Cij < 0  : M·ªôt bi·∫øn tƒÉng th√¨ bi·∫øn kia gi·∫£m 
					N·∫øu Cij = 0 , ch√∫ng kh√¥ng c√≥ m·ªëi quan h·ªá tuy·∫øn t√≠nh 
					

	# ·ª®ng d·ª•ng trong AI v√† Machine Learning 
		## 1. Gi·∫£m chi·ªÅu d·ªØ li·ªáu v·ªõi PCA (Principal Component Analysis)
			PCA s·ª≠ d·ª•ng ma tr·∫≠n hi·ªáp ph∆∞∆°ng sai ƒë·ªÉ t√¨m c√°c h∆∞·ªõng c√≥ ph∆∞∆°ng sai l·ªõn nh·∫•t trong d·ªØ li·ªáu v√† chi·∫øu d·ªØ li·ªáu v·ªÅ kh√¥ng gian c√≥ √≠t chi·ªÅu h∆°n. Qu√° tr√¨nh ch√≠nh : 
				- T√≠nh m√† tr·∫≠n hi·ªáp ph∆∞∆°ng sai c·ªßa d·ªØ li·ªáu ƒë·∫ßu v√†o 
				- T√¨m c√°c vector ri√™ng(eigenvectors) v√† gi√° tr·ªã ri√™ng(eigenvalues) c·ªßa ma tr·∫≠n 
				- Ch·ªçn c√°c th√†nh ph·∫ßn ch√≠nh(principal components) d·ª±a tr√™n gi√° tr·ªã ri√™ng l·ªõn nh·∫•t. 
				- Chi·∫øu d·ªØ li·ªáu sang kh√¥ng gian m·ªõi c√≥ s·ªë chi·ªÅu nh·ªè h∆°n nh∆∞ng v·∫´n gi·ªØ ƒë∆∞·ª£c th√¥ng tin quan tr·ªçng 
				
			PCA gi√∫p gi·∫£m noise v√† t·ªëi ∆∞u hi·ªáu su·∫•t c·ªßa c√°c m√¥ h√¨nh h·ªçc m√°y 
			
		## 2. Tr·ª±c quan h√≥a d·ªØ li·ªáu ƒëa chi·ªÅu 
			Khi d·ªØ li·ªáu c√≥ h√†ng trƒÉm ho·∫∑c h√†ng ng√†n ƒë·∫∑c tr∆∞ng, vi·ªác ph√¢n t√≠ch tr·ª±c ti·∫øp d·ªØ li·ªáu tr·ªü n√™n kh√≥ khƒÉn. PCA d·ª±a tr√™n ma tr·∫≠n hi·ªáp ph∆∞∆°ng sai c√≥ th·ªÉ gi·∫£m s·ªë chi·ªÅu xu·ªëng 2 ho·∫∑c 3 ƒë·ªÉ tr·ª±c quan h√≥a d·ªØ li·ªáu.
			
			ƒêi·ªÅu n√†y r·∫•t h·ªØu √≠ch khi ph√¢n t√≠ch d·ªØ li·ªáu h√¨nh ·∫£nh, √¢m thanh ho·∫∑c x·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n(NLP)
			
		##3 .Ph√°t hi·ªán t√≠nh ƒëa c·ªông tuy·∫øn t√≠nh trong h·ªìi quy tuy·∫øn t√≠nh 
			- N·∫øu hai bi·∫øn c√≥ hi·ªáp ph∆∞∆°ng sai l·ªõn, ch√∫ng c√≥ th·ªÉ ch·ª©a th√¥ng tin tr√πng l·∫∑p 
			- Khi x√¢y d·ª±ng m√¥ h√¨nh h·ªìi quy tuy·∫øn t√≠nh, ƒëi·ªÅu n√†y c√≥ th·ªÉ g√¢y ra overfitting. 
			- B·∫±ng c√°ch ki·ªÉm tra ma tr·∫≠n  hi·ªáp ph∆∞∆°ng sai, ta c√≥ th·ªÉ lo·∫°i b·ªè ho·∫∑c k·∫øt h·ª£p nh·ªØng bi·∫øn t∆∞∆°ng quan m·∫°nh ƒë·ªÉ t·ªëi ∆∞u m√¥ h√¨nh. 
			
		
		## 4. X√¢y d·ª±ng Gaussion Naive Bayers classifier 
			Trong b√†i to√°n ph√¢n lo·∫°i, gi·∫£ ƒë·ªãnh r·∫±ng d·ªØ li·ªáu tu√¢n theo ph√¢n ph·ªëi ƒëa bi·∫øn. 
			Ma tr·∫≠n hi·ªáp ph∆∞∆°ng sai gi√∫p ∆∞·ªõc l∆∞·ª£ng ph√¢n ph·ªëi n√†y ƒë·ªÉ t√≠nh to√°n x√°c su·∫•t c·ªßa t·ª´ng l·ªõp. 


			3. T√≠nh vector ri√™ng(eigenvectors) v√† gi√° tr·ªã ri√™ng(eigenvalues) c·ªßa S  		<* C·∫ßn t√¨m hi·ªÉu th√™m >		


//==========Lesson 8 == Lession 8 == Lession 8 == EigenVector(Vector ri√™ng)& EigenValue(Gi√° tr·ªã ri√™ng) ==========//

# Kh√°i ni·ªám 
	T∆∞·ªüng t∆∞·ª£ng b·∫°n c√≥ m·ªôt ph√©p bi·∫øn ƒë·ªïi tuy·∫øn t√≠nh A (t·ª©c l√† m·ªôt ma tr·∫≠n vu√¥ng) t√°c ƒë·ªông l√™n vector v /. 
	
	Th√¥ng th∆∞·ªùng, khi nh√¢n m·ªôt ma tr·∫≠n v·ªõi m·ªôt vector, h∆∞·ªõng c·ªßa vector c√≥ th·ªÉ b·ªã thay ƒë·ªïi. Nh∆∞ng n·∫øu c√≥ m·ªôt s·ªë vector ƒë·∫∑c bi·ªát m√† sau khi b·ªã bi·∫øn ƒë·ªïi v·∫´n gi·ªØ nguy√™n h∆∞·ªõng(ch·ªâ b·ªã k√©o d√£n ho·∫∑c thu nh·ªè l·∫°i), th√¨ nh·ªØng vector ƒë√≥ ƒë∆∞·ª£c g·ªçi l√† eigenvector (vector ri√™ng) v√† ƒë·ªô k√©o d√£n, thu nh·ªè ƒë∆∞·ª£c g·ªçi l√† gi√° tr·ªã ri√™ng(eigenvalues)
	
	
# C√¥ng th·ª©c to√°n h·ªçc 
		
		Av = Œªv 
		
	Trong ƒë√≥ 
	
		A l√† ma tr·∫≠n bi·∫øn ƒë·ªïi 
		v l√† vector ri√™ng 
		Œª l√† gi√° tr·ªã ri√™ng t∆∞∆°ng ·ª©ng v·ªõi vectir ri√™ng v 
		
# V√≠ d·ª• tr·ª±c quan 
		
	Gi·∫£ s·ª≠ c√≥ m·ªôt ma tr·∫≠n 
		
		
		A = [[2,0] , [0,3] ]
		
	v√† vector v 
		v = [1,0]
		
	
	Khi nh√¢n A v·ªõi v : 
		
		A v = [2,0]
		
	ta th·∫•y r·∫±ng vector k·∫øt qu·∫£ ch·ªâ b·ªã nh√¢n v·ªõi 2 nh∆∞ng kh√¥ng ƒë·ªïi h∆∞·ªõng, t·ª©c v l√† m·ªôt vector ri√™ng c·ªßa A v·ªõi gi√° tr·ªã ri√™ng Œª=2. 
	
	
# T·∫°i sao AI c·∫ßn eigenvalues  v√† eigenvectors 
	
	## Gi·∫£m chi·ªÅu d·ªØ li·ªáu (PCA - Principal Component Analysis )
		Khi x·ª≠ l√Ω d·ªØ li·ªáu l·ªõn(h√¨nh ·∫£nh, vƒÉn b·∫£n, d·ªØ li·ªáu t√†i ch√≠nh...) AI c·∫ßn gi·∫£m s·ªë l∆∞·ª£ng ƒë·∫∑c tr∆∞ng m√† v·∫´n gi·ªØ ƒë∆∞·ª£c th√¥ng tin quan tr·ªçng.
			
			- PCA d√πng eigenvectors c·ªßa ma tr·∫≠n hi·ªáp ph∆∞∆°ng sai ƒë·ªÉ t√¨m tr·ª•c ch√≠nh c·ªßa d·ªØ li·ªáu. 
			- D·ªØ li·ªáu ƒë∆∞·ª£c chi·∫øu l√™n tr·ª•c n√†y gi√∫p gi·∫£m chi·ªÅu v√† tƒÉng t·ªëc ƒë·ªô t√≠nh to√°n. 
			
	
	### X·ª≠ l√Ω h√¨nh ·∫£nh v√† nh·∫≠n di·ªán khu√¥n m·∫∑t 
		Eigenfaces l√† m·ªôt ph∆∞∆°ng ph√°p s·ª≠ d·ª•ng eigenvectors c·ªßa ·∫£nh ƒë·ªÉ nh·∫≠n di·ªán khu√¥n m·∫∑t 
		
	
	## H·ªçc s√¢u (Deep Learning) v√† ma tr·∫≠n tr·ªçng s·ªë 
	
		Trong m·∫°ng n∆°-ron eigenvalues gi√∫p ph√¢n t√≠ch s·ª± h·ªôi t·ª• c·ªßa c√°c thu·∫≠t to√°n t·ªëi ∆∞u h√≥a 
		
		

# C√°ch t√≠ch eigenvalues v√† eigenvectors 

	## ƒê·ªÉ t√¨m eigenvalues, ta gi·∫£i ph∆∞∆°ng tr√¨nh ƒë·∫∑c tr∆∞ng : 
		
		det(A - ŒªI) = 0
		
		
	## C√≤n eigenvectors ƒë∆∞·ª£c t√¨m b·∫±ng c√°ch gi·∫£i h·ªá ph∆∞∆°ng tr√¨nh(A - ŒªI)v = 0
	
		


# T·ªïng k·∫øt 
	
	- Eigenvectors l√† nh·ªØng vectgor kh√¥ng thay ƒë·ªïi h∆∞·ªõng sau ph√©p bi·∫øn ƒë·ªïi tuy·∫øn t√≠nh 
	- Eigenvalues l√† h·ªá s·ªë co gi√£n c·ªßa eigenvectors 
	- Ch√∫ng c√≥ ·ª©ng d·ª•ng quan tr·ªçng trong PCA, x·ª≠ l√Ω ·∫£nh, AI v√† h·ªçc s√¢u 
	
	










		
</pre><a id='backBottom' href='../AI-math-learning-list.html' style='display:none;'>üîô Quay l·∫°i danh s√°ch</a><br><button onclick='toggleTheme()'>üåô Chuy·ªÉn giao di·ªán</button></div><script>function toggleTheme() {   let mode = document.body.classList.contains('dark-mode') ? 'light-mode' : 'dark-mode';   document.body.className = mode; localStorage.setItem('theme', mode);   syncTheme();}function applyTheme() {   let savedTheme = localStorage.getItem('theme') || 'dark-mode';   document.body.className = savedTheme;   syncTheme();}function syncTheme() {   let preElement = document.querySelector('pre');   if (document.body.classList.contains('dark-mode')) { preElement.style.background = '#1e1e1e'; preElement.style.color = '#e0e0e0'; }   else { preElement.style.background = '#f5f5f5'; preElement.style.color = '#333333'; }}function checkPageHeight() {   let contentHeight = document.body.scrollHeight;   let windowHeight = window.innerHeight;   if (contentHeight > windowHeight * 1.2) {       document.getElementById('backBottom').style.display = 'block';   } else {       document.getElementById('backBottom').style.display = 'none';   }}</script></body></html>