<html><head><title>Lesson 20 == Xá»­ lÃ½ ngoon ngá»¯ NLP : Ä‘á»™t phÃ¡ phÃ¢n tÃ­ch vÄƒn báº£n báº±ng AI ==========//</title><style>body { font-family: Arial, sans-serif; transition: background 0.3s, color 0.3s; }.dark-mode { background-color: #121212; color: #e0e0e0; }.light-mode { background-color: #ffffff; color: #333333; }h1 { text-align: center; color: #73d9f5; }pre { padding: 15px; border-radius: 5px;       white-space: pre-wrap; word-wrap: break-word;       overflow-x: auto; max-width: 100%;       transition: background 0.3s, color 0.3s; }.dark-mode pre { background: #1e1e1e; color: #e0e0e0; }.light-mode pre { background: #f5f5f5; color: #333333; }#backTop, #backBottom {    font-size: 2em; padding: 20px 40px;    background: #bb86fc; color: white; text-decoration: none;    border-radius: 10px; display: inline-block; text-align: center; }#backTop:hover, #backBottom:hover { background: #9b67e2; }button { font-size: 1.5em; padding: 15px 30px;    background: #03dac6; color: #121212; border: none;    cursor: pointer; border-radius: 5px; display: block; margin: 10px auto; }button:hover { background: #02b8a3; }.dark-mode a { color: #03dac6; } .light-mode a { color: #007bff; }</style></head><body onload='applyTheme(); checkPageHeight()'><div class='container'><a id='backTop' href='../AI-learning-list.html'>ğŸ”™ Quay láº¡i danh sÃ¡ch</a><br><h1>Lesson 20 -- Xá»­ lÃ½ ngoon ngá»¯ NLP : Ä‘á»™t phÃ¡ phÃ¢n tÃ­ch vÄƒn báº£n báº±ng AI -//</h1><pre>
# Giá»›i thiá»‡u 
	Xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn (Natural Language Processing â€“ NLP) lÃ  má»™t lÄ©nh vá»±c thuá»™c trÃ­ tuá»‡ nhÃ¢n táº¡o (AI) chuyÃªn nghiÃªn cá»©u cÃ¡ch thá»©c mÃ¡y tÃ­nh tÆ°Æ¡ng tÃ¡c vá»›i ngÃ´n ngá»¯ con ngÆ°á»i. NLP táº­p trung vÃ o viá»‡c phÃ¡t triá»ƒn cÃ¡c thuáº­t toÃ¡n vÃ  mÃ´ hÃ¬nh giÃºp mÃ¡y tÃ­nh cÃ³ thá»ƒ hiá»ƒu, phÃ¢n tÃ­ch, táº¡o ra vÃ  pháº£n há»“i ngÃ´n ngá»¯ tá»± nhiÃªn cá»§a con ngÆ°á»i. Tá»« viá»‡c hiá»ƒu cÃ¡c lá»‡nh thoáº¡i trong trá»£ lÃ½ áº£o nhÆ° Siri vÃ  Google Assistant, Ä‘áº¿n dá»‹ch ngÃ´n ngá»¯ tá»± Ä‘á»™ng qua cÃ¡c cÃ´ng cá»¥ nhÆ° Google Dá»‹ch, hay phÃ¢n tÃ­ch cáº£m xÃºc vÃ  Ã½ kiáº¿n tá»« cÃ¡c bÃ i viáº¿t trÃªn máº¡ng xÃ£ há»™i, NLP Ä‘ang trá»Ÿ thÃ nh má»™t pháº§n khÃ´ng thá»ƒ thiáº¿u trong cuá»™c sá»‘ng vÃ  cÃ´ng viá»‡c hiá»‡n Ä‘áº¡i.
	
	
# CÃ¡c thÃ nh pháº§n chÃ­nh cá»§a NLP 
	Xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn (NLP) bao gá»“m nhiá»u bÆ°á»›c vÃ  phÆ°Æ¡ng phÃ¡p khÃ¡c nhau Ä‘á»ƒ phÃ¢n tÃ­ch vÃ  hiá»ƒu ngÃ´n ngá»¯ cá»§a con ngÆ°á»i. DÆ°á»›i Ä‘Ã¢y lÃ  cÃ¡c thÃ nh pháº§n chÃ­nh thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng trong quy trÃ¬nh xá»­ lÃ½ NLP:
	
	## 2.1. PhÃ¢n tÃ­ch cÃº phÃ¡p (Syntax Analysis) 
		PhÃ¢n tÃ­ch cÃº phÃ¡p lÃ  quÃ¡ trÃ¬nh xÃ¡c Ä‘á»‹nh cáº¥u trÃºc ngá»¯ phÃ¡p cá»§a cÃ¢u, Ä‘áº£m báº£o ráº±ng cÃ¢u Ä‘Ã³ tuÃ¢n thá»§ cÃ¡c quy táº¯c ngá»¯ phÃ¡p cá»§a ngÃ´n ngá»¯. BÆ°á»›c nÃ y thÆ°á»ng bao gá»“m: 
		
		### XÃ¡c Ä‘á»‹nh cÃ¡c thÃ nh pháº§n cá»§a cÃ¢u 
			Chá»§ ngá»¯, Ä‘á»™ng tá»«, tÃ¢n ngá»¯, v.v. 
			
		### PhÃ¢n tÃ­ch cáº¥u trÃºc cÃ¢y (parse tree) 
			Cáº¥u trÃºc cÃ¢y biá»ƒu diá»…n cÃ¡ch cÃ¡c tá»« trong cÃ¢u káº¿t há»£p vá»›i nhau dá»±a trÃªn ngá»¯ phÃ¡p. 
			
		### VÃ­ dá»¥ 
			CÃ¢u â€œCon mÃ¨o báº¯t con chuá»™tâ€ sáº½ Ä‘Æ°á»£c phÃ¢n tÃ­ch cÃº phÃ¡p thÃ nh má»™t cÃ¢y, trong Ä‘Ã³ â€œCon mÃ¨oâ€ lÃ  chá»§ ngá»¯, â€œbáº¯tâ€ lÃ  Ä‘á»™ng tá»«, vÃ  â€œcon chuá»™tâ€ lÃ  tÃ¢n ngá»¯.
			
		### Ã nghÄ©a 
			PhÃ¢n tÃ­ch cÃº phÃ¡p giÃºp hiá»ƒu cáº¥u trÃºc ngá»¯ phÃ¡p cá»§a cÃ¢u vÃ  Ä‘Ã³ng vai trÃ² quan trá»ng trong viá»‡c táº¡o ra cÃ¡c há»‡ thá»‘ng dá»‹ch thuáº­t tá»± Ä‘á»™ng hoáº·c sinh vÄƒn báº£n. 
			
	
	## 2.2. PhÃ¢n tÃ­ch ngá»¯ nghÄ©a (Semantic Analysis) 
	PhÃ¢n tÃ­ch ngá»¯ nghÄ©a lÃ  quÃ¡ trÃ¬nh hiá»ƒu Ã½ nghÄ©a cá»§a cÃ¡c tá»« vÃ  cÃ¢u trong ngá»¯ cáº£nh cá»¥ thá»ƒ. NÃ³ táº­p trung vÃ o viá»‡c lÃ m rÃµ cÃ¡c khÃ¡i niá»‡m, thá»±c thá»ƒ vÃ  quan há»‡ trong cÃ¢u.
	
		### Ngá»¯ nghÄ©a tá»« vá»±ng 
			Hiá»ƒu nghÄ©a cá»§a tá»« trong cÃ¢u dá»±a trÃªn tá»« Ä‘iá»ƒn hoáº·c cÃ¡c mÃ´ hÃ¬nh tá»« (word embeddings).


		### Ngá»¯ nghÄ©a cÃ¢u 
			Káº¿t há»£p cÃ¡c tá»« trong cÃ¢u Ä‘á»ƒ hiá»ƒu toÃ n bá»™ Ã½ nghÄ©a cá»§a cÃ¢u. VÃ­ dá»¥, cÃ¢u â€œCon mÃ¨o Äƒn cÃ¡â€ cÃ³ nghÄ©a lÃ  má»™t con mÃ¨o Ä‘ang tiÃªu thá»¥ thá»©c Äƒn lÃ  cÃ¡.
			
			
		### Ã nghÄ©a 
			PhÃ¢n tÃ­ch ngá»¯ nghÄ©a giÃºp mÃ´ hÃ¬nh NLP hiá»ƒu sÃ¢u hÆ¡n vá» ná»™i dung vÃ  Ã½ nghÄ©a tá»•ng thá»ƒ cá»§a vÄƒn báº£n, thay vÃ¬ chá»‰ nháº­n dáº¡ng tá»« ngá»¯ Ä‘Æ¡n láº».
		
		
		
	## 2.3. PhÃ¢n tÃ­ch ngá»¯ cáº£nh (Contextual Analysis) 
		PhÃ¢n tÃ­ch ngá»¯ cáº£nh lÃ  viá»‡c hiá»ƒu ná»™i dung cá»§a tá»« vÃ  cÃ¢u trong bá»‘i cáº£nh cá»¥ thá»ƒ. Má»™t tá»« cÃ³ thá»ƒ mang nhiá»u Ã½ nghÄ©a khÃ¡c nhau tÃ¹y thuá»™c vÃ o ngá»¯ cáº£nh, vÃ  phÃ¢n tÃ­ch ngá»¯ cáº£nh giÃºp xÃ¡c Ä‘á»‹nh nghÄ©a Ä‘Ãºng cá»§a tá»« trong cÃ¢u.
		
		### VÃ­ dá»¥ 
			Tá»« â€œÄ‘Ã¡â€ trong cÃ¢u â€œanh áº¥y Ä‘Ã¡ bÃ³ngâ€ cÃ³ nghÄ©a lÃ  hÃ nh Ä‘á»™ng, nhÆ°ng trong cÃ¢u â€œcá»¥c Ä‘Ã¡ náº±m trÃªn máº·t Ä‘áº¥tâ€, tá»« â€œÄ‘Ã¡â€ láº¡i mang nghÄ©a lÃ  má»™t váº­t thá»ƒ.
			
		### CÆ¡ cháº¿ chÃº Ã½ (Attention Mechanism)
			Trong cÃ¡c mÃ´ hÃ¬nh hiá»‡n Ä‘áº¡i nhÆ° Transformer, cÆ¡ cháº¿ nÃ y giÃºp mÃ¡y há»c táº­p trung vÃ o cÃ¡c tá»« quan trá»ng trong ngá»¯ cáº£nh, Ä‘á»“ng thá»i hiá»ƒu rÃµ má»‘i liÃªn há»‡ giá»¯a cÃ¡c tá»« trong má»™t cÃ¢u dÃ i hoáº·c má»™t Ä‘oáº¡n vÄƒn.
			
			
		### Ã nghÄ©a 
			PhÃ¢n tÃ­ch ngá»¯ cáº£nh giÃºp mÃ´ hÃ¬nh NLP hiá»ƒu Ä‘Æ°á»£c Ã½ nghÄ©a cá»§a tá»« dá»±a trÃªn cÃ¢u hoáº·c Ä‘oáº¡n vÄƒn xung quanh, nÃ¢ng cao Ä‘á»™ chÃ­nh xÃ¡c trong cÃ¡c tÃ¡c vá»¥ nhÆ° dá»‹ch thuáº­t hoáº·c tráº£ lá»i cÃ¢u há»i.
			
			
	## PhÃ¢n Ä‘oáº¡n vÄƒn báº£n (Text Segmentation)
		PhÃ¢n Ä‘oáº¡n vÄƒn báº£n lÃ  quÃ¡ trÃ¬nh chia nhá» vÄƒn báº£n thÃ nh cÃ¡c pháº§n cÃ³ Ã½ nghÄ©a nhÆ° tá»«, cÃ¢u hoáº·c Ä‘oáº¡n vÄƒn.
		
		### PhÃ¢n Ä‘oáº¡n tá»« (Tokenization)
			TÃ¡ch vÄƒn báº£n thÃ nh cÃ¡c Ä‘Æ¡n vá»‹ nhá» hÆ¡n gá»i lÃ  tá»« (tokens). VÃ­ dá»¥, cÃ¢u â€œTÃ´i thÃ­ch há»c mÃ¡yâ€ cÃ³ thá»ƒ Ä‘Æ°á»£c tÃ¡ch thÃ nh cÃ¡c tá»« â€œTÃ´iâ€, â€œthÃ­châ€, â€œhá»câ€, â€œmÃ¡yâ€.
			
		### PhÃ¢n Ä‘oáº¡n cÃ¢u (Sentence Segmentation)
			TÃ¡ch vÄƒn báº£n thÃ nh cÃ¡c cÃ¢u rá»i ráº¡c. Äiá»u nÃ y ráº¥t quan trá»ng trong cÃ¡c á»©ng dá»¥ng nhÆ° phÃ¢n tÃ­ch cáº£m xÃºc hay tÃ³m táº¯t vÄƒn báº£n
			
		### Ã nghÄ©a
			PhÃ¢n Ä‘oáº¡n vÄƒn báº£n lÃ  bÆ°á»›c quan trá»ng Ä‘áº§u tiÃªn trong nhiá»u há»‡ thá»‘ng NLP, giÃºp cÃ¡c mÃ´ hÃ¬nh xá»­ lÃ½ vÃ  phÃ¢n tÃ­ch vÄƒn báº£n dá»… dÃ ng hÆ¡n.
			
	## 2.5. Nháº­n dáº¡ng thá»±c thá»ƒ (Named Entity Recognition â€“ NER)
		Nháº­n dáº¡ng thá»±c thá»ƒ cÃ³ tÃªn (Named Entity Recognition â€“ NER) lÃ  quÃ¡ trÃ¬nh xÃ¡c Ä‘á»‹nh vÃ  phÃ¢n loáº¡i cÃ¡c thá»±c thá»ƒ trong vÄƒn báº£n, nhÆ° tÃªn ngÆ°á»i, Ä‘á»‹a Ä‘iá»ƒm, tá»• chá»©c, hoáº·c cÃ¡c sá»‘ liá»‡u cá»¥ thá»ƒ.
		
		### VÃ­ dá»¥ 
			Trong cÃ¢u â€œBill Gates thÃ nh láº­p Microsoft táº¡i Hoa Ká»³â€, NER sáº½ xÃ¡c Ä‘á»‹nh â€œBill Gatesâ€ lÃ  má»™t ngÆ°á»i, â€œMicrosoftâ€ lÃ  tá»• chá»©c, vÃ  â€œHoa Ká»³â€ lÃ  Ä‘á»‹a Ä‘iá»ƒm
			
		### CÃ¡c loáº¡i thá»±c thá»ƒ phá»• biáº¿n: 
			TÃªn ngÆ°á»i (Person): VÃ­ dá»¥, Elon Musk, Barack Obama.
			Äá»‹a Ä‘iá»ƒm (Location): VÃ­ dá»¥, HÃ  Ná»™i, New York.
			Tá»• chá»©c (Organization): VÃ­ dá»¥, Apple, Google.
			NgÃ y thÃ¡ng (Date): VÃ­ dá»¥, 2023, thÃ¡ng 9.
		
		### Ã nghÄ©a
			NER giÃºp trÃ­ch xuáº¥t cÃ¡c thÃ´ng tin quan trá»ng tá»« vÄƒn báº£n vÃ  lÃ  má»™t pháº§n quan trá»ng trong cÃ¡c há»‡ thá»‘ng phÃ¢n tÃ­ch dá»¯ liá»‡u lá»›n, tÃ¬m kiáº¿m thÃ´ng tin vÃ  trá»£ lÃ½ áº£o.
			
			
# 3. PhÆ°Æ¡ng phÃ¡p trong NLP 
	Xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn (NLP) lÃ  má»™t lÄ©nh vá»±c phá»©c táº¡p, bao gá»“m nhiá»u phÆ°Æ¡ng phÃ¡p khÃ¡c nhau Ä‘á»ƒ phÃ¢n tÃ­ch vÃ  hiá»ƒu ngÃ´n ngá»¯ cá»§a con ngÆ°á»i. Trong pháº§n nÃ y, chÃºng ta sáº½ Ä‘i sÃ¢u vÃ o cÃ¡c phÆ°Æ¡ng phÃ¡p chÃ­nh Ä‘Æ°á»£c sá»­ dá»¥ng trong NLP, tá»« nhá»¯ng ká»¹ thuáº­t cÆ¡ báº£n nhÆ° Bag of Words, TF-IDF, Ä‘áº¿n nhá»¯ng mÃ´ hÃ¬nh tiÃªn tiáº¿n hÆ¡n nhÆ° Word Embeddings vÃ  Transformer.
	
	
	## 3.1. Bag of Words (BoW) 
		Bag of Words (BoW) lÃ  má»™t trong nhá»¯ng ká»¹ thuáº­t cÆ¡ báº£n vÃ  Ä‘Æ¡n giáº£n nháº¥t trong NLP Ä‘á»ƒ biá»ƒu diá»…n vÄƒn báº£n. BoW khÃ´ng quan tÃ¢m Ä‘áº¿n ngá»¯ phÃ¡p hay thá»© tá»± cá»§a cÃ¡c tá»« mÃ  chá»‰ táº­p trung vÃ o táº§n suáº¥t xuáº¥t hiá»‡n cá»§a cÃ¡c tá»« trong má»™t vÄƒn báº£n.
		
		### NguyÃªn lÃ½ hoáº¡t Ä‘á»™ng 
			BoW xem vÄƒn báº£n nhÆ° má»™t táº­p há»£p cÃ¡c tá»« (hay â€œtÃºi tá»«â€), sau Ä‘Ã³ biá»ƒu diá»…n vÄƒn báº£n dÆ°á»›i dáº¡ng vector sá»‘ dá»±a trÃªn táº§n suáº¥t cá»§a má»—i tá»«. VÃ­ dá»¥, vá»›i hai cÃ¢u â€œMÃ¨o uá»‘ng sá»¯aâ€ vÃ  â€œChÃ³ uá»‘ng nÆ°á»›câ€, tÃºi tá»« sáº½ bao gá»“m [mÃ¨o, uá»‘ng, sá»¯a, chÃ³, nÆ°á»›c]. Sau Ä‘Ã³, má»—i vÄƒn báº£n sáº½ Ä‘Æ°á»£c biá»ƒu diá»…n thÃ nh má»™t vector táº§n suáº¥t, cháº³ng háº¡n nhÆ° [1, 1, 1, 0, 0] cho cÃ¢u Ä‘áº§u tiÃªn vÃ  [0, 1, 0, 1, 1] cho cÃ¢u thá»© hai. 
			
		### Æ¯u Ä‘iá»ƒm 
			ÄÆ¡n giáº£n, dá»… hiá»ƒu, cÃ³ thá»ƒ Ã¡p dá»¥ng cho cÃ¡c bÃ i toÃ¡n phÃ¢n loáº¡i vÄƒn báº£n vÃ  truy váº¥n thÃ´ng tin.
			
		### NhÆ°á»£c Ä‘iá»ƒm 
			BoW khÃ´ng xem xÃ©t Ä‘áº¿n ngá»¯ nghÄ©a hay thá»© tá»± tá»« trong cÃ¢u, dáº«n Ä‘áº¿n máº¥t mÃ¡t thÃ´ng tin ngá»¯ cáº£nh. 
			
			
	##TF-IDF (Term Frequency-Inverse Document Frequency) 
		TF-IDF lÃ  má»™t cáº£i tiáº¿n cá»§a BoW, giÃºp giáº£m thiá»ƒu áº£nh hÆ°á»Ÿng cá»§a cÃ¡c tá»« phá»• biáº¿n nhÆ°ng Ã­t quan trá»ng trong vÄƒn báº£n, vÃ­ dá»¥ nhÆ° â€œvÃ â€, â€œlÃ â€, â€œnhÆ°ngâ€. 
		
		### CÃ¡ch tÃ­nh toÃ¡n 
			TF (Term Frequency): LÃ  táº§n suáº¥t xuáº¥t hiá»‡n cá»§a tá»« trong vÄƒn báº£n. 
			IDF (Inverse Document Frequency): LÃ  nghá»‹ch Ä‘áº£o cá»§a táº§n suáº¥t xuáº¥t hiá»‡n cá»§a tá»« trong toÃ n bá»™ táº­p tÃ i liá»‡u. CÃ¡c tá»« xuáº¥t hiá»‡n thÆ°á»ng xuyÃªn trong nhiá»u tÃ i liá»‡u sáº½ cÃ³ trá»ng sá»‘ tháº¥p hÆ¡n. 
			
		CÃ´ng thá»©c TF-IDF: 
			\text{TF-IDF}(t, d) = \text{TF}(t, d) \times \log\left(\frac{N}{\text{DF}(t)}\right) 
			
		Trong Ä‘Ã³ : 
			t : Má»™t tá»« 
			d : Má»™t tÃ i liá»‡u cá»¥ thá»ƒ
			N: Tá»•ng sá»‘ tÃ i liá»‡u trong táº­p dá»¯ liá»‡u 
			DF(t): Sá»‘ tÃ i liá»‡u chá»©a tá»« t
			
		### Æ¯u Ä‘iá»ƒm 
			TF-IDF giÃºp loáº¡i bá» cÃ¡c tá»« thÆ°á»ng xuyÃªn xuáº¥t hiá»‡n nhÆ°ng khÃ´ng mang nhiá»u giÃ¡ trá»‹ ngá»¯ nghÄ©a, Ä‘á»“ng thá»i lÃ m ná»•i báº­t cÃ¡c tá»« Ä‘áº·c trÆ°ng cá»§a tá»«ng vÄƒn báº£n.
			
		NhÆ°á»£c Ä‘iá»ƒm	
			Giá»‘ng nhÆ° BoW, TF-IDF khÃ´ng xem xÃ©t Ä‘áº¿n ngá»¯ cáº£nh hay thá»© tá»± tá»«, do Ä‘Ã³ khÃ´ng phÃ¹ há»£p cho cÃ¡c bÃ i toÃ¡n Ä‘Ã²i há»i hiá»ƒu sÃ¢u vá» ngá»¯ nghÄ©a. 
			
	## 3.3. Word Embeddings 		
		Word Embeddings lÃ  má»™t bÆ°á»›c tiáº¿n lá»›n trong NLP, giÃºp biá»ƒu diá»…n cÃ¡c tá»« dÆ°á»›i dáº¡ng vector sá»‘ trong khÃ´ng gian liÃªn tá»¥c, trong Ä‘Ã³ cÃ¡c tá»« cÃ³ ngá»¯ nghÄ©a tÆ°Æ¡ng tá»± sáº½ náº±m gáº§n nhau. PhÆ°Æ¡ng phÃ¡p nÃ y Ä‘Ã£ cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ hiá»‡u suáº¥t cá»§a cÃ¡c mÃ´ hÃ¬nh há»c mÃ¡y trong cÃ¡c tÃ¡c vá»¥ ngÃ´n ngá»¯.

		### Word2Vec 
			Skip-gram: Dá»± Ä‘oÃ¡n tá»« ngá»¯ cáº£nh dá»±a trÃªn tá»« hiá»‡n táº¡i. MÃ´ hÃ¬nh nÃ y cá»‘ gáº¯ng dá»± Ä‘oÃ¡n cÃ¡c tá»« xung quanh (ngá»¯ cáº£nh) cá»§a má»™t tá»« trung tÃ¢m.
			Continuous Bag of Words (CBOW): Dá»± Ä‘oÃ¡n tá»« hiá»‡n táº¡i dá»±a trÃªn ngá»¯ cáº£nh. MÃ´ hÃ¬nh CBOW dá»± Ä‘oÃ¡n tá»« á»Ÿ giá»¯a dá»±a trÃªn cÃ¡c tá»« xung quanh.
			Cáº£ hai phÆ°Æ¡ng phÃ¡p nÃ y Ä‘á»u há»c cÃ¡ch biá»ƒu diá»…n cÃ¡c tá»« dÆ°á»›i dáº¡ng vector sá»‘, trong Ä‘Ã³ khoáº£ng cÃ¡ch giá»¯a cÃ¡c vector pháº£n Ã¡nh má»©c Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng vá» ngá»¯ nghÄ©a giá»¯a cÃ¡c tá»«.
			
		### GloVe (Global Vectors for Word Representation) 
			KhÃ¡c vá»›i Word2Vec, GloVe há»c cÃ¡c vector tá»« dá»±a trÃªn táº§n suáº¥t Ä‘á»“ng xuáº¥t hiá»‡n cá»§a tá»« trong toÃ n bá»™ táº­p dá»¯ liá»‡u. NÃ³ cá»‘ gáº¯ng tá»‘i Æ°u hÃ³a viá»‡c biá»ƒu diá»…n cÃ¡c tá»« sao cho cÃ¡c tá»« xuáº¥t hiá»‡n cÃ¹ng nhau trong vÄƒn báº£n sáº½ cÃ³ vector gáº§n nhau.
			
			Æ¯u Ä‘iá»ƒm: Word embeddings giÃºp mÃ¡y há»c náº¯m báº¯t Ä‘Æ°á»£c má»‘i quan há»‡ ngá»¯ nghÄ©a giá»¯a cÃ¡c tá»« vÃ  cÃ³ thá»ƒ Ã¡p dá»¥ng cho nhiá»u tÃ¡c vá»¥ NLP nhÆ° phÃ¢n loáº¡i vÄƒn báº£n, dá»‹ch mÃ¡y vÃ  phÃ¢n tÃ­ch cáº£m xÃºc.
			NhÆ°á»£c Ä‘iá»ƒm: KhÃ´ng thá»ƒ xá»­ lÃ½ tá»‘t cÃ¡c tá»« khÃ´ng cÃ³ trong tá»« Ä‘iá»ƒn (out-of-vocabulary words) vÃ  khÃ´ng náº¯m báº¯t Ä‘Æ°á»£c ngá»¯ cáº£nh thay Ä‘á»•i theo thá»i gian.
			
			
			
	## 3.4. MÃ´ hÃ¬nh Transformer 
		MÃ´ hÃ¬nh Transformer lÃ  má»™t trong nhá»¯ng Ä‘á»™t phÃ¡ lá»›n nháº¥t trong lÄ©nh vá»±c NLP, Ä‘áº·c biá»‡t sau khi mÃ´ hÃ¬nh nÃ y Ä‘Æ°á»£c giá»›i thiá»‡u bá»Ÿi Vaswani vÃ  cÃ¡c cá»™ng sá»± vÃ o nÄƒm 2017. Transformer Ä‘Ã£ thay tháº¿ hoÃ n toÃ n cÃ¡c mÃ´ hÃ¬nh dá»±a trÃªn RNN vÃ  LSTM trong nhiá»u tÃ¡c vá»¥ ngÃ´n ngá»¯, bao gá»“m dá»‹ch mÃ¡y vÃ  sinh vÄƒn báº£n.
		
		
		### Cáº¥u trÃºc mÃ´ hÃ¬nh
			Transformer sá»­ dá»¥ng cÆ¡ cháº¿ Attention (cÆ¡ cháº¿ chÃº Ã½) Ä‘á»ƒ xÃ¡c Ä‘á»‹nh nhá»¯ng pháº§n nÃ o cá»§a cÃ¢u cáº§n Ä‘Æ°á»£c chÃº trá»ng trong quÃ¡ trÃ¬nh xá»­ lÃ½. Cá»¥ thá»ƒ, cÆ¡ cháº¿ nÃ y giÃºp mÃ´ hÃ¬nh tá»± Ä‘á»™ng há»c cÃ¡ch táº­p trung vÃ o cÃ¡c tá»« quan trá»ng trong cÃ¢u khi phÃ¢n tÃ­ch hoáº·c dá»‹ch vÄƒn báº£n.
			Self-Attention: Má»—i tá»« trong cÃ¢u cÃ³ thá»ƒ â€œchÃº Ã½â€ Ä‘áº¿n táº¥t cáº£ cÃ¡c tá»« khÃ¡c trong cÃ¢u, khÃ´ng phá»¥ thuá»™c vÃ o khoáº£ng cÃ¡ch vá»‹ trÃ­ giá»¯a cÃ¡c tá»«.
			Encoder-Decoder: MÃ´ hÃ¬nh Transformer bao gá»“m má»™t pháº§n Encoder (mÃ£ hÃ³a) vÃ  Decoder (giáº£i mÃ£), Ä‘áº·c biá»‡t há»¯u Ã­ch trong cÃ¡c tÃ¡c vá»¥ dá»‹ch mÃ¡y.
			
		### CÃ¡c mÃ´ hÃ¬nh Transformer phá»• biáº¿n
			BERT (Bidirectional Encoder Representations from Transformers): Má»™t mÃ´ hÃ¬nh máº¡nh máº½ trong NLP, Ä‘Æ°á»£c huáº¥n luyá»‡n theo hai chiá»u, nghÄ©a lÃ  nÃ³ há»c cÃ¡ch dá»± Ä‘oÃ¡n cÃ¡c tá»« dá»±a trÃªn cáº£ bá»‘i cáº£nh trÆ°á»›c vÃ  sau trong má»™t cÃ¢u.
			
			GPT (Generative Pretrained Transformer): Má»™t mÃ´ hÃ¬nh lá»›n khÃ¡c, Ä‘Æ°á»£c huáº¥n luyá»‡n chá»§ yáº¿u theo má»™t chiá»u (tá»« trÆ°á»›c ra sau), ná»•i tiáº¿ng vá»›i kháº£ nÄƒng sinh vÄƒn báº£n tá»± nhiÃªn, sÃ¡ng táº¡o
			
			
			
		### Æ¯u Ä‘iá»ƒm 
			Transformer cÃ³ kháº£ nÄƒng xá»­ lÃ½ song song, nhanh hÆ¡n vÃ  hiá»‡u quáº£ hÆ¡n so vá»›i cÃ¡c mÃ´ hÃ¬nh tuáº§n tá»± nhÆ° LSTM, Ä‘á»“ng thá»i cho phÃ©p há»c cÃ¡c má»‘i quan há»‡ phá»©c táº¡p giá»¯a cÃ¡c tá»« trong vÄƒn báº£n.
			
		### NhÆ°á»£c Ä‘iá»ƒm
			MÃ´ hÃ¬nh Transformer Ä‘Ã²i há»i lÆ°á»£ng tÃ i nguyÃªn tÃ­nh toÃ¡n lá»›n, dáº«n Ä‘áº¿n chi phÃ­ cao trong huáº¥n luyá»‡n.
			
			
# 4. á»¨ng dá»¥ng thá»±c táº¿ cá»§a NLP
https://aicandy.vn/xu-ly-ngon-ngu-nlp-dot-pha-phan-tich-van-ban-bang-ai/

			
			
			
			
			
			
			
</pre><a id='backBottom' href='../AI-learning-list.html' style='display:none;'>ğŸ”™ Quay láº¡i danh sÃ¡ch</a><br><button onclick='toggleTheme()'>ğŸŒ™ Chuyá»ƒn giao diá»‡n</button></div><script>function toggleTheme() {   let mode = document.body.classList.contains('dark-mode') ? 'light-mode' : 'dark-mode';   document.body.className = mode; localStorage.setItem('theme', mode);   syncTheme();}function applyTheme() {   let savedTheme = localStorage.getItem('theme') || 'dark-mode';   document.body.className = savedTheme;   syncTheme();}function syncTheme() {   let preElement = document.querySelector('pre');   if (document.body.classList.contains('dark-mode')) { preElement.style.background = '#1e1e1e'; preElement.style.color = '#e0e0e0'; }   else { preElement.style.background = '#f5f5f5'; preElement.style.color = '#333333'; }}function checkPageHeight() {   let contentHeight = document.body.scrollHeight;   let windowHeight = window.innerHeight;   if (contentHeight > windowHeight * 1.2) {       document.getElementById('backBottom').style.display = 'block';   } else {       document.getElementById('backBottom').style.display = 'none';   }}</script></body></html>