<html><head><title>Lesson 16 == TÄƒng tá»‘c huáº¥n luyá»‡n vá»›i phÆ°Æ¡ng phÃ¡p Gradient Descent ==========//</title><style>body { font-family: Arial, sans-serif; transition: background 0.3s, color 0.3s; }.dark-mode { background-color: #121212; color: #e0e0e0; }.light-mode { background-color: #ffffff; color: #333333; }h1 { text-align: center; color: #73d9f5; }pre { padding: 15px; border-radius: 5px;       white-space: pre-wrap; word-wrap: break-word;       overflow-x: auto; max-width: 100%;       transition: background 0.3s, color 0.3s; }.dark-mode pre { background: #1e1e1e; color: #e0e0e0; }.light-mode pre { background: #f5f5f5; color: #333333; }#backTop, #backBottom {    font-size: 2em; padding: 20px 40px;    background: #bb86fc; color: white; text-decoration: none;    border-radius: 10px; display: inline-block; text-align: center; }#backTop:hover, #backBottom:hover { background: #9b67e2; }button { font-size: 1.5em; padding: 15px 30px;    background: #03dac6; color: #121212; border: none;    cursor: pointer; border-radius: 5px; display: block; margin: 10px auto; }button:hover { background: #02b8a3; }.dark-mode a { color: #03dac6; } .light-mode a { color: #007bff; }</style></head><body onload='applyTheme(); checkPageHeight()'><div class='container'><a id='backTop' href='../AI-learning-list.html'>ğŸ”™ Quay láº¡i danh sÃ¡ch</a><br><h1>Lesson 16 -- TÄƒng tá»‘c huáº¥n luyá»‡n vá»›i phÆ°Æ¡ng phÃ¡p Gradient Descent -//</h1><pre>
# KhÃ¡i niá»‡m 
	Gradient Descent (GD) lÃ  má»™t trong nhá»¯ng thuáº­t toÃ¡n tá»‘i Æ°u quan trá»ng vÃ  phá»• biáº¿n nháº¥t trong há»c mÃ¡y (machine learning) vÃ  trÃ­ tuá»‡ nhÃ¢n táº¡o (AI). Má»¥c tiÃªu chÃ­nh cá»§a thuáº­t toÃ¡n nÃ y lÃ  tÃ¬m giÃ¡ trá»‹ tá»‘i Æ°u (cá»±c tiá»ƒu hoáº·c cá»±c Ä‘áº¡i) cá»§a má»™t hÃ m máº¥t mÃ¡t (loss function) Ä‘á»ƒ tá»‘i Æ°u hÃ³a mÃ´ hÃ¬nh. Trong bá»‘i cáº£nh há»c mÃ¡y, Gradient Descent giÃºp Ä‘iá»u chá»‰nh cÃ¡c tham sá»‘ cá»§a mÃ´ hÃ¬nh, cháº³ng háº¡n nhÆ° trá»ng sá»‘ trong máº¡ng neural, sao cho hÃ m máº¥t mÃ¡t Ä‘Æ°á»£c giáº£m thiá»ƒu tá»‘i Ä‘a. 
	
	https://aicandy.vn/wp-content/uploads/2024/11/aicandy_gradient_descent.jpg 
	
	Gradient Descent hoáº¡t Ä‘á»™ng báº±ng cÃ¡ch tÃ­nh Ä‘áº¡o hÃ m cá»§a hÃ m máº¥t mÃ¡t theo cÃ¡c tham sá»‘ mÃ´ hÃ¬nh vÃ  Ä‘iá»u chá»‰nh cÃ¡c tham sá»‘ Ä‘Ã³ theo hÆ°á»›ng ngÆ°á»£c láº¡i vá»›i Ä‘áº¡o hÃ m (gradient). Báº±ng cÃ¡ch láº·p láº¡i quy trÃ¬nh nÃ y, thuáº­t toÃ¡n dáº§n dáº§n tiáº¿n gáº§n Ä‘áº¿n Ä‘iá»ƒm tá»‘i Æ°u. CÃ³ nhiá»u biáº¿n thá»ƒ cá»§a Gradient Descent nhÆ° Stochastic Gradient Descent (SGD), Mini-batch Gradient Descent, hay Batch Gradient Descent, má»—i loáº¡i phÃ¹ há»£p vá»›i cÃ¡c bÃ i toÃ¡n vÃ  dá»¯ liá»‡u khÃ¡c nhau.

# CÃ¡ch Gradient Descent hoáº¡t Ä‘á»™ng 
	Gradient Descent lÃ  má»™t thuáº­t toÃ¡n tá»‘i Æ°u, giÃºp tÃ¬m giÃ¡ trá»‹ cá»±c tiá»ƒu cá»§a má»™t hÃ m má»¥c tiÃªu (thÆ°á»ng lÃ  hÃ m máº¥t mÃ¡t). NÃ³ dá»±a trÃªn viá»‡c tÃ­nh toÃ¡n gradient (Ä‘áº¡o hÃ m báº­c nháº¥t) cá»§a hÃ m nÃ y vá»›i cÃ¡c tham sá»‘ cáº§n tá»‘i Æ°u vÃ  Ä‘iá»u chá»‰nh cÃ¡c tham sá»‘ theo hÆ°á»›ng ngÆ°á»£c láº¡i cá»§a gradient Ä‘á»ƒ giáº£m thiá»ƒu hÃ m máº¥t mÃ¡t.
	
	## CÃ¡c bÆ°á»›c thá»±c hiá»‡n 
		
		### Khá»Ÿi táº¡o tham sá»‘ ban Ä‘áº§u 
			Báº¯t Ä‘áº§u vá»›i giÃ¡ trá»‹ khá»Ÿi táº¡o cho cÃ¡c tham sá»‘ cáº§n tá»‘i Æ°u hÃ³a, gá»i lÃ  $\theta$. CÃ¡c giÃ¡ trá»‹ nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c chá»n ngáº«u nhiÃªn hoáº·c cá»‘ Ä‘á»‹nh, tÃ¹y vÃ o phÆ°Æ¡ng phÃ¡p khá»Ÿi táº¡o.
			VÃ­ dá»¥: trong má»™t bÃ i toÃ¡n há»“i quy tuyáº¿n tÃ­nh, $\theta$ cÃ³ thá»ƒ lÃ  cÃ¡c trá»ng sá»‘ (weights) cá»§a mÃ´ hÃ¬nh. 
		
		### TÃ­nh toÃ¡n giÃ¡ trá»‹ cá»§a hÃ m máº¥t mÃ¡t 
			HÃ m máº¥t mÃ¡t $J(\theta)$ Ä‘Ã¡nh giÃ¡ Ä‘á»™ sai lá»‡ch giá»¯a dá»± Ä‘oÃ¡n cá»§a mÃ´ hÃ¬nh vÃ  giÃ¡ trá»‹ thá»±c táº¿.
			VÃ­ dá»¥: Vá»›i bÃ i toÃ¡n há»“i quy tuyáº¿n tÃ­nh, hÃ m máº¥t mÃ¡t phá»• biáº¿n lÃ  hÃ m sai sá»‘ bÃ¬nh phÆ°Æ¡ng trung bÃ¬nh (Mean Squared Error â€“ MSE): 
			
			J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} (h_\theta(x_i) â€“ y_i)^2 
			Trong Ä‘Ã³, $m$ lÃ  sá»‘ lÆ°á»£ng máº«u dá»¯ liá»‡u, $h_\theta(x_i)$ lÃ  giÃ¡ trá»‹ dá»± Ä‘oÃ¡n cá»§a mÃ´ hÃ¬nh vá»›i tham sá»‘ $\theta$, vÃ  $y_i$ lÃ  giÃ¡ trá»‹ thá»±c táº¿. 
			
		### TÃ­nh gradient cá»§a hÃ m máº¥t mÃ¡t theo tham sá»‘ 
		
			Gradient cá»§a hÃ m máº¥t mÃ¡t Ä‘á»‘i vá»›i cÃ¡c tham sá»‘ lÃ  Ä‘áº¡o hÃ m báº­c nháº¥t cá»§a $J(\theta)$ theo tá»«ng tham sá»‘ $\theta$. NÃ³ biá»ƒu thá»‹ Ä‘á»™ dá»‘c vÃ  hÆ°á»›ng thay Ä‘á»•i cá»§a hÃ m máº¥t mÃ¡t. 
			Gradient Ä‘Æ°á»£c kÃ½ hiá»‡u lÃ  $\nabla_\theta J(\theta)$, vá»›i má»—i pháº§n tá»­ cá»§a nÃ³ tÆ°Æ¡ng á»©ng vá»›i Ä‘áº¡o hÃ m riÃªng pháº§n theo tá»«ng tham sá»‘ cá»§a mÃ´ hÃ¬nh.
			VÃ­ dá»¥: Vá»›i hÃ m $J(\theta)$ Ä‘Æ¡n giáº£n nhÆ° $J(\theta) = \theta^2$, gradient sáº½ lÃ :
			
			\nabla_\theta J(\theta) = 2\theta 
			Äiá»u nÃ y cho biáº¿t má»©c Ä‘á»™ vÃ  hÆ°á»›ng mÃ  $\theta$ nÃªn thay Ä‘á»•i Ä‘á»ƒ giáº£m giÃ¡ trá»‹ cá»§a $J(\theta)$.

		### Cáº­p nháº­t cÃ¡c tham sá»‘ 
			Sau khi tÃ­nh toÃ¡n gradient, ta cáº­p nháº­t cÃ¡c tham sá»‘ $\theta$ theo cÃ´ng thá»©c sau:
				\theta = \theta â€“ \alpha \nabla_\theta J(\theta) 
				
			á» Ä‘Ã¢y, $\alpha$ lÃ  tá»‘c Ä‘á»™ há»c (learning rate), cho biáº¿t kÃ­ch thÆ°á»›c bÆ°á»›c Ä‘i trong quÃ¡ trÃ¬nh tá»‘i Æ°u hÃ³a. Náº¿u $\alpha$ quÃ¡ lá»›n, quÃ¡ trÃ¬nh cÃ³ thá»ƒ bá» qua Ä‘iá»ƒm cá»±c tiá»ƒu; náº¿u $\alpha$ quÃ¡ nhá», quÃ¡ trÃ¬nh há»™i tá»¥ sáº½ ráº¥t cháº­m. 
			QuÃ¡ trÃ¬nh cáº­p nháº­t nÃ y Ä‘Æ°á»£c gá»i lÃ  â€œbÆ°á»›c gradientâ€, vÃ  má»—i láº§n thá»±c hiá»‡n Ä‘Æ°á»£c gá»i lÃ  má»™t â€œepochâ€.
			
		### Láº·p láº¡i quÃ¡ trÃ¬nh
			Quy trÃ¬nh nÃ y Ä‘Æ°á»£c láº·p láº¡i nhiá»u láº§n cho Ä‘áº¿n khi há»™i tá»¥, tá»©c lÃ  khi gradient Ä‘á»§ nhá» hoáº·c khi giÃ¡ trá»‹ cá»§a hÃ m máº¥t mÃ¡t khÃ´ng thay Ä‘á»•i nhiá»u giá»¯a cÃ¡c bÆ°á»›c láº·p.
			Há»™i tá»¥ thÆ°á»ng Ä‘áº¡t Ä‘Æ°á»£c khi gradient tiáº¿n gáº§n Ä‘áº¿n 0, nghÄ©a lÃ  Ä‘Ã£ Ä‘áº¡t Ä‘áº¿n Ä‘iá»ƒm cá»±c tiá»ƒu cá»§a hÃ m máº¥t mÃ¡t. 

		### Äiá»u kiá»‡n dá»«ng 
		
			Thuáº­t toÃ¡n Gradient Descent sáº½ dá»«ng khi:
			Sá»‘ láº§n láº·p (epoch) Ä‘áº¡t giá»›i háº¡n Ä‘Ã£ Ä‘á»‹nh trÆ°á»›c.
			Gradient trá»Ÿ nÃªn ráº¥t nhá», gáº§n nhÆ° báº±ng 0, hoáº·c sá»± thay Ä‘á»•i trong giÃ¡ trá»‹ cá»§a hÃ m máº¥t mÃ¡t khÃ´ng Ä‘Ã¡ng ká»ƒ.

		### VÃ­ dá»¥ trá»±c quan 
			Giáº£ sá»­ ta cÃ³ má»™t hÃ m Ä‘Æ¡n giáº£n $f(x) = x^2$ vÃ  muá»‘n tÃ¬m giÃ¡ trá»‹ $x$ sao cho $f(x)$ Ä‘áº¡t cá»±c tiá»ƒu (Ä‘iá»ƒm cá»±c tiá»ƒu cá»§a $f(x)$ náº±m táº¡i $x = 0$). 
			
		### 	Quy trÃ¬nh vá»›i Gradient Descent: 
			
			BÆ°á»›c 1: Chá»n giÃ¡ trá»‹ khá»Ÿi táº¡o ban Ä‘áº§u $x = 4$, chá»n tá»‘c Ä‘á»™ há»c $\alpha = 0.1$. 
			BÆ°á»›c 2: TÃ­nh gradient $\nabla_x f(x) = 2x$:Vá»›i $x = 4$, gradient lÃ  $2 \times 4 = 8$.
			BÆ°á»›c 3: Cáº­p nháº­t giÃ¡ trá»‹ cá»§a $x$: 
				x_{\text{new}} = x_{\text{old}} â€“ \alpha \times \nabla_x f(x_{\text{old}}) = 4 â€“ 0.1 \times 8 = 3.2 
			BÆ°á»›c 4: Láº·p láº¡i quy trÃ¬nh cho Ä‘áº¿n khi $x$ gáº§n báº±ng 0.

			Qua má»—i bÆ°á»›c, giÃ¡ trá»‹ cá»§a $x$ sáº½ cÃ ng ngÃ y cÃ ng nhá», hÆ°á»›ng Ä‘áº¿n giÃ¡ trá»‹ $0$, lÃ  Ä‘iá»ƒm cá»±c tiá»ƒu cá»§a hÃ m $f(x)$. 


	## Giáº£m thiá»ƒu lá»—i trong khi há»c 
		Trong há»c mÃ¡y, hÃ m máº¥t mÃ¡t Ä‘o lÆ°á»ng sá»± khÃ¡c biá»‡t giá»¯a giÃ¡ trá»‹ dá»± Ä‘oÃ¡n cá»§a mÃ´ hÃ¬nh vÃ  giÃ¡ trá»‹ thá»±c táº¿. VÃ­ dá»¥, trong há»“i quy tuyáº¿n tÃ­nh, má»¥c tiÃªu lÃ  Ä‘iá»u chá»‰nh cÃ¡c tham sá»‘ cá»§a mÃ´ hÃ¬nh (nhÆ° trá»ng sá»‘) sao cho dá»± Ä‘oÃ¡n cá»§a mÃ´ hÃ¬nh gáº§n Ä‘Ãºng vá»›i giÃ¡ trá»‹ thá»±c táº¿. Gradient Descent giÃºp tÃ¬m cÃ¡c giÃ¡ trá»‹ tham sá»‘ tá»‘i Æ°u báº±ng cÃ¡ch giáº£m thiá»ƒu giÃ¡ trá»‹ cá»§a hÃ m máº¥t mÃ¡t. 
		
	## 	 Code minh há»a 
		DÆ°á»›i Ä‘Ã¢y lÃ  vÃ­ dá»¥ sá»­ dá»¥ng PyTorch Ä‘á»ƒ minh há»a quÃ¡ trÃ¬nh cáº­p nháº­t tham sá»‘ báº±ng Gradient Descent cho hÃ m máº¥t mÃ¡t Ä‘Æ¡n giáº£n: 
		
		GradientDescentByPytorch.py


		### Giáº£i thÃ­ch 
			á»Ÿ vÃ­ dá»¥ trÃªn, ta khá»Ÿi táº¡o $x = 4$. á» má»—i bÆ°á»›c, giÃ¡ trá»‹ cá»§a $x$ Ä‘Æ°á»£c cáº­p nháº­t theo Gradient Descent vÃ  dáº§n dáº§n há»™i tá»¥ vá» giÃ¡ trá»‹ $x = 0$, lÃ  Ä‘iá»ƒm cá»±c tiá»ƒu cá»§a hÃ m $f(x) = x^2$. GiÃ¡ trá»‹ cá»§a hÃ m máº¥t mÃ¡t (loss) cÅ©ng giáº£m dáº§n theo cÃ¡c bÆ°á»›c láº·p 
			
			
# Váº¥n Ä‘á» trong Gradient Descent truyá»n thá»‘ng 
	Gradient Descent lÃ  má»™t trong nhá»¯ng thuáº­t toÃ¡n tá»‘i Æ°u hÃ³a phá»• biáº¿n nháº¥t trong há»c mÃ¡y, nÃ³ cÅ©ng gáº·p pháº£i nhiá»u váº¥n Ä‘á» khi Ä‘Æ°á»£c Ã¡p dá»¥ng trong thá»±c táº¿. DÆ°á»›i Ä‘Ã¢y lÃ  cÃ¡c váº¥n Ä‘á» chÃ­nh mÃ  Gradient Descent truyá»n thá»‘ng gáº·p pháº£i : 
		
		## Overfitting 
			Overfitting lÃ  má»™t hiá»‡n tÆ°á»£ng phá»• biáº¿n trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n mÃ´ hÃ¬nh há»c mÃ¡y, khi mÃ´ hÃ¬nh há»c quÃ¡ ká»¹ cÃ¡c chi tiáº¿t vÃ  nhiá»…u tá»« táº­p dá»¯ liá»‡u huáº¥n luyá»‡n. Khi Ä‘Ã³, mÃ´ hÃ¬nh cÃ³ kháº£ nÄƒng dá»± Ä‘oÃ¡n ráº¥t tá»‘t trÃªn dá»¯ liá»‡u huáº¥n luyá»‡n nhÆ°ng láº¡i hoáº¡t Ä‘á»™ng kÃ©m trÃªn dá»¯ liá»‡u kiá»ƒm tra hoáº·c dá»¯ liá»‡u thá»±c táº¿.
					https://aicandy.vn/wp-content/uploads/2024/09/aicandy_overfitting.jpg
					
			### Má»™t sá»‘ Ä‘áº·c Ä‘iá»ƒm cá»§a overfitting: 
			
				#### MÃ´ hÃ¬nh quÃ¡ phá»©c táº¡p 
					Overfitting thÆ°á»ng xáº£y ra khi mÃ´ hÃ¬nh cÃ³ quÃ¡ nhiá»u tham sá»‘, cháº³ng háº¡n nhÆ° cÃ¡c máº¡ng nÆ¡-ron nhiá»u nÃºt trong tá»«ng lá»›p. Nhá»¯ng mÃ´ hÃ¬nh nÃ y cÃ³ thá»ƒ dá»… dÃ ng khá»›p vá»›i cÃ¡c biáº¿n Ä‘á»•i nhá» vÃ  nhiá»…u trong táº­p dá»¯ liá»‡u, nhÆ°ng láº¡i máº¥t kháº£ nÄƒng tá»•ng quÃ¡t hÃ³a khi gáº·p cÃ¡c dá»¯ liá»‡u má»›i. 
					
				#### Hiá»‡u suáº¥t giáº£m trÃªn dá»¯ liá»‡u má»›i 
					DÃ¹ mÃ´ hÃ¬nh cÃ³ thá»ƒ Ä‘áº¡t hiá»‡u suáº¥t ráº¥t cao trÃªn táº­p huáº¥n luyá»‡n, khi Ã¡p dá»¥ng trÃªn dá»¯ liá»‡u má»›i hoáº·c dá»¯ liá»‡u kiá»ƒm tra, káº¿t quáº£ cÃ³ thá»ƒ kÃ©m hÆ¡n Ä‘Ã¡ng ká»ƒ. Äiá»u nÃ y lÃ  do mÃ´ hÃ¬nh Ä‘Ã£ há»c cÃ¡c máº«u khÃ´ng Ä‘áº¡i diá»‡n cho xu hÆ°á»›ng chung cá»§a dá»¯ liá»‡u. 

				#### Dáº¥u hiá»‡u overfitting 
					Má»™t dáº¥u hiá»‡u rÃµ rÃ ng cá»§a overfitting lÃ  sá»± chÃªnh lá»‡ch lá»›n giá»¯a Ä‘á»™ chÃ­nh xÃ¡c trÃªn táº­p huáº¥n luyá»‡n vÃ  Ä‘á»™ chÃ­nh xÃ¡c trÃªn táº­p kiá»ƒm tra. Khi mÃ´ hÃ¬nh liÃªn tá»¥c cáº£i thiá»‡n trÃªn dá»¯ liá»‡u huáº¥n luyá»‡n nhÆ°ng khÃ´ng cáº£i thiá»‡n trÃªn dá»¯ liá»‡u kiá»ƒm tra, Ä‘Ã³ lÃ  dáº¥u hiá»‡u mÃ´ hÃ¬nh Ä‘Ã£ báº¯t Ä‘áº§u overfit. 

			### CÃ¡ch giáº£m thiá»ƒu overfitting: 
			
				#### Sá»­ dá»¥ng regularization 
					CÃ¡c ká»¹ thuáº­t nhÆ° L1/L2 regularization hoáº·c Dropout giÃºp giáº£m thiá»ƒu overfitting báº±ng cÃ¡ch giá»›i háº¡n Ä‘á»™ phá»©c táº¡p cá»§a mÃ´ hÃ¬nh. 
					
				#### ThÃªm dá»¯ liá»‡u huáº¥n luyá»‡n 
					Náº¿u cÃ³ thÃªm dá»¯ liá»‡u, mÃ´ hÃ¬nh sáº½ há»c Ä‘Æ°á»£c nhiá»u máº«u phong phÃº hÆ¡n vÃ  giáº£m thiá»ƒu kháº£ nÄƒng overfit. 
					
				#### Early stopping 
					Theo dÃµi hiá»‡u suáº¥t trÃªn táº­p kiá»ƒm tra vÃ  dá»«ng quÃ¡ trÃ¬nh huáº¥n luyá»‡n khi mÃ´ hÃ¬nh báº¯t Ä‘áº§u cÃ³ dáº¥u hiá»‡u overfit thay vÃ¬ tiáº¿p tá»¥c huáº¥n luyá»‡n quÃ¡ lÃ¢u. 
					
					
		## Underfitting 
			Underfitting xáº£y ra khi má»™t mÃ´ hÃ¬nh há»c mÃ¡y khÃ´ng Ä‘á»§ kháº£ nÄƒng Ä‘á»ƒ khá»›p vá»›i dá»¯ liá»‡u huáº¥n luyá»‡n, dáº«n Ä‘áº¿n hiá»‡u suáº¥t kÃ©m trÃªn cáº£ táº­p huáº¥n luyá»‡n vÃ  dá»¯ liá»‡u kiá»ƒm tra. ÄÃ¢y lÃ  váº¥n Ä‘á» khi mÃ´ hÃ¬nh quÃ¡ Ä‘Æ¡n giáº£n hoáº·c khÃ´ng Ä‘á»§ phá»©c táº¡p Ä‘á»ƒ há»c cÃ¡c Ä‘áº·c Ä‘iá»ƒm quan trá»ng trong dá»¯ liá»‡u 
			
					https://aicandy.vn/wp-content/uploads/2024/09/aicandy_underfitting.png
					
			### Äáº·c Ä‘iá»ƒm cá»§a Underfitting 
				#### MÃ´ hÃ¬nh quÃ¡ Ä‘Æ¡n giáº£n 
					Underfitting thÆ°á»ng xáº£y ra khi mÃ´ hÃ¬nh khÃ´ng Ä‘á»§ phá»©c táº¡p Ä‘á»ƒ náº¯m báº¯t cÃ¡c má»‘i quan há»‡ trong dá»¯ liá»‡u. VÃ­ dá»¥, sá»­ dá»¥ng má»™t mÃ´ hÃ¬nh tuyáº¿n tÃ­nh cho dá»¯ liá»‡u cÃ³ quan há»‡ phi tuyáº¿n cÃ³ thá»ƒ dáº«n Ä‘áº¿n underfitting, vÃ¬ mÃ´ hÃ¬nh tuyáº¿n tÃ­nh khÃ´ng thá»ƒ Ä‘áº¡i diá»‡n cho cÃ¡c biáº¿n thá»ƒ phá»©c táº¡p trong dá»¯ liá»‡u 
					
				#### Dáº¥u hiá»‡u cá»§a underfitting 
					Má»™t dáº¥u hiá»‡u rÃµ rÃ ng cá»§a underfitting lÃ  khi Ä‘á»™ chÃ­nh xÃ¡c trÃªn táº­p huáº¥n luyá»‡n khÃ´ng Ä‘áº¡t má»©c mong muá»‘n vÃ  khÃ´ng cáº£i thiá»‡n nhiá»u khi so vá»›i Ä‘á»™ chÃ­nh xÃ¡c trÃªn dá»¯ liá»‡u kiá»ƒm tra. MÃ´ hÃ¬nh khÃ´ng thá»ƒ há»c Ä‘á»§ tá»« dá»¯ liá»‡u Ä‘á»ƒ cáº£i thiá»‡n hiá»‡u suáº¥t 
					
			### CÃ¡ch kháº¯c phá»¥c cá»§a underfiting 
				#### TÄƒng Ä‘á»™ phá»©c táº¡p cá»§a mÃ´ hÃ¬nh 
					Sá»­ dá»¥ng mÃ´ hÃ¬nh phá»©c táº¡p hÆ¡n hoáº·c thÃªm nhiá»u lá»›p vÃ o nÃºt trong máº¡ng nÆ¡-ron cÃ³ thá»ƒ giÃºp mÃ´ hÃ¬nh há»c Ä‘Æ°á»£c cÃ¡c Ä‘áº·c Ä‘iá»ƒm phá»©c táº¡p hÆ¡n tá»« dá»¯ liá»‡u. 
					
				#### Thay Ä‘á»•i Ä‘áº·c trÆ°ng dá»¯ liá»‡u 
					Táº¡o thÃªm Ä‘áº·c trÆ°ng hoáº·c Ã¡p dá»¥ng cÃ¡c ká»¹ thuáº­t khai thÃ¡c Ä‘áº·c trÆ°ng cÃ³ thá»ƒ giÃºp mÃ´ hÃ¬nh há»c Ä‘Æ°á»£c nhiá»u thÃ´ng tin hÆ¡n tá»« dá»¯ liá»‡u 
					
				#### Giáº£m regularization 
					Náº¿u mÃ´ hÃ¬nh Ä‘ang sá»­ dá»¥ng cÃ¡c ká»¹ thuáº­t regularization nhÆ° L1/L2, giáº£m má»©c regularization cÃ³ thá»ƒ giÃºp mÃ´ hÃ¬nh há»c Ä‘Æ°á»£c nhiá»u hÆ¡n tá»« dá»¯ liá»‡u huáº¥n luyá»‡n  
					
		## Lá»±a chá»n learning rate khÃ´ng tá»‘i Æ°u 
				Learning rate ($\alpha$) lÃ  tham sá»‘ quan trá»ng trong Gradient Descent. Náº¿u khÃ´ng chá»n Ä‘Ãºng giÃ¡ trá»‹ learning rate, thuáº­t toÃ¡n cÃ³ thá»ƒ gáº·p nhiá»u váº¥n Ä‘á» 
				
			### Learning rate quÃ¡ lá»›n 
				#### KhÃ´ng há»™i tá»¥ 
					Learning rate lá»›n khiáº¿n cÃ¡c bÆ°á»›c nháº£y trong quÃ¡ trÃ¬nh tá»‘i Æ°u hÃ³a trá»Ÿ nÃªn quÃ¡ lá»›n, khiáº¿n mÃ´ hÃ¬nh bá» qua Ä‘iá»ƒm tá»‘i Æ°u cá»¥c bá»™ hoáº·c toÃ n cá»¥c. Äiá»u nÃ y lÃ m cho quÃ¡ trÃ¬nh huáº¥n luyá»‡n khÃ´ng há»™i tá»¥ vÃ  máº¥t á»•n Ä‘á»‹nh, dáº«n Ä‘áº¿n káº¿t quáº£ khÃ´ng chÃ­nh xÃ¡c. 
					
				#### Dao Ä‘á»™ng quanh Ä‘iá»ƒm tá»‘i Æ°u 
					Thay vÃ¬ dáº§n dáº§n giáº£m thiá»ƒu hÃ m máº¥t mÃ¡t vÃ  tiáº¿n Ä‘áº¿n Ä‘iá»ƒm tá»‘i Æ°u, má»™t learning rate quÃ¡ lá»›n cÃ³ thá»ƒ lÃ m cho mÃ´ hÃ¬nh dáº¡o Ä‘á»™ng xung quanh Ä‘iá»ƒm tá»‘i Æ°u mÃ  khÃ´ng bao giá» Ä‘áº¡t Ä‘Æ°á»£c sá»± há»™i tá»¥. Äiá»u nÃ y xáº£y ra do mÃ´ hÃ¬nh liÃªn tá»¥c vÆ°á»£t qua Ä‘iá»ƒm tá»‘i Æ°u mÃ  khÃ´ng cÃ³ kháº£ nÄƒng dá»«ng láº¡i. 
					
				#### Giáº£m Ä‘á»™ chÃ­nh xÃ¡c 
					Náº¿u learning rate quÃ¡ cao, mÃ´ hÃ¬nh cÃ³ thá»ƒ há»c quÃ¡ nhanh vÃ  khÃ´ng ká»‹p tá»‘i Æ°u hÃ³a cÃ¡c trá»ng sá»‘ chÃ­nh xÃ¡c cho tá»«ng bÆ°á»›c. Äiá»u nÃ y dáº«n Ä‘áº¿n sai sá»‘ lá»›n hÆ¡n vÃ  lÃ m giáº£m Ä‘á»™ chÃ­nh xÃ¡c cá»§a mÃ´ hÃ¬nh, ngay cáº£ khi mÃ´ hÃ¬nh cÃ³ tiá»m nÄƒng Ä‘áº¡t hiá»‡u quáº£ tá»‘t hÆ¡n 
					
				#### HÃ m máº¥t mÃ¡t tÄƒng cao 
					Trong má»™t sá»‘ trÆ°á»ng há»£p, learning rate lá»›n cÃ³ thá»ƒ lÃ m cho giÃ¡ trá»‹ hÃ m máº¥t mÃ¡t tÄƒng thay vÃ¬ giáº£m, co cÃ¡c cáº­p nháº­t trá»ng sá»‘ lá»›n gÃ¢y ra sá»± thay Ä‘á»•i máº¡nh trong hÆ°á»›ng gradient, khiáº¿n mÃ´ hÃ¬nh trá»Ÿ nÃªn kÃ©m hiá»‡u quáº£ hÆ¡n 
					
			### Learning rate quÃ¡ nhá» 
				
				#### QuÃ¡ trÃ¬nh huáº¥n luyá»‡n cháº­m cháº¡p 
					Learning rate nhá» lÃ m cho cÃ¡c bÆ°á»›c cáº­p nháº­t trá»ng sá»‘ ráº¥t nhá», dáº«n Ä‘áº¿n tá»‘c Ä‘á»™ há»™i tá»¥ cháº­m. MÃ´ hÃ¬nh cáº§n nhiá»u thá»i gian vÃ  sá»‘ láº§n láº·p Ä‘á»ƒ Ä‘áº¡t Ä‘áº¿n Ä‘iá»ƒm tá»‘i Æ°u, lÃ m kÃ©o dÃ i quÃ¡ trÃ¬nh huáº¥n luyá»‡n, Ä‘áº·c biá»‡t vá»›i cÃ¡c táº­p dá»¯ liá»‡u lá»›n vÃ  mÃ´ hÃ¬nh phá»©c táº¡p 
					
				#### Há»™i tá»¥ táº¡i Ä‘iá»ƒm cá»¥c bá»™ 
					Má»™t learning  rate nhá» cÃ³ thá»ƒ khiáº¿n mÃ´ hÃ¬nh bá»‹ máº¯c káº¹t táº¡i cÃ¡c Ä‘iá»ƒm tá»‘i Æ°u cá»¥c bá»™ thay vÃ¬ Ä‘áº¡t Ä‘áº¿n Ä‘iá»ƒm tá»‘i Æ°u toÃ n cá»¥c. Äiá»u nÃ y xáº£y ra khi bÆ°á»›c cáº­p nháº­t quÃ¡ nhá» Ä‘á»ƒ thoÃ¡t khá»i cÃ¡c thung lÅ©ng trong khÃ´ng gian hÃ m máº¥t mÃ¡t, dáº«n Ä‘áº¿n káº¿t quáº£ khÃ´ng tá»‘i Æ°u 
					
				#### Chi phÃ­ tÃ­nh toÃ¡n tÄƒng cao 
					Vá»›i learning rate quÃ¡ nhá», sá»‘ láº§n láº·p cáº§n thiáº¿t Ä‘á»ƒ giáº£m thiá»ƒu hÃ m máº¥t mÃ¡t tÄƒng lÃªn, kÃ©o theo chi phÃ­ tÃ­nh toÃ¡n cÅ©ng tÄƒng. Äiá»u nÃ y cÃ³ thá»ƒ gÃ¢y lÃ£ng phÃ­ tÃ i nguyÃªn tÃ­nh toÃ¡n mÃ  khÃ´ng Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t tá»‘t hÆ¡n. 
					
		## Nguy cÆ¡ overfitting 
			Do quÃ¡ trÃ¬nh huáº¥n luyá»‡n kÃ©o dÃ i, mÃ´ hÃ¬nh cÃ³ thá»ƒ há»c quÃ¡ ká»¹ cÃ¡c chi tiáº¿t cá»§a táº­p dá»¯ liá»‡u huáº¥n luyá»‡n, dáº«n Ä‘áº¿n overfitting. Máº·c dÃ¹ hÃ m máº¥t mÃ¡t trÃªn táº­p huáº¥n luyá»‡n giáº£m dáº§n, mÃ´ hÃ¬nh cÃ³ thá»ƒ khÃ´ng tá»•ng quÃ¡t tá»‘t trÃªn dá»¯ liá»‡u kiá»ƒm tra, lÃ m giáº£m kháº£ nÄƒng dá»± Ä‘oÃ¡n chÃ­nh xÃ¡c. 
			
			### VÃ­ dá»¥ minh há»a lá»±a chá»n learning rate : 
				GradientDescentSelectLearningRateByPytorch.py

				#### khi lá»±a chá»n learning rate lÃ  0.5, chÆ°Æ¡ng trÃ¬nh khÃ´ng há»™i tá»¥ 
					https://aicandy.vn/wp-content/uploads/2024/09/aicandy_learning_rate_5.jpg
				
				#### Khi lá»±a chá»n learning rate lÃ  0.01, chÆ°Æ¡ng trÃ¬nh há»™i tá»¥ nhanh vÃ  á»•n Ä‘á»‹nh. 
					https://aicandy.vn/wp-content/uploads/2024/09/aicandy_learning_rate_01.jpg
					
				#### Khi lá»±a chá»n learning rate lÃ  0.001, chÆ°Æ¡ng trÃ¬nh há»™i tá»¥ ráº¥t cháº­m. 
					https://aicandy.vn/wp-content/uploads/2024/09/aicandy_learning_rate_001.jpg 
					
			Viá»‡c lá»±a chá»n learning rate áº£nh hÆ°á»Ÿng nhiá»u tá»›i kháº£ nÄƒng cÅ©ng nhÆ° tá»‘c Ä‘á»™ há»™i tá»¥ cá»§a chÆ°Æ¡ng trÃ¬nh. Äá»ƒ tÃ¬m Ä‘Æ°á»£c giÃ¡ trá»‹ learning rate tá»‘t, cáº§n thá»­ nghiá»‡m nhiá»u láº§n vÃ  vá»›i bá»™ dá»¯ liá»‡u tÆ°Æ¡ng Ä‘á»‘i lá»›n 
		
	# PhÆ°Æ¡ng phÃ¡p tá»‘i Æ°u Gradient Descent 
		PhÆ°Æ¡ng phÃ¡p Gradient Descent lÃ  má»™t trong nhá»¯ng thuáº­t toÃ¡n tá»‘i Æ°u phá»• biáº¿n nháº¥t Ä‘á»ƒ Ä‘iá»u chá»‰nh cÃ¡c tham sá»‘ cá»§a mÃ´ hÃ¬nh nháº±m giáº£m thiá»ƒu hÃ m máº¥t mÃ¡t. QuÃ¡ trÃ¬nh nÃ y Ä‘Æ°á»£c thá»±c hiá»‡n báº±ng cÃ¡ch cáº­p nháº­t cÃ¡c tham sá»‘ cá»§a mÃ´ hÃ¬nh theo hÆ°á»›ng ngÆ°á»£c láº¡i cá»§a gradient cá»§a hÃ m máº¥t mÃ¡t 
		
		## Gradient Descent cÆ¡ báº£n 
			Gradient Descent cÆ¡ báº£n cáº­p nháº­t tham sá»‘ báº±ng cÃ¡ch di chuyá»ƒn theo hÆ°á»›ng ngÆ°á»£c láº¡i cá»§a gradient cá»§a hÃ m máº¥t mÃ¡t, Ä‘Æ°á»£c tÃ­nh toÃ¡n dá»±a trÃªn toÃ n bá»™ táº­p dá»¯ liá»‡u. 
			
				Cáº­p nháº­t tham sá»‘ $\theta$ cÃ³ thá»ƒ Ä‘Æ°á»£c viáº¿t dÆ°á»›i dáº¡ng 
					Î¸â†Î¸-Î·âˆ‚J/âˆ‚Î¸
					
		Trong Ä‘Ã³ 
			$\theta$: cÃ¡c tham sá»‘ cá»§a mÃ´ hÃ¬nh
			$J(\theta)$: hÃ m máº¥t mÃ¡t
			$\eta$: tá»‘c Ä‘á»™ há»c (learning rate)
			$\frac{\partial J}{\partial \theta}$: gradient cá»§a hÃ m máº¥t mÃ¡t Ä‘á»‘i vá»›i cÃ¡c tham sá»‘

		PhÆ°Æ¡ng phÃ¡p nÃ y cÃ³ thá»ƒ cháº­m náº¿u táº­p dá»¯ liá»‡u quÃ¡ lá»›n , vÃ¬ má»—i láº§n tÃ­nh toÃ¡n gradient yÃªu cáº§u duyá»‡t qua toÃ n bá»™ dá»¯ liá»‡u. 
		Äá»ƒ tÄƒng tá»‘c, cÃ³ má»™t sá»‘ biáº¿n thá»ƒ cá»§a Gradient Descent Ä‘Ã£ Ä‘Æ°á»£c phÃ¡t triá»ƒn, má»—i phÆ°Æ¡ng phÃ¡p nháº±m má»¥c Ä‘Ã­ch cáº£i thiá»‡n hiá»‡u quáº£ huáº¥n luyá»‡n
		
		## Stochastic Gradient Descent(SGD)
			Thay vÃ¬ tÃ­nh toÃ¡n gradient dá»±a trÃªn toÃ n bá»™ táº­p dá»¯ liá»‡u, Stochastic Gradient Descent(SGD) cáº­p nháº­t tham sá»‘ dá»±a trÃªn má»™t máº«u nhá» dá»¯ liá»‡u (mini-batch). Äiá»u nÃ y giÃºp giáº£m chi phÃ­ tÃ­nh toÃ¡n vÃ  tÄƒng tá»‘c Ä‘á»™ huáº¥n luyá»‡n, tuy nhiÃªn cÅ©ng cÃ³ thá»ƒ gÃ¢y ra dao Ä‘á»™ng lá»›n hÆ¡n trong quÃ¡ trÃ¬nh há»™i tá»¥. 
			
			### CÃ´ng thá»©c cáº­p nháº­t SGD lÃ  : 
				Î¸â†Î¸-Î·âˆ‚J_i/âˆ‚Î¸ 
				Trong Ä‘Ã³  $J_i$ lÃ  hÃ m máº¥t mÃ¡t cá»§a má»™t Ä‘iá»ƒm dá»¯ liá»‡u ngáº«u nhiÃªn $i$.
				
			### ChÆ°Æ¡ng trÃ¬nh máº«u vá»›i pytorch 
				VÃ­ dá»¥ cho chuá»—i Ä‘áº§u vÃ o lÃ  x = [[1.0], [2.0], [3.0], [4.0]], tÆ°Æ¡ng á»©ng vá»›i Ä‘áº§u ra lÃ  y = [[3.0], [5.0], [7.0], [9.0]]. ChÆ°Æ¡ng trÃ¬nh sáº½ dá»± Ä‘oÃ¡n khi x = [[5.0]] thÃ¬ y lÃ  bao nhiÃªu? 

				StochasticGradientDescentByPytorch.py
		
		## Momentum 
			
			SGD cÃ³ thá»ƒ gáº·p khÃ³ khÄƒn trong viá»‡c há»™i tá»¥ nhanh do dao Ä‘á»™ng máº¡nh khi gradient thay Ä‘á»•i Ä‘á»™t ngá»™t. 
			Äá»ƒ kháº¯c phá»¥c Ä‘iá»u nÃ y, phÆ°Æ¡ng phÃ¡p Momentum Ä‘Æ°á»£c sá»­ dá»¥ng nháº±m giáº£m dao Ä‘á»™ng vÃ  giÃºp quÃ¡ trÃ¬nh há»™i tá»¥ nhanh hÆ¡n 
			Momentum lÆ°u giá»¯ thÃ´ng tin tá»« cÃ¡c gradient trÆ°á»›c Ä‘Ã³ vÃ  tÄƒng tá»‘c theo hÆ°á»›ng cá»§a gradient tÃ­ch lÅ©y. 
			
			### CÃ´ng thá»©c cáº­p nháº­t vá»›i Momentum lÃ  : 
				
				#### Cáº­p nháº­t váº­n tá»‘c (velocity)
					v_t = \gamma v_{t-1} + \eta \frac{\partial J}{\partial \theta} 
					
				Trong Ä‘Ã³ : 
					v_t : Váº­n tá»‘c táº¡i thá»i Ä‘iá»ƒm t, Ä‘áº¡i diá»‡n cho tá»•ng há»£p cÃ¡c gradient trÆ°á»›c Ä‘Ã³ 
					\gamma : há»‡ sá»‘ momen , thÆ°á»ng náº±m trong khoáº£ng  0 <= \gamma < 1 (thÆ°á»ng lÃ  0.9)
					\eta 	: Tá»‘c Ä‘á»™ há»c (learning rate)
					\frac{\partial J}{\partial \theta}  : Gradient cá»§a hÃ m máº¥t mÃ¡t J(\theta) theo tham sá»‘ \theta 
				
		
			### Cáº­p nháº­t tham sá»‘ mÃ´ hÃ¬nh : 
				\theta_t = \theta_{t-1} â€“ v_t 
				
				Trong Ä‘Ã³ : 
				\theta_t  : Tham sá»‘ mÃ´ hÃ¬nh táº¡i thá»i Ä‘iá»ƒm t 
				v_t     : Váº­n tá»‘c Ä‘Ã£ Ä‘Æ°á»£c tÃ­nh á»Ÿ bÆ°á»›c trÃªn 
				
				
		## Adam (Adaptive Moment Estimation)
			
			Adam lÃ  má»™t phÆ°Æ¡ng phÃ¡p tá»‘i Æ°u hiá»‡n Ä‘áº¡i káº¿t há»£p giá»¯a Momentum vÃ  RMSProp(Root Mean Square Propagation)
			PhÆ°Æ¡ng phÃ¡p nÃ y sá»­ dá»¥ng cáº£ hai thÃ´ng tin : moment thá»© nháº¥t (trung bÃ¬nh Ä‘á»™ng cá»§a gradient) vÃ  moment thá»© hai(trung bÃ¬nh Ä‘á»™ng cá»§a bÃ¬nh phÆ°Æ¡ng gradient), giÃºp cÃ¢n báº±ng viá»‡c Ä‘iá»u chá»‰nh tá»‘c Ä‘á»™ há»c vÃ  giá»¯ láº¡i tÃ­nh á»•n Ä‘á»‹nh .
			
			### CÃ´ng thá»©c toÃ¡n há»c : 
				
				#### TÃ­nh toÃ¡n moment báº­c nháº¥t : 
					m_t = \beta_1 m_{t-1} + (1 â€“ \beta_1) \frac{\partial J}{\partial \theta} 
					
					Trong Ä‘Ã³ : 
					$m_t$: Moment báº­c nháº¥t táº¡i thá»i Ä‘iá»ƒm $t$, trung bÃ¬nh Ä‘á»™ng cá»§a gradient.
					$\beta_1$: Há»‡ sá»‘ giáº£m bá»›t moment báº­c nháº¥t, thÆ°á»ng cÃ³ giÃ¡ trá»‹ khoáº£ng 0.9.
					$\frac{\partial J}{\partial \theta}$: Gradient cá»§a hÃ m máº¥t mÃ¡t $J(\theta)$ theo tham sá»‘ $\theta$.	
				
				
				#### TÃ­nh toÃ¡n moment báº­c hai 
					v_t = \beta_2 v_{t-1} + (1 â€“ \beta_2) \left( \frac{\partial J}{\partial \theta} \right)^2 
					
				Trong Ä‘Ã³ 
					$v_t$: Moment báº­c hai táº¡i thá»i Ä‘iá»ƒm $t$, trung bÃ¬nh Ä‘á»™ng cá»§a bÃ¬nh phÆ°Æ¡ng gradient.
				
				#### Hiá»‡u chá»‰nh moment báº­c nháº¥t vÃ  báº­c hai (bias corection)
					\hat{m}_t = \frac{m_t}{1 â€“ \beta_1^t} 
					\hat{v}_t = \frac{v_t}{1 â€“ \beta_2^t}
				
				#### Cáº­p nháº­t tham sá»‘ 
					\theta_t = \theta_{t-1} â€“ \eta \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon} 
					
				Trong Ä‘Ã³ 
					$\eta$: Tá»‘c Ä‘á»™ há»c (learning rate).
					$\epsilon$: Má»™t giÃ¡ trá»‹ ráº¥t nhá» (thÆ°á»ng lÃ  $10^{-8}$) Ä‘á»ƒ trÃ¡nh chia cho 0.
					$\theta_t$: Tham sá»‘ mÃ´ hÃ¬nh táº¡i thá»i Ä‘iá»ƒm $t$.
				
			###  ChÆ°Æ¡ng trÃ¬nh máº«u vá»›i pytorch 
				VÃ­ dá»¥ cho chuá»—i Ä‘áº§u vÃ o lÃ  x = [[1.0], [2.0], [3.0], [4.0]], tÆ°Æ¡ng á»©ng vá»›i Ä‘áº§u ra lÃ  y = [[3.0], [5.0], [7.0], [9.0]]. ChÆ°Æ¡ng trÃ¬nh sáº½ dá»± Ä‘oÃ¡n khi x = [[5.0]] thÃ¬ y lÃ  bao nhiÃªu? 
				
				GradientDescentAdamByPytorch.py
				
	# Káº¿t luáº­n 
		Gradient Descent lÃ  má»™t trong nhá»¯ng thuáº­t toÃ¡n ná»n táº£ng vÃ  quan trá»ng nháº¥t trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n mÃ´ hÃ¬nh AI. Vá»›i kháº£ nÄƒng tá»‘i Æ°u hÃ³a cÃ¡c tham sá»‘ cá»§a mÃ´ hÃ¬nh thÃ´ng qua viá»‡c giáº£m thiá»ƒu hÃ m máº¥t mÃ¡t, Gradient Descent giÃºp cÃ¡c mÃ´ hÃ¬nh há»c mÃ¡y Ä‘áº¡t Ä‘Æ°á»£c Ä‘á»™ chÃ­nh xÃ¡c vÃ  hiá»‡u quáº£ cao hÆ¡n 
		
		Tuy nhiÃªn, Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c tá»‘c Ä‘á»™ há»™i tá»¥ nhanh vÃ  trÃ¡nh cÃ¡c váº¥n Ä‘á» nhÆ° overfitting hay underfitting, viá»‡c Ã¡p dá»¥ng cÃ¡c phÆ°Æ¡ng phÃ¡p tá»‘i Æ°u nhÆ° Momentum, Adam vÃ  Learning Rate Scheduling lÃ  cáº§n thiáº¿t. Báº±ng cÃ¡ch káº¿t há»£p cÃ¡c ká»¹ thuáº­t nÃ y, chÃºng ta khÃ´ng chá»‰ cáº£i thiá»‡n tá»‘c Ä‘á»™ huáº¥n luyá»‡n mÃ  cÃ²n tÄƒng cháº¥t lÆ°á»£ng cá»§a mÃ´ hÃ¬nh  
		Trong tÆ°Æ¡ng lai, tá»‘i Æ°u hÃ³a Gradient Descent sáº½ tiáº¿p tá»¥c Ä‘Ã³ng vai trÃ² quan trá»ng trong sá»± phÃ¡t triá»ƒn cá»§a AI vÃ  há»c mÃ¡y. 

</pre><a id='backBottom' href='../AI-learning-list.html' style='display:none;'>ğŸ”™ Quay láº¡i danh sÃ¡ch</a><br><button onclick='toggleTheme()'>ğŸŒ™ Chuyá»ƒn giao diá»‡n</button></div><script>function toggleTheme() {   let mode = document.body.classList.contains('dark-mode') ? 'light-mode' : 'dark-mode';   document.body.className = mode; localStorage.setItem('theme', mode);   syncTheme();}function applyTheme() {   let savedTheme = localStorage.getItem('theme') || 'dark-mode';   document.body.className = savedTheme;   syncTheme();}function syncTheme() {   let preElement = document.querySelector('pre');   if (document.body.classList.contains('dark-mode')) { preElement.style.background = '#1e1e1e'; preElement.style.color = '#e0e0e0'; }   else { preElement.style.background = '#f5f5f5'; preElement.style.color = '#333333'; }}function checkPageHeight() {   let contentHeight = document.body.scrollHeight;   let windowHeight = window.innerHeight;   if (contentHeight > windowHeight * 1.2) {       document.getElementById('backBottom').style.display = 'block';   } else {       document.getElementById('backBottom').style.display = 'none';   }}</script></body></html>