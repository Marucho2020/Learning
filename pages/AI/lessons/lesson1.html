<html><head><title>Untitled</title><style>body { font-family: Arial, sans-serif; transition: background 0.3s, color 0.3s; }.dark-mode { background-color: #121212; color: #e0e0e0; }.light-mode { background-color: #ffffff; color: #333333; }h1 { text-align: center; color: #73d9f5; }pre { padding: 15px; border-radius: 5px;       white-space: pre-wrap; word-wrap: break-word;       overflow-x: auto; max-width: 100%;       transition: background 0.3s, color 0.3s; }.dark-mode pre { background: #1e1e1e; color: #e0e0e0; }.light-mode pre { background: #f5f5f5; color: #333333; }#backTop, #backBottom {    font-size: 2em; padding: 20px 40px;    background: #bb86fc; color: white; text-decoration: none;    border-radius: 10px; display: inline-block; text-align: center; }#backTop:hover, #backBottom:hover { background: #9b67e2; }button { font-size: 1.5em; padding: 15px 30px;    background: #03dac6; color: #121212; border: none;    cursor: pointer; border-radius: 5px; display: block; margin: 10px auto; }button:hover { background: #02b8a3; }.dark-mode a { color: #03dac6; } .light-mode a { color: #007bff; }</style></head><body onload='applyTheme(); checkPageHeight()'><div class='container'><a id='backTop' href='../java-learning-list.html'>ğŸ”™ Quay láº¡i danh sÃ¡ch</a><br><h1>Untitled</h1><pre>//============================ Lá»‹ch sá»­ phÃ¡t triá»ƒn vÃ  á»©ng dá»¥ng thá»±c tiá»…n trong Ä‘á»i sá»‘ng ==========================// 
# TrÃ­ tuá»‡ nhÃ¢n táº¡o lÃ  gÃ¬ 
	TrÃ­ tuá»‡ nhÃ¢n táº¡o (Artigicial Intelligence - AI )lÃ  má»™t lÄ©nh vá»±c cá»§a khoa há»c mÃ¡y tÃ­nh táº­p trung vÃ o viá»‡c táº¡o ra cÃ¡c há»‡ thá»‘ng cÃ³ kháº£ nÄƒng thá»±c hiá»‡n cÃ¡c nháº­m vá»¥ Ä‘Ã²i há»i trÃ­ tuá»‡ cá»§a con ngÆ°á»i. Nhá»¯ng nhiá»‡m vá»¥ nÃ y bao gá»“m há»c há»i, lÃ½ luáº­n, nháº­n thá»©c, hiá»ƒu Ä‘Æ°á»£c ngÃ´n ngá»¯ tá»± nhiÃªn, nháº­n diá»‡n hÃ¬nh áº£nh, vÃ  tháº­m chÃ­ lÃ  ra quyáº¿t Ä‘á»‹nh. AI cÃ³ thá»ƒ Ä‘Æ°á»£c xem nhÆ° lÃ  má»™t cá»‘ gáº¯ng Ä‘á»ƒ lÃ m cho mÃ¡t mÃ³c thÃ´ng minh hÆ¡n, cÃ³ kháº£ nÄƒng thá»±c hiá»‡n Ä‘Æ°á»£c cÃ¡c cÃ´ng viá»‡c phá»©c táº¡p mÃ  trÆ°á»›c Ä‘Ã¢y chá»‰ cÃ³ con ngÆ°á»i má»›i cÃ³ thá»ƒ lÃ m Ä‘Æ°á»£c. 
	
# Lá»‹ch sá»­ phÃ¡t triá»ƒn trÃ­ tuá»‡ nhÃ¢n táº¡o 
	## 2.1 Giai Ä‘oáº¡n ná»n mÃ³ng 
		
		1943 : 
			Hai nhÃ  khoa há»c Warren McCulloch vÃ  Walter Pitts Ä‘Ã£ Ä‘á» xuáº¥t mÃ´ hÃ¬nh toÃ¡n há»c Ä‘áº§u tiÃªn cá»§a má»™t máº¡ng nÆ¡-ron,má»Ÿ Ä‘áº§u cho sá»± phÃ¡t triá»ƒn cá»§a trÃ­ tuá»‡ nhÃ¢n táº¡o 
		1950 :
			Alan Turing, Má»™t nhÃ  toÃ n há»c ngÆ°á»i Anh, Ä‘Ã£ Ä‘Æ°a ra bÃ i kiá»ƒm tra Turing ná»•i tiáº¿ng nháº±m xÃ¡c Ä‘á»‹nh liá»‡u má»™t mÃ¡y tÃ­nh cÃ³ thá»ƒ biá»ƒu hiá»‡n trÃ­ thÃ´ng minh ngnag vá»›i con ngÆ°á»i hay khÃ´ng. 
		1956 : 
			Thuáº­t ngá»¯ "Artificial Intelligence" chÃ­nh thá»©c ra Ä‘á»i táº¡i há»™i nghá»‹ Dartmouth, Ä‘Æ°á»£c xem lÃ  cá»™t má»‘c khai sinh cá»§a lÄ©nh vá»±c nÃ y. 
		1966 : 
			ChÆ°Æ¡ng trÃ¬nh ELIZA, má»™t chatbot Ä‘Æ¡n giáº£n mÃ´ phá»ng cuá»™c Ä‘á»‘i thoáº¡i cá»§a con ngÆ°á»i Ä‘Ã£ ra Ä‘á»i. DÃ¹ háº¡n cháº¿ nhÆ°ng nÃ³ Ä‘Ã£ má»Ÿ Ä‘áº§u cho cÃ¡c nghiÃªn cá»©u vá» xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn. 
	
	## Giai Ä‘oáº¡n thá»±c táº¿ 
	
		1970-1980 :
			CÃ¡c chuyÃªn gia(Expert Systems) nhÆ° DENDRAL vÃ  MYCIN Ä‘Æ°á»£c phÃ¡t triá»ƒn á»©ng, á»©ng dá»¥ng AI vÃ o cÃ¡c lÄ©nh vá»±c cá»¥ thá»ƒ nhÆ° hÃ³a há»c vÃ  y há»c. 
		1997 
			MÃ¡y tÃ­nh Deep Blue cá»§a IBM Ä‘Ã¡nh báº¡i nhÃ  vÃ´ Ä‘á»‹ch cá» vua tháº¿ giá»›i Garry Kasparov, chá»©ng tá» sá»©c máº¡nh cá»§a AI trong viá»‡c giáº£i quyáº¿t cÃ¡c váº¥n Ä‘á» phá»©c táº¡p 
	
	## 2.3 Giai Ä‘oáº¡n bÃ¹ng ná»• 
	
		2011 : 
			IBM Watson, Má»™t há»‡ thá»‘ng AI máº¡nh máº½ Ä‘Ã£ chiáº¿n tháº¯ng trong cuá»™c thi Jeopardy! , Ä‘Ã¡nh báº¡i hai nhÃ  vÃ´ Ä‘á»‹ch cá»§a con ngÆ°á»i. ÄÃ¢y lÃ  má»™t cá»™t má»‘c quan trá»ng chá»©ng minh kháº£ nÄƒng cá»§a AI trong viá»‡c xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn vÃ  phÃ¢n tÃ­ch dá»¯ liá»‡u phá»©c táº¡p. 
		
		2012 :
			Má»™t bÆ°á»›c Ä‘á»™t phÃ¡ lá»›n tá»ng AI lÃ  sá»± phÃ¡t triá»ƒn cá»§a thuáº­t toÃ¡n há»c sÃ¢u(deep- learning), Ä‘áº·c biá»‡t lÃ  máº¡ng nÆ¡-ron tÃ­ch cháº­p(Convolutional Neural Networks- CNN). AlexNet, má»™t mÃ´ hÃ¬nh há»c sÃ¢u Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi Alex Krizhevsky vÃ  cÃ¡c Ä‘á»“ng nghiá»‡p, Ä‘Ã£ giÃ nh chiáº¿n tháº±ng táº¡i cuá»™c thi ImageNet, Ä‘Ã¡nh dáº¥u sá»± trá»—i dáº­y cá»§a deep learning trong nháº­n diá»‡n hÃ¬nh áº£nh 
			
		2014 : 
			Google DeepMind giá»›i thiá»‡u AlphaGO, má»™t chÆ°Æ¡ng trÃ¬nh AI cÃ³ kháº£ nÄƒng chÆ¡i cá» vÃ¢y. Äáº¿n nÄƒm 2016 AlphaGO Ä‘Ã¡nh báº¡i nhÃ  vÃ´ Ä‘á»‹ch tháº¿ giá»›i Lee Sedol, sá»± kiá»‡n nÃ y Ä‘Æ°á»£c coi lÃ  má»™t trong nhá»¯ng bÆ°á»›c tiáº¿n lá»›n nháº¥t cá»§a AI trong lÄ©nh vá»±c trÃ² chÆ¡i 
			
		2015 : 
			OpenAI, Má»™t tá»• chá»©c nghiÃªn cá»©u AI phi lá»£i nhuáº­n, Ä‘Æ°á»£c thÃ nh láº­p vá»›i má»¥c tiÃªu Ä‘áº£m báº£o ráº±ng trÃ­ tuá»‡ nhÃ¢n táº¡o mang láº¡i lá»£i Ã­ch cho toÃ n nhÃ¢n loáº¡i. OPENAI Ä‘Ã£ nhanh chÃ³ng trá»Ÿ thÃ nh má»™t trong nhá»¯ng trung tÃ¢m nghiÃªn cá»©u AI hÃ ng Ä‘áº§u tháº¿ giá»›i. 
		2016 : 
			Sá»± xuáº¥t hiá»‡n cá»§a cÃ¡c trá»£ lÃ½ áº£o thÃ´ng minh nhÆ° Amazon Alexa, Google Assistant, Microsoft Cortana Ä‘Ã£ Ä‘Æ°a AI vÃ o Ä‘á»i sá»‘ng hÃ ng ngÃ y cá»§a ngÆ°á»i tiÃªu dÃ¹ng, giÃºp phá»• biáº¿n cÃ´ng nghá»‡ nÃ y trÃªn quy mÃ´ lá»›n 
		
		2017 : 
			Google giá»›i thiá»‡u mÃ´ hÃ¬nh Transformer, Má»™t kiáº¿n trÃºc má»›i trong há»c sÃ¢u Ä‘Ã£ cÃ¡ch máº¡ng hÃ³a xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn(Narital Language Processing - NLP ). Transformer lÃ  cÆ¡ sá»Ÿ cho cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ máº¡nh máº½ nhÆ° BERT vÃ  GPT (Generative Pre-trained Transformer)
			
	## 2.4 Giai Ä‘oáº¡n trÃ­ tuá»‡ tá»•ng quÃ¡t 
		2020 : 
			GPT-3 , Má»™t mÃ´ hÃ¬nh ngÃ´n ngá»¯ khá»•ng lá»“ vá»›i 175 tá»· tham sá»‘, Ä‘Æ°á»£c OpenAI giá»›i thiá»‡u, GPT-3 Ä‘Ã£ thá»ƒ hiá»‡n kháº£ nÄƒng Ä‘Ã¡ng kinh ngáº¡c trong viá»‡c táº¡o ra vÄƒn báº£n tá»± nhiÃªn, tráº£ lá»i cÃ¢u há»i, dá»‹ch thuáº­t, vÃ  tháº­m chÃ­ lÃ  sÃ¡ng táº¡o ná»™i dung. ÄÃ¢y lÃ  má»™t trong nhá»¯ng mÃ´ hÃ¬nh AI tiÃªn tiáº¿n nháº¥t thá»i Ä‘iá»ƒm Ä‘Ã³ 
		2021 : 
			Alpha Fold cá»§a DeepMind Ä‘Ã£ Ä‘áº¡t Ä‘Æ°á»£c bÆ°á»›c Ä‘á»™t phÃ¡ lá»›n trong sinh há»c, vá»›i kháº£ nÄƒng dá»± Ä‘oÃ¡n cáº¥u trÃºc protein tá»« chuá»—i amino acid vá»›i Ä‘á»™ chÃ­nh xÃ¡c cau. Äiá»u nÃ y Ä‘Ã£ giÃºp giáº£i quyáº¿t má»™t trong nhá»¯ng thÃ¡ch thá»©c lá»›n nháº¥t trong khoa há»c sá»± sá»‘ng vÃ  má»Ÿ ra cÆ¡ há»™i lá»›n trong nghiÃªn cá»©u y há»c 
		2022 : 
			ChatGPT, dá»±a trÃªn mÃ´ hÃ¬nh GPT-3.5, Ä‘Ã£ Ä‘Æ°á»£c phÃ¡t hÃ nh bá»Ÿi OpenAI , vÃ  trá»Ÿ thÃ nh má»™t trong nhá»¯ng chatbot AI phá»• biáº¿n nháº¥t, Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i trong nhiá»u lÄ©nh vá»±c tá»« giÃ¡o dá»¥c, tiáº¿p thá»‹ Ä‘áº¿n há»— trá»£ khÃ¡ch hÃ ng. Kháº£ nÄƒng táº¡o ra cÃ¡c Ä‘oáº¡n vÄƒn báº£n cÃ³ ngá»¯ cáº£nh phá»©c táº¡p vÃ  tÆ°Æ¡ng tÃ¡c tá»± nhiÃªn cá»§a ChatGPT Ä‘Ã£ thu hÃºt sá»± chÃº Ã½ lá»›n tá»« cÃ´ng chÃºng 
			
		2023 : 
			GPT-4 Ä‘Æ°á»£c ra máº¯t, mang láº¡i nhiá»u cáº£i tiáº¿n so vá»›i phiÃªn báº£n trÆ°á»›c Ä‘Ã³, vá»›i kháº£ nÄƒng xá»­ lÃ½ vÄƒn báº£n tá»‘t hÆ¡n, hiá»ƒu ngá»¯ cáº£nh sÃ¢u hÆ¡n vÃ  há»— trá»£ nhiá»u ngÃ´n ngá»¯ khÃ¡c nhau. GPT-4 Ä‘Ã£ Ä‘Æ°á»£c tÃ­ch há»£p vÃ o nhiá»u ná»n táº£ng vÃ  á»©ng dá»¥ng, tá»« dá»‹ch vá»¥ khÃ¡ch hÃ ng Ä‘áº¿n táº¡o ná»™i dung sÃ¡ng táº¡o  
		
			
# 3 CÃ¡c loáº¡i hÃ¬nh trÃ­ tuá»‡ nhÃ¢n táº¡o 
	
	AI cÃ³ thá»ƒ Ä‘Æ°á»£c chia thÃ nh 3 loáº¡i chÃ­nh dá»±a trÃªn má»©c Ä‘á»™ thÃ´ng minh vÃ  kháº£ nÄƒng cá»§a chÃºng 
	
	## AI háº¹p(Narrow AI )
			CÃ²n Ä‘Æ°á»£c gá»i lÃ  AI yáº¿u, loáº¡i nÃ y chá»‰ cÃ³ thá»ƒ thá»±c hiá»‡n má»™t nhiá»‡m vá»¥ cá»¥ thá»ƒ. VÃ­ dá»¥ Ä‘iá»ƒn hÃ¬nh lÃ  cÃ¡c há»‡ thá»‘ng nháº­n diá»‡n giá»ng nÃ³i nhÆ° Siri hay Google Assitant.

	## AI Tá»•ng quÃ¡t(General AI )
		ÄÃ¢y lÃ  loáº¡i AI cÃ³ kháº£ nÄƒng thá»±c hiá»‡n báº¥t ká»³ nhiá»‡m vá»¥ trÃ­ tuá»‡ nÃ o mÃ  con ngÆ°á»i cÃ³ thá»ƒ lÃ m. Tuy nhiÃªn General AI hiá»‡n táº¡i váº«n chá»‰ tá»“n táº¡i trong lÃ½ thuyáº¿t vÃ  nghiÃªn cá»©u 
		
	## AI SiÃªu viá»‡t(Super AI )
		ÄÃ¢y lÃ  viá»…n cáº£nh vá» AI vÆ°á»£t trá»™i hÆ¡n con ngÆ°á»i trong má»i khÃ­a cáº¡nh, tá»« sÃ¡ng táº¡o Ä‘áº¿n trÃ­ thÃ´ng minh xÃ£ há»™i. Tuy nhiÃªn, loáº¡i hÃ¬nh AI nÃ y váº«n cÃ²n ráº¥t xa vá»›i vÃ  lÃ  má»™t chá»§ Ä‘á» cá»§a nhiá»u tranh luáº­n vá» Ä‘áº¡o Ä‘á»©c vÃ  an toÃ n. 

# á»¨ng dá»¥ng cá»§a trÃ­ tuá»‡ nhÃ¢n táº¡o 
	AI Ä‘ang thay Ä‘á»•i cÃ¡ch chÃºng ta sá»‘ng vÃ  lÃ m viá»‡c. DÆ°á»›i Ä‘Ã¢y lÃ  má»™t sá»‘ lÄ©nh vá»±c mÃ  AI Ä‘Ã£ cÃ³ tÃ¡c Ä‘á»™ng máº¡nh máº½: 
		
		## Y táº¿ : 
			AI Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ phÃ¢n tÃ­ch hÃ¬nh áº£nh y khoa, dá»± Ä‘oÃ¡n bá»‡nh táº­t vÃ  há»— trá»£ trong viá»‡c phÃ¡t triá»ƒn thuá»‘c má»›i 
			CÃ¡c há»‡ thá»‘ng AI nhÆ° Watson cá»§a IBM Ä‘Ã£ giÃºp bÃ¡c sÄ© Ä‘Æ°a ra chuáº©n Ä‘oÃ¡n vÃ  Ä‘á» xuáº¥t cÃ¡c phÆ°Æ¡ng phÃ¡p Ä‘iá»u trá»‹  
		## Giao thÃ´ng 
			CÃ¡c há»‡ thá»‘ng xe tá»± lÃ¡i, há»‡ thá»‘ng Ä‘á»‹nh vá»‹ thÃ´ng minh vÃ  quáº£n lÃ½ giao thÃ´ng Ä‘á»u sá»­ dá»¥ng AI Ä‘á»ƒ nÃ¢ng cao Ä‘á»™ an toÃ n vÃ  hiá»‡u quáº£. 
		## TÃ i chÃ­nh 
			AI giÃºp phÃ¢n tÃ­ch dá»¯ liá»‡u tÃ i chÃ­nh, dá»± Ä‘oÃ¡n xu hÆ°á»›ng thá»‹ trÆ°á»ng vÃ  phÃ¡t hiá»‡n gian láº­n. CÃ¡c bot giao dá»‹ch tá»± Ä‘á»™ng cá»—ng Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i trÃªn cÃ¡c sÃ n giao dá»‹ch 
		## Giáº£i trÃ­ 
			Tá»« cÃ¡c há»‡ thá»‘ng gá»£i Ã½ phim trÃªn Netflix hay Spotify Ä‘áº¿n cÃ¡c trÃ² chÆ¡i Ä‘iá»‡n tá»­ thÃ´ng minh, AI Ä‘ang lÃ m thay Ä‘á»•i cÃ¡ch chÃºng ta tiÃªu thá»¥ ná»™i dung giáº£i trÃ­ 
		## GiÃ¡o dá»¥c 
			AI cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ táº¡o ra cÃ¡c chÆ°Æ¡ng trÃ¬nh há»c táº­p cÃ¡ nhÃ¢n hÃ³a, giÃºp há»c sinh há»c táº­p hiá»‡u quáº£ hÆ¡n vÃ  giÃ¡o viÃªn quáº£n lÃ½ lá»›p há»c dá»… dÃ ng hÆ¡n
		## An ninh máº¡ng 
			AI giÃºp phÃ¡t hiá»‡n vÃ  ngÄƒn cháº·n cÃ¡c má»‘i Ä‘e dá»a máº¡ng má»™t cÃ¡ch nhanh chÃ³ng vÃ  chÃ­nh xÃ¡c hÆ¡n so vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p truyá»n thá»‘ng 
			
# 5. Káº¿t luáº­n 
		TrÃ­ tuá»‡ nhÃ¢n táº¡o lÃ  má»™t trong nhá»¯ng cÃ´ng nghá»‡ Ä‘á»™t phÃ¡t nháº¥t cá»§a tháº¿ ká»· 21, vá»›i tiá»m nÄƒng thay Ä‘á»•i má»i khÃ­a cáº¡nh cá»§a cuá»™c sá»‘ng. Tá»« nhá»¯ng khá»Ÿi Ä‘áº§u khiÃªm tá»‘n trong cÃ¡c phÃ²ng thÃ­ nghiá»‡m khoa há»c, AI Ä‘Ã£ phÃ¡t triá»ƒn thÃ nh má»™t cÃ´ng cá»¥ máº¡nh máº½ Ä‘Æ°á»£c á»©ng dá»¥ng trong nhiá»u lÄ©nh vá»±c 
		Tuy nhiÃªn, Ä‘á»ƒ AI thá»±c sá»± trá»Ÿ thÃ nh má»™t lá»±c lÆ°á»£ng tá»‘t, chÃºng ta cáº§n tiáº¿p tá»¥c nghiÃªn cá»©u vÃ  phÃ¡t triá»ƒn nÃ³ theo hÆ°á»›ng an toÃ n, Ä‘áº¡o Ä‘á»©c vÃ  cÃ³ trÃ¡ch nhiá»‡m  
			
			
			

//===========================================================================================================================================================//
//======================== Tá»« Ä‘iá»ƒn, cÃ¡c tá»« khÃ³a, thuáº­t ngá»¯ sá»­ dá»¥ng trong lÄ©nh vá»±c AI ==========================================// 
//===========================================================================================================================================================//



# 1 CÆ¡ báº£n vá» trÃ­ tuá»‡ nhÃ¢n táº¡o 
		
		
	## Artificial Intelligence - AI 
		Artificial Intelligence : lÃ  lÄ©nh vá»±c nghiÃªn cá»©u vÃ  phÃ¡t triá»ƒn cÃ¡c há»‡ thá»‘ng mÃ¡y tÃ­nh cÃ³ kháº£ nÄƒng thá»±c hiá»‡n cÃ¡c nhiá»‡m vá»¥ mÃ  trÆ°á»›c Ä‘Ã¢y yÃªu cáº§u trÃ­ tuá»‡ cá»§a con ngÆ°á»i, nhÆ° nháº­n dáº¡ng giá»ng nÃ³i, nháº­n dáº¡ng hÃ¬nh áº£nh, ra quyáº¿t Ä‘á»‹nh vÃ  ngÃ´n ngá»¯ 
		
	## Machine Learning - ML 
		Machine Learning : LÃ  má»™t nhÃ¡nh cá»§a AI , táº­p trung vÃ o viá»‡c phÃ¡t triá»ƒn cÃ¡c thuáº­t toÃ¡n vÃ  mÃ´ hÃ¬nh cho phÃ©p mÃ¡y tÃ­nh há»c tá»« dá»¯ liá»‡u mÃ  khÃ´ng cáº§n láº­p trÃ¬nh rÃµ rÃ ng. CÃ¡c á»©ng dá»¥ng phá»• biáº¿n cá»§a há»c mÃ¡y bao gá»“m phÃ¢n loáº¡i, há»“i quy, vÃ  dá»± Ä‘oÃ¡n 
		
	## Deep Learning - DL 
		Deeplearning : LÃ  má»™t pháº§n cá»§a há»c mÃ¡y, sá»­ dá»¥ng cÃ¡c máº¡ng nÆ¡ ron nhÃ¢n táº¡o vá»›i nhiá»u lá»›p(deep neural networks) Ä‘á»ƒ mÃ´ phá»ng cÃ¡ch bá»™ nÃ£o con ngÆ°á»i hoáº¡t Ä‘á»™ng, tá»« Ä‘Ã³ giáº£i quyáº¿t cÃ¡c bÃ i toÃ¡n phá»©c táº¡p nhÆ° nháº­n dáº¡ng giá»ng nÃ³i vÃ  thá»‹ giÃ¡c mÃ¡y tÃ­nh. 
	
	## Natural Language Processing - NLP 
		Natural Language Processing : lÃ  lÄ©nh vá»±c nghiÃªn cá»©u vÃ  phÃ¡t triá»ƒn cÃ¡c thuáº­t toÃ¡n cho phÃ©p mÃ¡y tÃ­nh hiá»ƒu, diá»…n giáº£i vÃ  pháº£n há»“i ngÃ´n ngá»¯ tá»± nhiÃªn cá»§a con ngÆ°á»i. CÃ¡c á»©ng dá»¥ng cá»§a NLP bao gá»“m chat bot, dá»‹ch mÃ¡y vÃ  phÃ¢n tÃ­ch cáº£m xÃºc . 
		
	## Computer Vision 
		Computer Vision : LÃ  lÄ©nh vá»±c cá»§a AI chuyÃªn vá» viá»‡c cho phÃ©p mÃ¡y tÃ­nh nhÃ¬n tháº¥y vÃ  hiá»ƒu Ä‘Æ°á»£c thÃ´ng tin tá»« hÃ¬nh áº£nh vÃ  video. NÃ³ Ä‘Æ°á»£c á»©ng dá»¥ng trong nhiá»u lÄ©nh vá»±c nhÆ° nháº­n dáº¡ng khuÃ´n máº·t, xe tá»± lÃ¡i vÃ  phÃ¢n tÃ­ch video 
		
	## Artificial General Intelligence - AGI 
		Artificial General Intelligence : LÃ  hÃ¬nh thá»©c AI tiÃªn tiáº¿n cÃ³ kháº£ nÄƒng thá»±c hiá»‡n báº¥t ká»³ nhiá»‡m vá»¥ trÃ­ tuá»‡ nÃ o mÃ  con ngÆ°á»i cÃ³ thá»ƒ lÃ m. Máº·c dÃ¹ AGI hiá»‡n nay chá»‰ lÃ  má»™t khÃ¡i niá»‡m lÃ½ thuyáº¿t, nhÆ°ng nÃ³ lÃ  má»¥c tiÃªu cuá»‘i cÃ¹ng cá»§a nhiá»u nghiÃªn cá»©u AI 
		
	## Narrow AI 
		Narrow AI lÃ  loáº¡i AI Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ thá»±c hiá»‡n má»™t nhiá»‡m vá»¥ cá»¥ thá»ƒ, cháº³ng háº¡n nhÆ° dá»± Ä‘oÃ¡n xu hÆ°á»›ng mua hÃ ng hoáº·c nháº­n dáº¡ng khuÃ´n máº·t. ÄÃ¢y lÃ  dáº¡ng AI phá»• biáº¿n nháº¥t hiá»‡n nay 


# MÃ´ hÃ¬nh máº¡ng NÆ¡-ron(Model and Neural Networks)

	##	AI Model 
		### AI Model lÃ  sá»± triá»ƒn khai cá»§a cÃ¡c thuáº­t toÃ¡n AI, Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn dá»¯ liá»‡u Ä‘á»ƒ thá»±c hiá»‡n má»™t nhiá»‡m vá»¥ cá»¥ thá»ƒ, nhÆ° phÃ¢n loáº¡i hÃ¬nh áº£nh hoáº·c dá»± Ä‘oÃ¡n xu hÆ°á»›ng thá»‹ trÆ°á»ng  
	
	## Artificial Neural Networks - ANN 
	
		### Artificial Neural Networks lÃ  mÃ´ hÃ¬nh há»c mÃ¡y mÃ´ phá»ng cÃ¡ch thá»©c hoáº¡t Ä‘á»™ng cá»§a bá»™ nÃ£o con ngÆ°á»i, sá»­ dá»¥ng cÃ¡c Ä‘Æ¡n vá»‹ tÃ­nh toÃ¡n gá»i lÃ  nÆ¡-ron nhÃ¢n táº¡o Ä‘á»ƒ truyá»n vÃ  xá»­ lÃ½ thÃ´ng tin  
		
		### Neuron(Perceptron)
					LÃ  thÃ nh pháº§n cÆ¡ báº£n nháº¥t trong má»™t máº¡ng nÆ¡-ron nhÃ¢n táº¡o (Artificial Neural Network). NÃ³ hoáº¡t Ä‘Ã´ng nhÆ° má»™t Ä‘Æ¡n vá»‹ tÃ­nh toÃ¡n, mÃ´ phá»ng cÃ¡ch má»™t nÆ¡-ron sinh há»c xá»­ lÃ½ thÃ´ng tin. Perceptron nháº­n má»™t hoáº·c nhiá»u Ä‘áº§u vÃ o, gÃ¡n trá»ng sá»‘ cho tá»«ng Ä‘áº§u vÃ o, cá»™ng chÃºng láº¡i, thÃªm bias(náº¿u cÃ³), rá»“i Ã¡p dá»¥ng má»™t hÃ m kÃ­ch hoáº¡t Ä‘á»ƒ táº¡o ra Ä‘áº§u ra.
			
		### Activation Function 
			LÃ  má»™t hÃ m toÃ¡n há»c Ä‘Æ°á»£c Ã¡p dá»¥ng cho má»—i neuron trong máº¡ng nÆ¡-ron, quyáº¿t Ä‘á»‹nh Ä‘áº§u ra cá»§a neuron Ä‘Ã³ dá»±a trÃªn tá»•ng trá»ng sá»‘ cá»§a cÃ¡c Ä‘áº§u vÃ o. HÃ m kÃ­ch hoáº¡t giÃºp máº¡ng há»c Ä‘Æ°á»£c cÃ¡c má»‘i quan há»‡ phi tuyáº¿n tÃ­nh, tá»« Ä‘Ã³ lÃ m tÄƒng kháº£ nÄƒng cá»§a máº¡ng trong viá»‡c giáº£i quyáº¿t cÃ¡c bÃ i toÃ¡n phá»©c táº¡p.
		
		### Back Propagation 
			LÃ  má»™t thuáº­t toÃ¡n quan trá»ng trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n máº¡ng nÆ¡-ron nhÃ¢n táº¡o(Artificial Neural Networks - ANN ). NÃ³ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ tá»‘i Æ°u hÃ³a cÃ¡c trá»ng sá»‘(weight) trong máº¡ng báº±ng cÃ¡ch cáº­p nháº­t chÃºng dá»±a trÃªn lá»—i(loss) mÃ  mÃ´ hÃ¬nh táº¡o ra, qua Ä‘Ã³ giÃºp cáº£i thiá»‡n hiá»‡u suáº¥t cá»§a mÃ´ hÃ¬nh 
			
		### Forward propagation 
			LÃ  quÃ¡ trÃ¬nh truyá»n dá»¯ liá»‡u tá»« Ä‘áº§u vÃ o qua cÃ¡c lá»›p trong máº¡ng nÆ¡-ron, tÃ­nh toÃ¡n cÃ¡c giÃ¡ trá»‹ Ä‘áº§u ra cá»§a tá»«ng lá»›p dá»±a trÃªn trá»ng sá»‘ vÃ  hÃ m kÃ­ch hoáº¡t. ÄÃ¢y lÃ  bÆ°á»›c Ä‘áº§u tiÃªn trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n máº¡ng nÆ¡-ron vÃ  cÅ©ng lÃ  bÆ°á»›c Ä‘á»ƒ máº¡ng nÆ¡-ron táº¡o ra dá»± Ä‘oÃ¡n(prediction)
			
			
	## Convolutional Neural Networks - CNN 
		### Convolutional Neural Networks 
			lÃ  má»™t loáº¡i máº¡ng nÆ¡-ron Ä‘Æ°á»£c tháº¿t káº¿ Ä‘áº·c biá»‡t Ä‘á»ƒ xá»­ lÃ½ dá»¯ liá»‡u dáº¡ng hÃ¬nh áº£nh. NÃ³ sá»­ dá»¥ng cÃ¡c táº§ng tÃ­ch cháº­p(convolutional layers) Ä‘á»ƒ tá»± Ä‘á»™ng phÃ¡t hiá»‡n cÃ¡c Ä‘áº·c trÆ°ng trong áº£nh. 
			
			
		### Convolutional Layer 
			LÃ  má»™t thÃ nh pháº§n cÆ¡ báº£n trong Convolutional Neural Networks(CNN), Ä‘Æ°á»£c sá»­ dá»¥ng chá»§ yáº¿u trong cÃ¡c bÃ i toÃ¡n xá»­ lÃ½ hÃ¬nh áº£nh vÃ  video, nhÆ°ng cÅ©ng cÃ³ thá»ƒ Ã¡p dá»¥ng cho cÃ¡c loáº¡i dá»¯ liá»‡u khÃ¡c. Lá»›p nÃ y thá»±c hiá»‡n má»™t phÃ©p toÃ¡n tÃ­ch cáº­p(convolution) giá»¯a Ä‘áº§u vÃ o vÃ  cÃ¡c bá»™ lá»c(filter) hoáº·c kernel, giÃºp trÃ­ch xuáº¥t cÃ¡c Ä‘áº·c trÆ°ng(features) tá»« dá»¯ liá»‡u. 
			
		### Kernel/Filter 
			LÃ  má»™t ma tráº­n nhá» Ä‘Æ°á»£c sá»­ dá»¥ng trong quÃ¡ trÃ¬nh Convolution(tÃ­ch cháº­p) trong Convolutional Neural Networks (CNN). Kernel hay Filter cÃ³ nhiá»‡m vá»¥ trÃ­ch xuáº¥t cÃ¡c Ä‘áº·c trÆ°ng(features) tá»« Ä‘áº§u vÃ o nhÆ° hÃ¬nh áº£nh, Ã¢m thanh hoáº·c cÃ¡c dá»¯ liá»‡u khÃ¡c. Má»—i kernel Ä‘Æ°á»£c há»c trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n cá»§a máº¡ng nÆ¡-ron vÃ  cÃ³ nhiá»‡m vá»¥ tÃ¬m kiáº¿m cÃ¡c Ä‘áº·c trÆ°ng trong dá»¯ liá»‡u Ä‘áº§u vÃ o nhÆ° cÃ¡c cáº¡nh(edges) ,  gÃ³c(cornes), káº¿t cáº¥u(textures), vÃ  cÃ¡c máº«u phá»©c táº¡p hÆ¡n 
			
		### Stride 
			LÃ  má»™t tham sá»‘ quan trá»ng trong quÃ¡ trÃ¬nh Convolution(tÃ­ch cháº­p) trong Convolutional Neural Networks(CNN). Stride xÃ¡c Ä‘á»‹nh má»©c Ä‘á»™ di chuyá»ƒn cá»§a kernel/filter qua dá»¯ liá»‡u Ä‘áº§u vÃ o trong má»—i láº§n quÃ©t. NÃ³ áº£nh hÆ°á»Ÿng trá»±c tiáº¿p Ä‘áº¿n kÃ­ch thÆ°á»›c cá»§a feature map(báº£n Ä‘á»“ Ä‘áº·c trÆ°ng) Ä‘áº§u ra. Stride thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ kiá»ƒm soÃ¡t kÃ­ch thÆ°á»›c cá»§a feature map Ä‘áº§u ra vÃ  cÅ©ng giÃºp giáº£m thiá»ƒu sá»‘ lÆ°á»£ng phÃ©p toÃ¡n cáº§n thiáº¿t trong máº¡ng nÆ¡-ron. 
			
		### Padding 
			LÃ  má»™t ká»¹ thuáº­t trong Convolutional Neural Networks(CNN) Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ thÃªm cÃ¡c giÃ¡ trá»‹(thÆ°á»ng lÃ  giÃ¡ trá»‹ 0) vÃ o xung quanh biÃªn cá»§a dá»¯ liá»‡u Ä‘áº§u vÃ o, cháº³ng háº¡n nhÆ° hÃ¬nh áº£nh, trÆ°á»›c khi Ã¡p dá»¥ng phÃ©p tÃ­ch cháº­p(convolution). Khi kernel(bá»™ lá»c) quÃ©t qua Ä‘áº§u vÃ o, cÃ¡c giÃ¡ trá»‹ á»Ÿ biÃªn cá»§a áº£nh cÃ³ thá»ƒ khÃ´ng cÃ³ Ä‘á»§ pixel xung quanh Ä‘á»ƒ tÃ­nh toÃ¡n vá»›i kernel(vÃ­ dá»¥, khi kernel khÃ´ng thá»ƒ quÃ©t hoÃ n toÃ n qua má»™t vÃ¹ng á»Ÿ gÃ³c cá»§a áº£nh). Padding giÃºp giá»¯ cho cÃ¡c thÃ´ng tin á»Ÿ biÃªn áº£nh khÃ´ng bá»‹ máº¥t Ä‘i. 
		
		### Pooling Layer 
			LÃ  má»™t thÃ nh pháº§n quan trá»ng trong Convolutional Neural Networks(CNN), giÃºp giáº£m kÃ­ch thÆ°á»›c cá»§a dá»¯ liá»‡u Ä‘áº§u vÃ o, tá»« Ä‘Ã³ giáº£m sá»‘ lÆ°á»£ng tham sá»‘ vÃ  Ä‘á»™ phá»©c táº¡p cá»§a tÃ­nh toÃ¡n, Ä‘á»“ng thá»i giá»¯ láº¡i cÃ¡c Ä‘áº·c trÆ°ng quan trá»ng cá»§a dá»¯ liá»‡u. Pooling layer giÃºp giáº£m Ä‘á»™ phÃ¢n giáº£i cá»§a feature map, giÃºp giáº£m sá»‘ lÆ°á»£ng phÃ©p toÃ¡n cáº§n thiáº¿t trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n vÃ  lÃ m cho mÃ´ hÃ¬nh Ã­t bá»‹ overfiting hÆ¡n 
	


	## Recurrent Neural Networks - RNN 
		
		### Recurrent Neural Networks lÃ  má»™t loáº¡i máº¡ng nÆ¡-ron chuyÃªn xá»­ lÃ½ dá»¯ liá»‡u tuáº§n tá»±, nhÆ° vÄƒn báº£n hoáº·c chuá»—i thá»i gian, nhá» kháº£ nÄƒng ghi nhá»› thÃ´ng tin tá»« cÃ¡c bÆ°á»›c trÆ°á»›c Ä‘Ã³ Ä‘á»ƒ Ä‘Æ°a ra dá»± Ä‘oÃ¡n 
		
		### Hidden State 
			LÃ  má»™t khÃ¡i niá»‡m cá»‘t lÃµi trong Recurrent Neural Networks(RNNs), Ä‘áº¡i diá»‡n cho cÃ¡c tráº¡ng thÃ¡i bÃªn trong cá»§a mÃ´ hÃ¬nh táº¡i má»™t thá»i Ä‘iá»ƒm cá»¥ thá»ƒ trong bá»™ chuá»—i Ä‘áº§u vÃ o. NÃ³ lÆ°u trá»¯ thÃ´ng tin ngá»¯ cáº£nh tá»« cÃ¡c bÆ°á»›c thá»i gian trÆ°á»›c Ä‘Ã³, giÃºp mÃ´ hÃ¬nh duy trÃ¬ má»™t dáº¡ng bá»™ nhá»› Ä‘á»ƒ xá»­ lÃ½ dá»¯ liá»‡u tuáº§n tá»± 
			
		###	Recurrent Connection 
			LÃ  má»™t thÃ nh pháº§n cá»‘t lÃµi trong Recurrent Neural Networks(RNNs), Ä‘áº¡i diá»‡n cho liÃªn káº¿t ngÆ°á»£c(feedback loop)trong máº¡ng. NÃ³ cho phÃ©p thÃ´ng tin tá»« cÃ¡c bÆ°á»›c thá»i gian trÆ°á»›c Ä‘Ã³ Ä‘Æ°á»£c truyá»n ngÆ°á»£c láº¡i vÃ o máº¡ng, giÃºp RNN duy trÃ¬ bá»™ nhá»› vÃ  xá»­ lÃ½ dá»¯ liá»‡u tuáº§n tá»± má»™t cÃ¡ch hiá»‡u quáº£.
	
	## Transformer Neural Networks 
		
		### Transformer Neural Network lÃ  má»™t kiáº¿n trÃºc máº¡ng nÆ¡-ron má»›i, máº¡nh máº½ , chuyÃªn dÃ¹ng trong NLP, giÃºp mÃ´ hÃ¬nh xá»­ lÃ½ dá»¯ liá»‡u song song vÃ  hiá»‡u quáº£ hÆ¡n, Ä‘áº·c biá»‡t lÃ  trong viá»‡c dá»‹ch mÃ¡y vÃ  táº¡o vÄƒn báº£n.
		
		### Self-Attention 
			CÆ¡ cháº¿ cho phÃ©p mÃ´ hÃ¬nh táº­p trung vÃ o cÃ¡c tá»« liÃªn quan trong cÃ¹ng má»™t cÃ¢u Ä‘á»ƒ hiá»ƒu ngá»¯ cáº£nh. Trong Ä‘Ã³ Scaled Dot-Product Attention lÃ  má»™t phÆ°Æ¡ng phÃ¡p hiá»‡u quáº£ Ä‘á»ƒ tÃ­nh attention giá»¯a cÃ¡c vertor tá»« vÃ  Multi-Head Attention lÃ  ká»¹ thuáº­t sá»­ dá»¥ng nhiá»u head attention song song Ä‘á»ƒ mÃ´ hÃ¬nh hÃ³a giá»¯a cÃ¡c ngá»¯ cáº£nh khÃ¡c nhau 
			  
		### Positional Encoding 
			LÃ  má»™t thÃ nh pháº§n quan trá»ng trong kiáº¿n trÃºc Transformer, Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ cung cáº¥p thÃ´ng tin vá» vá»‹ trÃ­ cá»§a tá»« trong má»™t chuá»—i dá»¯ liá»‡u Ä‘áº§u vÃ o. Äiá»u nÃ y cáº§n thiáº¿t vÃ¬ Transformer khÃ´ng cÃ³ cáº¥u trÃºc tuáº§n tá»± nhÆ° cÃ¡c mÃ´ hÃ¬nh RNN hoáº·c LSTM, nÃªn báº£n thÃ¢n nÃ³ khÃ´ng thá»ƒ tá»± Ä‘á»™ng nháº­n biáº¿t thá»© tá»± hoáº·c vá»‹ trÃ­ cá»§a cÃ¡c tá»« trong chuá»—i 
			
		### Masked Attention 
			QuÃ¡ trÃ¬nh sá»­ dá»¥ng má»™t ma tráº­n máº·t náº¡(mask matrix ) Ä‘á»ƒ loáº¡i bá» hoáº·c giáº£m trá»ng sá»‘ cá»§a cÃ¡c tá»« khÃ´ng Ä‘Æ°á»£c phÃ©p nhÃ¬n tháº¥y. Äiá»u nÃ y Ä‘áº£m báº£o ráº±ng khi dá»± Ä‘oÃ¡n tá»« t, chá»‰ cÃ¡c tá»« tá»« 1 Ä‘áº¿n t Ä‘Æ°á»£c phÃ©p tham gia tÃ­nh toÃ¡n, cÃ¡c tá»« sau t sáº½ bá»‹ gÃ¡n trá»ng sá»‘ Ã¢m vÃ´ cÃ¹ng trong khÃ´ng gian logit, dáº«n Ä‘áº¿n xÃ¡c suáº¥t xuáº¥t attention cá»§a chÃºng báº±ng 0  


# 	Ká»¹ thuáº­t há»c mÃ¡y(Machine Learning Techniques )
		
		## Supervised Learning lÃ  phÆ°Æ¡ng phÃ¡p há»c mÃ¡y trong Ä‘Ã³ mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn má»™t táº­p dá»¯ liá»‡u Ä‘Ã£ gáº¯n nhÃ£n, nháº±m dá»± Ä‘oÃ¡n cÃ¡c nhÃ£n cho dá»¯ liá»‡u má»›i 
		
		## Unsupervised Learning 
			lÃ  phÆ°Æ¡ng phÃ¡p há»c mÃ¡y nÆ¡i mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn dá»¯ liá»‡u chÆ°a gáº¯n nhÃ£n, vÃ  nhá»‡m vá»¥ cá»§a nÃ³ lÃ  tÃ¬m ra cÃ¡c cáº¥u trÃºc áº©n hoáº·c má»‘i quan há»‡ trong dá»¯ liá»‡u  
			
		## Semi-supervised Learning 
			Semi-Supervised Learning lÃ  sá»± káº¿t há»£p giá»¯a há»c cÃ³ giÃ¡m sÃ¡t, sá»­ dá»¥ng má»™t lÆ°á»£ng nhá» dá»¯ liá»‡u gáº¯n nhÃ£n káº¿t há»£p vá»›i dá»¯ liá»‡u chÆ°a gáº¯n nhÃ£n Ä‘á»ƒ cáº£i thiá»‡n Ä‘á»™ chÃ­nh xÃ¡c cá»§a mÃ´ hÃ¬nh 
			
		## Reinforcement Learning 
			LÃ  phÆ°Æ¡ng phÃ¡p há»c mÃ¡y nÆ¡i mÃ´ hÃ¬nh há»c thÃ´ng qua thá»­ vÃ  sai, nháº­n pháº§n thÆ°á»Ÿng hoáº·c pháº¡t tá»« mÃ´i trÆ°á»ng Ä‘á»ƒ cáº£i thiá»‡n hiá»‡u suáº¥t theo thá»i gian 
			
		## Transfer Learning 
			lÃ  ká»¹ thuáº­t trong Ä‘Ã³ má»™t mÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn má»™t nhiá»‡m vá»¥ cÃ³ thá»ƒ Ä‘Æ°á»£c tinh chá»‰nh vÃ  sá»­ dá»¥ng láº¡i cho má»™t nhiá»‡m vá»¥ khÃ¡c, giÃºp tiáº¿t kiá»‡m thá»i gian vÃ  tÃ i nguyÃªn 
			
		## Deep Learning 
			LÃ  phÆ°Æ¡ng phÃ¡p há»c mÃ¡y sá»­ dá»¥ng máº¡ng nÆ¡-ron sÃ¢u Ä‘á»ƒ há»c tá»« dá»¯ liá»‡u lá»›n vÃ  phá»©c táº¡p, Ä‘áº·c biá»‡t hiá»‡u quáº£ trong xá»­ lÃ½ hÃ¬nh áº£nh, Ã¢m thanh vÃ  ngÃ´n ngá»¯  
			
		## Continous Learning 
			LÃ  kháº£ nÄƒng cá»§a má»™t mÃ´ hÃ¬nh AI Ä‘á»ƒ há»c há»i liÃªn tá»¥c tá»« dá»¯ liá»‡u má»›i, thÃ­ch nghi vá»›i cÃ¡c thay Ä‘á»•i vÃ  cáº£i thiá»‡n hiá»‡u suáº¥t mÃ  khÃ´ng cáº§n huáº¥n luyá»‡n láº¡i tá»« Ä‘áº§u 

# CÃ¡c thuáº­t toÃ¡n vÃ  phÆ°Æ¡ng phÃ¡p(Algorithms and Methods)
	
		##Regression 
			Regression(Há»“i quy) lÃ  má»™t thuáº­t toÃ¡n há»c mÃ¡y(machine learning) Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ dá»± Ä‘oÃ¡n giÃ¡ trá»‹ liÃªn tá»¥c dá»±a trÃªn cÃ¡c Ä‘áº·c trÆ°ng Ä‘áº§u vÃ o. Trong há»“i quy, má»‘i quan há»‡ giá»¯a biáº¿n Ä‘áº§u vÃ o(features) vÃ  biáº¿n má»¥c tiÃªu(target) Ä‘Æ°á»£c mÃ´ hÃ¬nh hÃ³a, vÃ  má»¥c tiÃªu lÃ  dá»± Ä‘oÃ¡n má»™t sá»‘ giÃ¡ trá»‹ thá»±c tá»« má»™t hoáº·c nhiá»u Ä‘áº·c trÆ°ng  
		
		## Classification 
			Classification(PhÃ¢n loáº¡i) lÃ  má»™t thuáº­t toÃ¡n há»c mÃ¡y Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ phÃ¢n nhÃ³m hoáº·c phÃ¢n loáº¡i cÃ¡c Ä‘á»‘i tÆ°á»£ng vÃ o cÃ¡c nhÃ³m hoáº·c háº¡ng má»¥c cá»¥ thá»ƒ, dá»±a trÃªn cÃ¡c Ä‘áº·c trÆ°ng Ä‘áº§u vÃ o. Trong phÃ¢n loáº¡i, má»¥c tiÃªu lÃ  gÃ¡n má»—i vÃ­ dá»¥ dá»¯ liá»‡u(Ä‘Æ°á»£c mÃ´ táº£ bá»Ÿi cÃ¡c Ä‘áº·c trÆ°ng) vÃ o má»™t trong cÃ¡c nhÃ£n hoáº·c lá»›p Ä‘Ã£ Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a tá»« trÆ°á»›c 
			
		## Clustering 
			Clustering (PhÃ¢n nhÃ³m) lÃ  má»™t thuáº­t toÃ¡n há»c mÃ¡y khÃ´ng giÃ¡m sÃ¡t(unsupervised learning)Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ phÃ¢n loáº¡i cÃ¡c Ä‘á»‘i tÆ°á»£ng hoáº·c dá»¯ liá»‡u vÃ o cÃ¡c nhÃ³m(clusters) sao cho cÃ¡c Ä‘á»‘i tÆ°á»£ng trong cÃ¹ng má»™t nhÃ³m cÃ³ Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng cao, trong khi cÃ¡c Ä‘á»‘i tÆ°á»£ng giá»¯a cÃ¡c nhÃ³m khÃ¡c nhau cÃ³ Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng tháº¥p .
			
		## Dimensionality Reduction 
			Dimensionality Reduction(Giáº£m chiá»u dá»¯ liá»‡u) lÃ  má»™t ká»¹ thuáº­t trong há»c mÃ¡y vÃ  khai thÃ¡c dá»¯ liá»‡u dÃ¹ng Ä‘á»ƒ giáº£m sá»‘ lÆ°á»£ng biáº¿n(feature) trong má»™t táº­p dá»¯ liá»‡u mÃ  khÃ´ng lÃ m máº¥t quÃ¡ nhiá»u thÃ´ng tin quan trá»ng. Má»¥c tiÃªu cá»§a ká»¹ thuáº­t nÃ y lÃ  giÃºp giáº£m Ä‘á»™ phá»©c táº¡p cá»§a dá»¯ liá»‡u, lÃ m cho quÃ¡ trÃ¬nh xá»­ lÃ½ dá»¯ liá»‡u vÃ  mÃ´ hÃ¬nh hÃ³a trá»Ÿ nÃªn nhanh chÃ³ng vÃ  hiá»‡u quáº£ hÆ¡n, Ä‘á»“ng thá»i giÃºp giáº£m hiá»‡n tÆ°á»£ng overfiting(mÃ´ hÃ¬nh quÃ¡ khá»›p)
			
		## Decision Trees 
			Decision Trees(CÃ¢y quyáº¿t Ä‘á»‹nh ) lÃ  má»™t thuáº­t toÃ¡n há»c mÃ¡y Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ phÃ¢n loáº¡i(classification) vÃ  há»“i quy(regression). ÄÃ¢y lÃ  má»™t mÃ´ hÃ¬nh há»c mÃ¡y giÃ¡m sÃ¡t (supervised learning) cÃ³ thá»ƒ Ä‘Æ°á»£c mÃ´ phá»ng nhÆ° má»™t cÃ¢y, trong Ä‘Ã³ má»—i nÃºt(node) Ä‘áº¡i diá»‡n cho má»™t cÃ¢u há»i vá» má»™t Ä‘áº·c trÆ°ng(feature) cá»§a dá»¯ liá»‡u vÃ  má»—i nhÃ¡nh (branch) Ä‘áº¡i diá»‡n cho má»™t káº¿t quáº£ cá»§a cÃ¢u há»i Ä‘Ã³. Cuá»‘i cÃ¹ng cÃ¡c lÃ¡(leaf nodes) cá»§a cÃ¢y chá»©a cÃ¡c quyáº¿t Ä‘á»‹nh hoáº·c dá»± Ä‘oÃ¡n cho káº¿t quáº£ 
			
		## Random Forest 	
			Random Forrest(Rá»«ng ngáº«u nhiÃªn) lÃ  má»™t thuáº­t toÃ¡n há»c mÃ¡y ensemble(há»£p nháº¥t) Ä‘Æ°á»£c sá»­ dá»¥ng trong cÃ¡c bÃ i toÃ¡n phÃ¢n loáº¡i(classification) vÃ  há»“i quy(regression). Random Forest xÃ¢y dá»±ng nhiá»u cÃ¢y quyáº¿t Ä‘á»‹nh(decision trees) trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n vÃ  sá»­ dá»¥ng káº¿t quáº£ tá»« táº¥t cáº£ cÃ¡c cÃ¢y nÃ y Ä‘á»ƒ Ä‘Æ°a ra dá»± Ä‘oÃ¡n, giÃºp cáº£i thiá»‡n Ä‘á»™ chÃ­nh xÃ¡c vÃ  giáº£m hiá»‡n tÆ°á»£ng overfitting(quÃ¡ khá»›p) mÃ  cÃ¡c cÃ¢y quyáº¿t Ä‘á»‹nh Ä‘Æ¡n láº» thÆ°á»ng gáº·p pháº£i 
			
		## Support Vector Machines -  SVM 
			Support Vector Machines(SVM) lÃ  má»™t thuáº­t toÃ¡n há»c mÃ¡y máº¡nh máº½, chá»§ yáº¿u Ä‘Æ°á»£c sá»­ dá»¥ng cho cÃ¡c bÃ i toÃ¡n phÃ¢n loáº¡i(classification ) vÃ  há»“i quy(regression). SVM Ä‘áº·c biá»‡t ná»•i báº­t trong viá»‡c tÃ¬m ra má»™t siÃªu pháº³ng(hyperplane) tá»‘i Æ°u Ä‘á»ƒ phÃ¢n chia cÃ¡c lá»›p trong khÃ´ng gian Ä‘áº·c trÆ°ng(feature space) sao chi khoáº£ng cÃ¡ch giá»¯a cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u gáº§n nháº¥t cá»§a cÃ¡c lá»›p khÃ¡c nhau lÃ  lá»›n nháº¥t 



# 5. Xá»­ lÃ½ vÃ  phÃ¢n tÃ­ch dá»¯ liá»‡u(Data Processing and Analysis )

		## Big Data 
			Big Data(Dá»¯ liá»‡u lá»›n) LÃ  thuáº­t ngá»¯ dá»¥ng Ä‘á»ƒ chá»‰ cÃ¡c táº­p dá»¯ liá»‡u cÃ³ kÃ­ch thÆ°á»›c vÃ  Ä‘á»™ phá»©c táº¡p vÆ°á»£t quÃ¡ kháº£ nÄƒng xá»­ lÃ½ cá»§a cÃ¡c cÃ´ng cá»¥ vÃ  pháº§n má»m truyá»n thá»‘ng. NhÅ©ng dá»¯ liá»‡u nÃ y thÆ°á»ng Ä‘áº¿n tá»« nhiá»u nguá»“n khÃ¡c nhau vÃ  cÃ³ thá»ƒ cÃ³ má»™t lÆ°á»£ng khá»•ng lá»“, Ä‘a dáº¡ng, vÃ  phÃ¡t triá»ƒn nhanh chÃ³ng. BigData khÃ´ng chá»‰ bao gá»“m dá»¯ liá»‡u cÃ³ kÃ­ch thÆ°á»›c lá»›n, mÃ  cÃ²n bao gá»“m cÃ¡c Ä‘áº·c Ä‘iá»ƒm nhÆ° tÃ­nh Ä‘a dáº¡ng vÃ  tá»‘c Ä‘á»™ thay Ä‘á»•i nhanh cá»§a dá»¯ liá»‡u. 
			
		## Data Preprocessing 
			Data Preprocessing (Tiá»n xá»­ lÃ½ dá»¯ liá»‡u) lÃ  quÃ¡ trÃ¬nh trÃ¬nh chuáº©n bá»‹ vÃ  lÃ m sáº¡ch sá»­ liá»‡u thÃ´ Ä‘á»ƒ cÃ³ thá»ƒ sá»­ dá»¥ng hiá»‡u quáº£ trong cÃ¡c mÃ´ hÃ¬nh há»c mÃ¡y(machine learning)  hoáº·c phÃ¢n tÃ­ch dá»¯ liá»‡u. Dá»¯ liá»‡u thÃ´ cÃ³ thá»ƒ chá»©a nhiá»u váº¥n Ä‘á» nhÆ° thiáº¿u sÃ³t, nhiá»…u, hoáº·c khÃ´ng Ä‘á»“ng nháº¥t, vÃ¬ váº­y quÃ¡ trÃ¬nh tiá»n xá»­ lÃ½ giÃºp cáº£i thiá»‡n cháº¥t lÆ°á»£ng cá»§a dá»¯ liá»‡u, tá»« Ä‘Ã³ giÃºp nÃ¢ng cao Ä‘á»™ chÃ­nh xÃ¡c vÃ  hiá»‡u quáº£ cá»§a mÃ´ hÃ¬nh. 

		## Data Augmentation 
			Data Augmentation (TÄƒng cÆ°á»ng dá»¯ liá»‡u) lÃ  má»™t ká»¹ thuáº­t trong há»c mÃ¡y vÃ  há»c sÃ¢u(deep learning) dÃ¹ng Ä‘á»ƒ má»Ÿ rá»™ng vÃ  cáº£i thiá»‡n bá»™ dá»¯ liá»‡u huáº¥n luyá»‡n báº±ng cÃ¡ch táº¡o ra cÃ¡c biáº¿n thá»ƒ má»›i tá»« cÃ¡c dá»¯ liá»‡u gá»‘c mÃ  khÃ´ng cáº§n pháº£i thu tháº­p thÃªm dá»¯ liá»‡u má»›i. QuÃ¡ trÃ¬nh nÃ y giÃºp mÃ´ hÃ¬nh há»c Ä‘Æ°á»£c nhiá»u Ä‘áº·c Ä‘iá»ƒm hÆ¡n, tá»« Ä‘Ã³ tÄƒng kháº£ nÄƒng tá»•ng quÃ¡t vÃ  giáº£m hiá»‡n tÆ°á»£ng overfitting(mÃ´ hÃ¬nh quÃ¡ khá»›p vá»›i dá»¯ liá»‡u huáº¥n luyá»‡n).
			
		## Data Mining 
			Data Mining lÃ  quÃ¡ trÃ¬nh khai thÃ¡c thÃ´ng tin há»¯u Ã­ch tá»« má»™t lÆ°á»£ng lá»›n dá»¯ liá»‡u báº±ng cÃ¡ch sá»­ dá»¥ng cÃ¡c ká»¹ thuáº­t thá»‘ng kÃª, há»c mÃ¡y, vÃ  phÃ¢n tÃ­ch dá»¯ liá»‡u. Má»¥c tiÃªu cá»§a data mining lÃ  tÃ¬m ra cÃ¡c máº«u, má»—i quan há»‡, hoáº·c thÃ´ng tin tiá»m áº©n trong dá»¯ liá»‡u, giÃºp Ä‘Æ°a ra cÃ¡c quyáº¿t Ä‘á»‹nh kinh doanh, dá»± Ä‘oÃ¡n, hoáº·c phÃ¡t hiá»‡n cÃ¡c xu hÆ°á»›ng.
			
		## Predictive Analytics
			Predictive Analytics (PhÃ¢n tÃ­ch dá»± Ä‘oÃ¡n ) lÃ  má»™t nhÃ¡nh cá»§a phÃ¢n tÃ­ch dá»¯ liá»‡u dÃ¹ng cÃ¡c phÆ°Æ¡ng phÃ¡p thá»‘ng kÃª, há»c mÃ¡y, vÃ  cÃ¡c thuáº­t toÃ¡n Ä‘á»ƒ phÃ¢n tÃ­ch dá»¯ liá»‡u hiá»‡n cÃ³ vÃ  dá»± Ä‘oÃ¡n cÃ¡c xu hÆ°á»›ng, sá»± kiá»‡n hoáº·c hÃ nh vi trong tÆ°Æ¡ng lai. Má»¥c tiÃªu cá»§a predictive analytics lÃ  sá»­ dá»¥ng dá»¯ liá»‡u quÃ¡ khá»© Ä‘á»ƒ Ä‘Æ°a ra cÃ¡c dá»± Ä‘oÃ¡n vá» nhá»¯ng gÃ¬ cÃ³ thá»ƒ xáº£y ra trong tÆ°Æ¡ng lai, giÃºp doanh nghiá»‡p vÃ  tá»• chá»©c Ä‘Æ°a ra quyáº¿t Ä‘á»‹nh hiá»‡u quáº£ hÆ¡n.
			
		## Data Visualization 
			Data Visualization(Trá»±c quan hÃ³a dá»¯ liá»‡u)  lÃ  quÃ¡ trÃ¬nh sá»­ dá»¥ng cÃ¡c Ä‘á»“ há»a, biá»ƒu Ä‘á»“ vÃ  cÃ¡c hÃ¬nh thá»©c trá»±c quan khÃ¡c Ä‘á»ƒ biá»ƒu diá»…n dá»¯ liá»‡u. Má»¥c tiÃªu cá»§a data visualization lÃ  giÃºp ngÆ°á»i dÃ¹ng hiá»ƒu vÃ  phÃ¢n tÃ­ch dá»¯ liá»‡u má»™t cÃ¡ch dá»… dÃ ng vÃ  trá»±c quan, tá»« Ä‘Ã³ há»— trá»£ viá»‡c ra quyáº¿t Ä‘á»‹nh chÃ­nh xÃ¡c hÆ¡n.
			
		## Structured and Unstructured Data	
			Structured and Unstructured Data lÃ  dá»¯ liá»‡u Ä‘Æ°á»£c tá»• chá»©c dÆ°á»›i dáº¡ng báº£ng vá»›i cÃ¡c hÃ ng vÃ  cá»™t, dá»… dÃ ng xá»­ lÃ½ báº±ng cÃ¡c há»‡ thá»‘ng cÆ¡ sá»Ÿ dá»¯ liá»‡u truyá»n thá»‘ng. Dá»¯ liá»‡u phi cáº¥u trÃºc, ngÆ°á»£c láº¡i , khÃ´ng theo má»™t cáº¥u trÃºc cá»‘ Ä‘á»‹nh, bao gá»“m vÄƒn báº£n, hÃ¬nh áº£nh, video vÃ  yÃªu cáº§u cÃ¡c ká»¹ thuáº­t Ä‘áº·c biá»‡t Ä‘á»ƒ xá»­ lÃ½.
			
# 6. ÄÃ¡nh giÃ¡ vÃ  tá»‘i Æ°u hÃ³a mÃ´ hÃ¬nh(Model Evaluation and Optimization)
		
		## Accuracy
			Accuracy(Äá»™ chÃ­nh xÃ¡c) lÃ  má»™t chá»‰ sá»‘ dá»¥ng Ä‘á»ƒ Ä‘o lÆ°á»ng hiá»‡u quáº£ cá»§a má»™t mÃ´ hÃ¬nh há»c mÃ¡y, Ä‘áº·c biá»‡t lÃ  trong cÃ¡c bÃ i toÃ¡n phÃ¢n loáº¡i. Äá»™ chÃ­nh xÃ¡c Ä‘Æ°á»£c tÃ­nh báº±ng tá»· lá»‡ giá»¯a sá»‘ lÆ°á»£ng dá»± Ä‘oÃ¡n Ä‘Ãºng so vá»›i tá»•ng sá»‘ dá»± Ä‘oÃ¡n mÃ  mÃ´ hÃ¬nh thá»±c hiá»‡n .
		
		## Loss Function 
			LossFunction (HÃ m máº¥t mÃ¡t ) lÃ  má»™t hÃ m toÃ¡n há»c dÃ¹ng Ä‘á»ƒ Ä‘o lÆ°á»ng má»©c Ä‘á»™ sai lá»‡ch hoáº·c Ä‘á»™ chÃ­nh xÃ¡c cá»§a dá»± Ã¡n so vá»›i giÃ¡ trá»‹ thá»±c táº¿ trong má»™t mÃ´ hÃ¬nh mÃ¡y. NÃ³ giÃºp mÃ´ hÃ¬nh "hiá»ƒu" Ä‘Æ°á»£c sá»± khÃ¡c biá»‡t giá»¯a cÃ¡c giÃ¡ trá»‹ dá»± Ä‘oÃ¡n vÃ  giÃ¡ trá»‹ tháº­t, tá»« Ä‘Ã³ cÃ³ thá»ƒ Ä‘iá»u chá»‰nh cÃ¡c tham sá»‘ cá»§a mÃ´ hÃ¬nh Ä‘á»ƒ cáº£i thiá»‡n hiá»‡u quáº£ dá»± Ä‘oÃ¡n trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n. Má»¥c tiá»ƒu cá»§a viá»‡c sá»­ dá»¥ng loss function lÃ  tá»‘i thiá»ƒu hÃ³a giÃ¡ trá»‹ hÃ m máº¥t mÃ¡t trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n, giÃºp mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n chÃ­nh xÃ¡c hÆ¡n.
			
		## Overfitting
			Overfitting (QuÃ¡ khá»›p) lÃ  hiá»‡n tÆ°á»£ng khi má»™t mÃ´ hÃ¬nh há»c mÃ¡y quÃ¡ chi tiáº¿t cÃ¡c Ä‘áº·c Ä‘iá»ƒm cá»§a dá»¯ liá»‡u huáº¥n luyá»‡n, Ä‘áº¿n má»©c mÃ´ hÃ¬nh há»c nhá»¯ng nhiá»…u(noise) vÃ  sá»± biáº¿n Ä‘á»™ng khÃ´ng cÃ³ Ã½ nghÄ©a, thay vÃ¬ chá»‰ há»c Ä‘Æ°á»£c cÃ¡c máº«u vÃ  xu hÆ°á»›ng chung trong dá»¯ liá»‡u. Khi xáº£y ra overfitting, mÃ´ hÃ¬nh cÃ³ thá»ƒ Ä‘áº¡t Ä‘Æ°á»£c Ä‘á»™ chÃ­nh xÃ¡c ráº¥t cao trÃªn táº­p huáº¥n luyá»‡n, nhÆ°ng láº¡i hoáº¡t Ä‘á»™ng kÃ©m trÃªn cÃ¡c dá»¯ liá»‡u chÆ°a tháº¥y(dá»¯ liá»‡u kiá»ƒm tra hoáº·c dá»¯ liá»‡u thá»±c táº¿), vÃ¬ nÃ³ khÃ´ng thá»ƒ tá»•ng quÃ¡t tá»‘t.
			
		## Underfitting 
			Underfitting(Thiáº¿u khá»›p) lÃ  hiá»‡n tÆ°á»£ng khi má»™t mÃ´ hÃ¬nh há»c mÃ¡y khÃ´ng Ä‘á»§ kháº£ nÄƒng há»c Ä‘Æ°á»£c cÃ¡c máº«u vÃ  xu hÆ°á»›ng trong dá»¯ liá»‡u huáº¥n luyá»‡n, dáº«n Ä‘áº¿n viá»‡c mÃ´ hÃ¬nh khÃ´ng thá»ƒ dá»± Ä‘oÃ¡n chÃ­nh xÃ¡c ngay cáº£ trÃªn dá»¯ liá»‡u huáº¥n luyá»‡n, chá»© Ä‘á»«ng nÃ³i Ä‘áº¿n dá»¯ liá»‡u má»›i. Khi má»™t mÃ´ hÃ¬nh bá»‹ underfiting, NÃ³ khÃ´ng Ä‘á»§ phá»©c táº¡p Ä‘á»ƒ mÃ´ táº£ cÃ¡c má»‘i quan há»‡ trong dá»¯ liá»‡u, khiá»ƒn hiá»‡u suáº¥t cá»§a mÃ´ hÃ¬nh tháº¥p á»Ÿ cáº£ táº­p huáº¥n luyá»‡n vÃ  táº­p kiá»ƒm tra.
			
		## Cross-validation 
			Cross-valication(Kiá»ƒm tra chÃ©o) lÃ  má»™t ká»¹ thuáº­t Ä‘Æ°á»£c sá»­ dá»¥ng trong há»c mÃ¡y Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ kháº£ nÄƒng tá»•ng quÃ¡t cá»§a má»™t mÃ´ hÃ¬nh trÃªn má»™t táº­p dá»¯ liá»‡u chÆ°a Ä‘Æ°á»£c sá»­ dá»¥ng trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n. NÃ³ giÃºp kiá»ƒm tra Ä‘á»™ chÃ­nh xÃ¡c vÃ  tÃ­nh á»•n Ä‘á»‹nh cá»§a mÃ´ hÃ¬nh khi Ã¡p dá»¥ng vÃ o cÃ¡c dá»¯ liá»‡u khÃ¡c nhau, trÃ¡nh tÃ¬nh tráº¡ng mÃ´ hÃ¬nh bá»‹ overfitting hoáº·c underfitting.
			
		## Hyperparameter Optimization 
			Hyperparameter Optimization(Tá»‘i Æ°u hÃ³a siÃªu tham sá»‘)  lÃ  quÃ¡ trÃ¬nh tÃ¬m kiáº¿m cÃ¡c giÃ¡ trá»‹ tá»‘i Æ°u cho cÃ¡c siÃªu tham sá»‘ trong mÃ´ hÃ¬nh há»c mÃ¡y Ä‘á»ƒ cáº£i thiá»‡n hiá»‡u suáº¥t cá»§a mÃ´ hÃ¬nh. CÃ¡c siÃªu tham sá»‘ lÃ  cÃ¡c tham sá»‘ Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh trÆ°á»›c khi huáº¥n luyá»‡n mÃ´ hÃ¬nh vÃ  khÃ´ng Ä‘Æ°á»£c Ä‘iá»u chá»‰nh trong quÃ¡ trÃ¬nh há»c, khÃ¡c vá»›i cÃ¡c tham sá»‘ cá»§a mÃ´ hÃ¬nh(nhÆ° trá»ng sá»‘ trong máº¡ng nÆ¡-ron).
			
		## Precision, Recall, and F1 Score 
			Precision Ä‘o lÆ°á»ng tá»· lá»‡ dá»± Ä‘oÃ¡n Ä‘Ãºng trong sá»‘ cÃ¡c dá»± Ä‘oÃ¡n mÃ  mÃ´ hÃ¬nh Ä‘Ã£ xÃ¡c Ä‘á»‹nh lÃ  dÆ°Æ¡ng tÃ­nh.
			Recall Ä‘o lÆ°á»ng kháº£ nÄƒng cá»§a mÃ´ hÃ¬nh trong viá»‡c tÃ¬m ra táº¥t cáº£ cÃ¡c trÆ°á»ng há»£p dÆ°Æ¡ng tÃ­nh tháº­t sá»±. 
			F1 Score lÃ  trung bÃ¬nh Ä‘iá»u hÃ²a giá»¯a Ä‘á»™ chÃ­nh xÃ¡c vÃ  Ä‘á»™ thu há»“i, cung cáº¥p má»™t thÆ°á»›c Ä‘o cÃ¢n báº±ng giá»¯a hai chá»‰ sá»‘ nÃ y.

//======================== Sá»± khÃ¡c biá»‡t giá»¯a AI,ML,DL ===========================// 

# 1 Giá»›i thiá»‡u  
	Trong ká»· nguyÃªn cÃ´ng nghá»‡ sá»‘ Ä‘ang bÃ¹ng ná»•, ba thuáº­t ngá»¯ ná»•i báº­t- TrÃ­ tuá»‡ nhÃ¢n táº¡o(AI), mÃ¡y há»c (Machine learning - ML ) ,  Há»c sÃ¢u (Deep Learning - DL) Ä‘Ã£ trá»Ÿ thÃ nh trung tÃ¢m cá»§a nhiá»u cuá»™c tháº£o luáº­n vÃ  nghiÃªn cá»©u 
	
# 2 TrÃ­ tuá»‡ nhÃ¢n táº¡o (AI )	
	
	## Äá»‹nh nghÄ©a : 
		TrÃ­ tuá»‡ nhÃ¢n táº¡o lÃ  má»™t nhanh rá»™ng lá»›n cá»§a khoa há»c mÃ¡y tÃ­nh, táº­p trung vÃ o viá»‡c táº¡o ra cÃ¡c há»‡ thá»‘ng thÃ´ng tin cÃ³ kháº£ nÄƒng thá»±c hiá»‡n cÃ¡c nhiá»‡m vá»¥ thÆ°á»ng Ä‘Ã²i há»i trÃ­ thÃ´ng minh cá»§a con ngÆ°á»i 
	
	## Äáº·c Ä‘iá»ƒm chÃ­nh 
		TrÃ­ tuá»‡ nhÃ¢n táº¡o (AI) lÃ  má»™t lÄ©nh vá»±c trong khoa há»c mÃ¡y tÃ­nh, nháº±m táº¡o ra cÃ¡c há»‡ thá»‘ng cÃ³ kháº£ nÄƒng thá»±c hiá»‡n cÃ¡c nhiá»‡m vá»¥ mÃ  bÃ¬nh thÆ°á»ng cáº§n cÃ³ trÃ­ tuá»‡ con ngÆ°á»i. CÃ¡c nhiá»‡m vá»¥ nÃ y bao gá»“m viá»‡c hiá»ƒu ngÃ´n ngá»¯ tá»± nhiÃªn, nháº­n dáº¡ng giá»ng nÃ³i, ra quyáº¿t Ä‘á»‹nh, vÃ  tháº­m chÃ­ lÃ  sÃ¡ng táº¡o. AI cÃ³ thá»ƒ Ä‘Æ°á»£c chia thÃ nh hai loáº¡i chÃ­nh : 
			
		### AI Yáº¿u (Narrow AI)
			- AI yáº¿u, hay cÃ²n gá»i lÃ  AI háº¹p, lÃ  loáº¡i trÃ­ tuá»‡ nhÃ¢n táº¡o Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ thá»±c hiá»‡n má»™t nhiá»‡m vá»¥ cá»¥ thá»ƒ hoáº·c má»™t loáº¡t cÃ¡c nhiá»‡m vá»¥ cá»¥ thá»ƒ. NÃ³ khÃ´ng cÃ³ kháº£ nÄƒng hoáº¡t Ä‘á»™ng ngoÃ i pháº¡m vi nhiá»‡m vá»¥ Ä‘Æ°á»£c láº­p trÃ¬nh sáºµn. VÃ­ dá»¥ vá» AI yáº¿u bao gá»“m cÃ¡c há»‡ thá»‘ng nháº­n diá»‡n khuÃ´n máº·t, trá»£ lÃ½ áº£o Siri, Alexa, vÃ  cÃ¡c cÃ´ng cá»¥ gá»£i Ã½ trÃªn cÃ¡c ná»n táº£ng trá»±c tuyáº¿n. Máº·c dÃ¹ ráº¥t máº¡nh máº½ trong viá»‡c thá»±c hiá»‡n cÃ¡c nhiá»‡m vá»¥ Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh rÃµ rÃ ng, AI yáº¿u khÃ´ng thá»ƒ suy nghÄ©, lÃ½ luáº­n hoáº·c Ä‘Æ°a ra quyáº¿t Ä‘á»‹nh ngoÃ i pháº¡m vi nhá»¯ng gÃ¬ nÃ³ Ä‘Ã£ Ä‘Æ°á»£c láº­p trÃ¬nh Ä‘á»ƒ lÃ m. 
			
		### AI máº¡nh(General AI )
			- General AI hay cÃ²n gá»i lÃ  AI tá»•ng quÃ¡t, lÃ  loáº¡i trÃ­ tuá»‡ nhÃ¢n táº¡o cÃ³ kháº£ nÄƒng hiá»ƒu, há»c há»i, vÃ  thá»±c hiá»‡n báº¥t ká»³ nhiá»‡m vá»¥ trÃ­ tuá»‡ nÃ o mÃ  con ngÆ°á»i cÃ³ thá»ƒ thá»±c hiá»‡n. NÃ³ cÃ³ kháº£ nÄƒng suy nghÄ©, láº­p luáº­n, vÃ  thÃ­ch á»©ng vá»›i nhá»¯ng tÃ¬nh huá»‘ng má»›i mÃ  khÃ´ng cáº§n sá»± can thiá»‡p cá»§a con ngÆ°á»i hoáº·c láº­p trÃ¬nh trÆ°á»›c. AI máº¡nh khÃ´ng chá»‰ cÃ³ kháº£ nÄƒng thá»±c hiá»‡n nhiá»u nhiá»‡m vá»¥ khÃ¡c nhau mÃ  cÃ²n cÃ³ thá»ƒ chuyá»ƒn giao kiáº¿n thá»©c tá»« má»™t lÄ©nh vá»±c nÃ y sang lÄ©nh vá»±c khÃ¡c, tÆ°Æ¡ng tá»± nhÆ° con ngÆ°á»i. Tuy nhiÃªn, AI máº¡nh váº«n Ä‘ang trong giai Ä‘oáº¡n nghiÃªn cá»©u vÃ  phÃ¡t triá»ƒn, vÃ  chÆ°a cÃ³ há»‡ thá»‘ng nÃ o Ä‘áº¡t Ä‘Æ°á»£c má»©c Ä‘á»™ thÃ´ng minh nhÆ° váº­y 
			
			TrÃ­ tuá»‡ nhÃ¢n táº¡o lÃ  ná»n trng cho cÃ¡c cÃ´ng nghá»‡ tiÃªn tiÃªn nhÆ° Machine Learning , Deep Learning , giÃºp cáº£i thiá»‡n hiá»‡u suáº¥t vÃ  kháº£ nÄƒng tá»± Ä‘á»™ng hÃ³a cá»§a cÃ¡c há»‡ thá»‘ng. 
			
	## á»¨ng dá»¥ng
		### Trá»£ lÃ½ áº£o vÃ  chatbot 
			- AI Ä‘Æ°á»£c sá»­ dá»¥ng trong cÃ¡c trá»£ lÃ½ áº£o nhÆ° Siri, Alexa, Google Assistent Ä‘á»ƒ xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn, tráº£ lá»i cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng, vÃ  thá»±c hiá»‡n cÃ¡c tÃ¡c vá»¥ Ä‘Æ¡n giáº£n nhÆ° Ä‘áº·t báº£o thá»©c, gá»­i tin nháº¯n, hoáº·c Ä‘iá»u khiá»ƒn cÃ¡c thiáº¿t bá»‹ trong nhÃ  thÃ´ng minh. 
		### Há»‡ thá»‘ng gá»£i Ã½ : 
			- CÃ¡c há»‡ thá»‘ng gá»£i Ã½ dá»±a trÃªn AI cÃ³ thá»ƒ phÃ¢n tÃ­ch dá»¯ liá»‡u ngÆ°á»i dÃ¹ng Ä‘á»ƒ Ä‘Æ°a ra cÃ¡c Ä‘á» xuáº¥t cÃ¡ nhÃ¢n hÃ³a, cháº³ng háº¡n nhÆ° gá»£i Ã½ phim trÃªn Netflix hoáº·c sáº£n pháº©m trÃªn Amazon. Nhá»¯ng há»‡ thá»‘ng nÃ y thÆ°á»ng dá»±a trÃªn cÃ¡c quy táº¯c vÃ  logic Ä‘Æ¡n giáº£n thay vÃ¬ cÃ¡c thuáº­t toÃ¡n há»c mÃ¡y phá»©c táº¡p.
			
		### Tá»± Ä‘á»™ng hÃ³a quy trÃ¬nh: 
			- AI Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ tá»± Ä‘á»™ng hÃ³a cÃ¡c quy trÃ¬nh láº·p Ä‘i láº·p láº¡i trong cÃ¡c doanh nghiá»‡p. Cháº³ng háº¡n, AI cÃ³ thá»ƒ xá»­ lÃ½ cÃ¡c giao dá»‹ch tÃ i chÃ­nh, kiá»ƒm tra vÃ  xá»­ lÃ½ cÃ¡c tÃ i liá»‡u, hoáº·c quáº£n lÃ½ lá»‹ch trÃ¬nh tá»± Ä‘á»™ng mÃ  khÃ´ng cáº§n sá»­ dá»¥ng cÃ¡c ká»¹ thuáº­t há»c mÃ¡y phá»©c táº¡p 
		### Há»‡ thá»‘ng Ä‘iá»u khiá»ƒn thÃ´ng minh 
			Trong cÃ¡c há»‡ thá»‘ng Ä‘iá»u khiá»ƒn nhÆ° nhÃ  thÃ´ng minh hoáº·c quáº£n lÃ½ nÄƒng lÆ°á»£ng, AI giÃºp tá»‘i Æ°u hÃ³a viá»‡c sá»­ dá»¥ng tÃ i nguyÃªn báº±ng cÃ¡ch Ä‘iá»u chá»‰nh nhiá»‡t Ä‘á»™, Ã¡nh sÃ¡ng vÃ  cÃ¡c thiáº¿t bá»‹ Ä‘iá»‡n tá»­ dá»±a trÃªn cÃ¡c quy táº¯c vÃ  lá»‹ch trÃ¬nh Ä‘á»‹nh trÆ°á»›c. 
		
		### PhÃ¡t hiá»‡n vÃ  ngÄƒn cháº·n gian láº­n
			AI cÃ³ thá»ƒ Ä‘Æ°á»£c láº­p trÃ¬nh Ä‘á»ƒ phÃ¡t hiá»‡n cÃ¡c máº«u báº¥t thÆ°á»ng trong cÃ¡c giao dá»‹ch tÃ i chÃ­nh hoáº·c hoáº¡t Ä‘á»™ng trá»±c tuyáº¿n, tá»« Ä‘Ã³ ngÄƒn cháº·n cÃ¡c hÃ nh vi gian láº­n. Há»‡ thá»‘ng nÃ y sá»­ dá»¥ng cÃ¡c quy táº¯c vÃ  logic vÃ  phÃ¢n tÃ­ch cÃ¡c sá»± kiá»‡n trong thá»i gian thá»±c. 

		### Xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn cÆ¡ báº£n 
			- AI cÃ³ thá»ƒ thá»±c hiá»‡n cÃ¡c nhiá»‡m vá»¥ xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn Ä‘Æ¡n giáº£n, nhÆ° phÃ¢n tÃ­ch cÃ¡c vÄƒn báº£n, dá»‹ch ngÃ´n ngá»¯, hoáº·c trÃ­nh xuáº¥t thÃ´ng tin tá»« vÄƒn báº£n mÃ  khÃ´ng cáº§n sá»­ dá»¥ng Ä‘áº¿n cÃ¡c thuáº­t toÃ¡n há»c sÃ¢u phá»©c táº¡p.
	
	
	
	
# 3 Machine Learning (ML)
	
	## Äá»‹nh nghÄ©a : 
		MachineLearning lÃ  má»™t phÃ¢n nhÃ¡nh cá»§a AI, táº­p trung vÃ o viá»‡c phÃ¡t triá»ƒn cÃ¡c thuáº­t toÃ¡n vÃ  mÃ´ hÃ¬nh thá»‘ng kÃª cho phÃ©p mÃ¡y tÃ­nh há»c tá»« dá»¯ liá»‡u mÃ  khÃ´ng cáº§n Ä‘Æ°á»£c láº­p trÃ¬nh cá»¥ thá»ƒ.
		
	## Äáº·c Ä‘iá»ƒm chÃ­nh  
		
		### Kháº£ nÄƒng há»c tá»« dá»¯ liá»‡u : 
			ML sá»­ dá»¥ng dá»¯ liá»‡u lÃ m nguá»“n tÃ i nguyÃªn chÃ­nh Ä‘á»ƒ há»c vÃ  phÃ¡t triá»ƒn cÃ¡c mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n hoáº·c nháº­n diá»‡n. Thay vÃ¬ dá»±a trÃªn cÃ¡c quy táº¯c cá»‘ Ä‘á»‹nh, mÃ¡y tÃ­nh cÃ³ thá»ƒ há»c tá»« cÃ¡c máº«u trong dá»¯ liá»‡u Ä‘á»ƒ Ä‘Æ°a ra quyáº¿t Ä‘á»‹nh hoáº·c dá»± Ä‘oÃ¡n. 
		
		### Tá»± Ä‘á»™ng cáº£i thiá»‡n theo thá»i gian 
			ML cÃ³ kháº£ nÄƒng cáº£ thiá»‡n hiá»‡u suáº¥t cá»§a mÃ¬nh thÃ´ng qua viá»‡c tiáº¿p tá»¥c há»c há»i tá»« dá»¯ liá»‡u má»›i. CÃ ng cÃ³ nhiá»u dá»¯ liá»‡u vÃ  thá»i gian há»c, mÃ´ hÃ¬nh ML cÃ ng trá»Ÿ nÃªn chÃ­nh xÃ¡c vÃ  hiá»‡u quáº£ hÆ¡n 

		### Äa dáº¡ng cÃ¡c thuáº­t toÃ¡n 
			ML bao gá»“m cÃ¡c thuáº­t toÃ¡n khÃ¡c nhau, tá»« cÃ¡c thuáº­t toÃ¡n Ä‘Æ¡n giáº£n nhÆ° há»“i quy tuyáº¿n tÃ­nh(linear regression) vÃ  cÃ¢y quyáº¿t Ä‘á»‹nh(decision tree) Ä‘áº¿n cÃ¡c thuáº­t toÃ¡n phá»©c táº¡p hÆ¡n nhÆ° máº¡ng nÆ¡-ron nhÃ¢n táº¡o(neural network) vÃ  mÃ¡y vector há»— trá»£(support vector machines ). Má»—i thuáº­t toÃ¡n cÃ³ Æ°u Ä‘iá»ƒm riÃªng vÃ  Ä‘Æ°á»£c Ã¡p dá»¥ng tÃ¹y thuá»™c vÃ o loáº¡i bÃ i toÃ¡n cáº§n giáº£i quyáº¿t.
			
		### Kháº£ nÄƒng tá»•ng quÃ¡t hÃ³a 
			ML tÃ¬m cÃ¡ch xÃ¢y dá»±ng cÃ¡c mÃ´ hÃ¬nh cÃ³ thá»ƒ tá»•ng quÃ¡t hÃ³a, tá»©c lÃ  khÃ´ng chá»‰ hoáº¡t Ä‘á»™ng tá»‘t trÃªn dá»¯ liá»‡u Ä‘Ã£ há»c mÃ  cÃ²n cÃ³ thá»ƒ Ã¡p dá»¥ng chÃ­nh xÃ¡c dá»¯ liá»‡u má»›i chÆ°a tá»«ng tháº¥y. Äiá»u nÃ y ráº¥t quan trá»ng Ä‘á»ƒ Ä‘áº£m báº£o ráº±ng mÃ´ hÃ¬nh khÃ´ng chá»‰ ghi nhá»› dá»¯ liá»‡u quÃ¡ má»©c mÃ  cÃ²n cÃ³ kháº£ nÄƒng suy Ä‘oÃ¡n. 
			
		### Sá»± phá»¥ thuá»™c vÃ o dá»¯ liá»‡u 
			Hiá»‡u quáº£ cá»§a cÃ¡c mÃ´ hÃ¬nh ML phá»¥ thuá»™c ráº¥t lá»›n vÃ o cháº¥t lÆ°á»£ng vÃ  sá»‘ lÆ°á»£ng dá»¯ liá»‡u. Náº¿u dá»¯ liá»‡u Ä‘áº§u vÃ o khÃ´ng Ä‘á»§ hoáº·c bá»‹ nhiá»…u, mÃ´ hÃ¬nh ML cÃ³ thá»ƒ Ä‘Æ°a ra káº¿t quáº£ khÃ´ng chÃ­nh xÃ¡c hoáº·c khÃ´ng á»•n Ä‘á»‹nh.
	
	
	## PhÆ°Æ¡ng phÃ¡p há»c : 
		### Há»c cÃ³ giÃ¡m sÃ¡t 	
			Dá»¯ liá»‡u Ä‘Æ°á»£c huáº¥n luyá»‡n Ä‘Æ°á»£c gáº¯n nhÃ£n, nghÄ©a lÃ  má»—i dá»¯ liá»‡u Ä‘áº§u vÃ o Ä‘i kÃ¨m vá»›i má»™t Ä‘áº§u ra mong muá»‘n(nhÃ£n) . Má»¥c tiÃªu lÃ  há»c má»™t hÃ m Ã¡nh xáº¡ tá»« Ä‘áº§u vÃ o Ä‘áº¿n Ä‘áº§u ra, sao cho mÃ´ hÃ¬nh cÃ³ thá»ƒ dá»± Ä‘oÃ¡n chÃ­nh xÃ¡c nhÃ£n cho dá»¯ liá»‡u má»›i, chÆ°a tá»«ng tháº¥y 
		
		### Há»c khÃ´ng giÃ¡m sÃ¡t 
			Dá»¯ liá»‡u huáº¥n luyá»‡n khÃ´ng cÃ³ nhÃ£n, vÃ  má»¥c tiÃªu lÃ  tÃ¬m ra cáº¥u trÃºc áº©n trong dá»¯ liá»‡u. PhÆ°Æ¡ng phÃ¡p nÃ y Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ khÃ¡m phÃ¡ dá»¯ liá»‡u, tÃ¬m ra máº«u, hoáº·c giáº£m thiá»ƒu kÃ­ch thÆ°á»›c dá»¯ liá»‡u. 
		
		### Há»c há»c bÃ¡n giÃ¡m sÃ¡t 
			Há»c bÃ¡n giÃ¡m sÃ¡t táº­n dá»¥ng má»™t lÆ°á»£ng nhá» dá»¯ liá»‡u cÃ³ nhÃ£n cÅ©ng vá»›i má»™t lÆ°á»£ng lá»›n dá»¯ liá»‡u khÃ´ng cÃ³ nhÃ£n. Má»¥c tiÃªu lÃ  sá»­ dá»¥ng thÃ´ng tin tá»« dá»¯ liá»‡u khÃ´ng nhÃ£n Ä‘á»ƒ cáº£i thiá»‡n hiá»‡u suáº¥t cá»§a mÃ´ hÃ¬nh, Ä‘áº·c biá»‡t lÃ  khi viá»‡c gáº¯n nhÃ£n dá»¯ liá»‡u tá»‘n kÃ©m hoáº·c khÃ³ khÄƒn  
			
		### Há»c tÄƒng cÆ°á»ng  
			Trong há»c tÄƒng cÆ°á»ng, má»™t tÃ¡c nhÃ¢n(agent ) há»c cÃ¡c thá»±c hiá»‡n cÃ¡c hÃ nh Ä‘á»™ng trong má»™t mÃ´i trÆ°á»ng nháº±m tá»‘i Ä‘a hÃ³a pháº¥n thÆ°á»Ÿng tÃ­ch lÅ©y theo thá»i gian. TÃ¡c nhÃ¢n khÃ´ng Ä‘Æ°á»£c cung cáº¥p dá»¯ liá»‡u gáº¯n nhÃ£n nhÆ° trong há»c cÃ³ giÃ¡m sÃ¡t, mÃ  pháº£i tá»± há»c tá»« cÃ¡c tráº£i nghiá»‡m vÃ  pháº£n há»“i nháº­n Ä‘Æ°á»£c tá»« mÃ´i trÆ°á»ng . 
		
		
	
	## á»¨ng dá»¥ng  
	
		### Email Filtering(Lá»c email)
			Há»c mÃ¡y sá»­ dá»¥ng trong cÃ¡c há»‡ thá»‘ng email Ä‘á»ƒ phÃ¢n loáº¡i thÆ° thÃ nh cÃ¡c danh má»¥c nhÆ° ThÆ° rÃ¡c(Spam), thÆ° há»£p lá»‡(Non-Spam). Thuáº­t toÃ¡n ML sáº½ há»c tá»« cÃ¡c email trÆ°á»›c Ä‘Ã¢y mÃ  ngÆ°á»i dÃ¹ng Ä‘Ã£ Ä‘Ã¡nh dáº¥u lÃ  thÆ° rÃ¡c hoáº·c khÃ´ng pháº£i cÃ¡c thÆ° rÃ¡c Ä‘á»ƒ cáº£i thiá»‡n Ä‘á»™ chÃ­nh xÃ¡c trong viá»‡c phÃ¢n loáº¡i email má»›i.
		
		### Recommendation System(Há»‡ thá»‘ng gá»£i Ã½)
			CÃ¡c há»‡ thá»‘ng gá»£i Ã½ nhÆ° trong Netflix, Amazon, Spotify sá»­ dá»¥ng ML Ä‘á»ƒ phÃ¢n tÃ­ch hÃ nh vi cá»§a ngÆ°á»i dÃ¹ng vÃ  gá»£i Ã½ cÃ¡c sáº³n pháº©m, phim, hoáº·c bÃ i hÃ¡t dá»±a trÃªn sá»Ÿ thÃ­ch vÃ  lá»‹ch sá»­ hoáº¡t Ä‘á»™ng cá»§a há». Thuáº­t toÃ¡n há»c mÃ¡t vÃ  há»c tá»« dá»¯ liá»‡u ngÆ°á»i dÃ¹ng Ä‘á»ƒ Ä‘Æ°a ra cÃ¡c Ä‘á» xuáº¥t cÃ¡ nhÃ¢n hÃ³a. 
			
		### Customer Segmentation(PhÃ¢n khÃºc khÃ¡ch hÃ ng)
			Trong tiáº¿p thá»‹ vÃ  bÃ¡n hÃ ng, ML Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ phÃ¢n chia khÃ¡ch hÃ ng thÃ nh cÃ¡c nhÃ³m dá»±a trÃªn hÃ nh vi mua sáº¯m, Ä‘á»™ tuá»•i, sá»Ÿ thÃ­ch, hoáº·c giÃ¡ trá»‹ kinh táº¿. PhÃ¢n khÃºc khÃ¡ch hÃ ng giÃºp doanh nghiá»‡p tá»‘i Æ°u hÃ³a chiáº¿n lÆ°á»£c tiáº¿p thá»‹ vÃ  cung cáº¥p cÃ¡c sáº£n pháº©m hoáº·c dá»‹ch vá»¥ phÃ¹ há»£p cho tá»«ng nhÃ³m. 
			
		### Predictive Maintenance(Báº£o trÃ¬ dá»± Ä‘oÃ¡n)
			ML Ä‘Æ°á»£c sá»­ dá»¥ng trong cÃ¡c nghÃ nh cÃ´ng nghiá»‡p Ä‘á»ƒ dá»± Ä‘oÃ¡n khi nÃ o mÃ¡y mÃ³c hoáº·c thiáº¿t bá»‹ cÃ³ thá»ƒ há»ng hÃ³c dá»±a trÃªn dá»¯ liá»‡u lá»‹ch sá»­ hoáº¡t Ä‘á»™ng. Äiá»u nÃ y giÃºp giáº£m thiá»ƒu thá»i gian cháº¿t vÃ  chi phÃ­ báº£o trÃ¬ báº±ng cÃ¡ch thá»±c hiá»‡n báº£o trÃ¬ Ä‘Ãºng lÃºc, trÆ°á»›c khi sá»± cá»‘ xáº£y ra. 
		
		### Fraud Detection(PhÃ¡t hiá»‡n gian láº­n )
			Há»c mÃ¡y Ä‘Æ°á»£c Ã¡p dá»¥ng trong lÄ©nh vá»±c tÃ i chÃ­nh Ä‘á»ƒ phÃ¡t hiá»‡n cÃ¡c hÃ nh vi gian láº­n. CÃ¡c thuáº­t toÃ¡n ML cÃ³ thá»ƒ phÃ¢n tÃ­ch hÃ ng triá»‡u giao dá»‹ch trong thá»i gian thá»±c Ä‘á»ƒ phÃ¡t thiá»‡n cÃ¡c máº«u báº¥t thÆ°á»ng hoáº·c cÃ¡c hoáº¡t Ä‘á»™ng Ä‘Ã¡ng ngá», tá»« Ä‘Ã³ ngÄƒn cháº·n gian láº­n trÆ°á»›c khi nÃ³ gÃ¢y ra thiá»‡t háº¡i. 
			
		### Credit Scoring(ÄÃ¡nh giÃ¡ tÃ­n dá»¥ng) 
			CÃ¡c tá»• chá»©c tÃ i chÃ­nh sá»­ dá»¥ng ML Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ má»©c Ä‘á»™ tÃ­n nhiá»‡m cá»§a khÃ¡ch hÃ ng. ML phÃ¢n tÃ­ch lá»‹ch sá»­ tÃ i chÃ­nh, hÃ nh vi chi tiÃªu, vÃ  cÃ¡c yáº¿u tá»‘ khÃ¡c Ä‘á»ƒ dá»± Ä‘oÃ¡n kháº£ nÄƒng thanh toÃ¡n cá»§a khÃ¡ch hÃ ng, tá»« Ä‘Ã³ quyáº¿t Ä‘á»‹nh viá»‡c cáº¥p tin dá»¥ng hoáº·c lÃ£i suáº¥t cho vay.
			
		### Price Optimization(Tá»‘i Æ°u hÃ³a giÃ¡ cáº£)
			ML Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ tá»‘i Æ°u hÃ³a giÃ¡ cáº£ cho cÃ¡c sáº£n pháº©m hoáº·c dá»‹ch vá»¥ dá»±a trÃªn nhu cáº§u thá»‹ trÆ°á»ng, hÃ nh vi cá»§a khÃ¡ch hÃ ng, vÃ  giÃ¡ cáº£ cá»§a Ä‘á»‘i thá»§ cáº¡nh tranh. Äiá»u nÃ y giÃºp doanh nghiá»‡p tá»‘i Æ°u hÃ³a doanh thu vÃ  lá»£i nhuáº­n 
		
		### Image Recognition in Healthcare(Nháº­n diá»‡n hÃ¬nh áº£nh trong y táº¿)
			Há»c mÃ¡y Ä‘Æ°á»£c Ã¡p dá»¥ng trong cÃ¡c há»‡ thá»‘ng nháº­n diá»‡n hÃ¬nh áº£nh y khoa Ä‘á»ƒ phÃ¢n tÃ­ch cÃ¡c hÃ¬nh áº£nh nhÆ° X-quang, MRI, vÃ  siÃªu Ã¢m. CÃ¡c mÃ´ hÃ¬nh ML cÃ³ thá»ƒ giÃºp bÃ¡c sÄ© phÃ¡t hiá»‡n cÃ¡c dáº¥u hiá»‡u bá»‡nh lÃ½ nhÆ° khá»‘i u hoáº·c gÃ£y xÆ°Æ¡ng dá»±a trÃªn hÃ¬nh áº£nh. 
			
		### Sentiment Analysis(PhÃ¢n tÃ­ch cáº£m xÃºc)
			ML Ä‘Æ°á»£c sá»­ dá»¥ng trong phÃ¢n tÃ­ch cáº£m xÃºc tá»« vÄƒn báº£n, vÃ­ dá»¥ nhÆ° Ä‘Ã¡nh giÃ¡ sáº£n pháº©m, bÃ¬nh luáº­n trÃªn máº¡ng xÃ£ há»™i, hoáº·c pháº£n há»“i khÃ¡ch hÃ ng. Thuáº­t toÃ¡n há»c mÃ¡y phÃ¢n tÃ­ch ngÃ´n ngá»¯ tá»± nhiÃªn Ä‘á»ƒ xÃ¡c Ä‘á»‹nh cáº£m xÃºc tÃ­ch cá»±c, tiÃªu cá»±c, hoáº·c trung tÃ­nh trong cÃ¡c vÄƒn báº£n  
		
		### Supply Chain Optimization(Tá»‘i Æ°u hÃ³a chuá»—i cung á»©ng)
			ML giÃºp tá»‘i Æ°u hÃ³a chuá»—i cung á»©ng báº±ng cÃ¡ch dá»± Ä‘oÃ¡n nhu cáº§u sáº£n pháº©m, quáº£n lÃ½ hÃ ng tá»“n kho, vÃ  tá»‘i Æ°u hÃ³a lá»™ trÃ¬nh váº­n chuyá»ƒn. Äiá»u nÃ y giÃºp doanh nghiá»‡p giáº£m thiá»ƒu chi phÃ­ vÃ  tÄƒng cÆ°á»ng hiá»‡u quáº£ hoáº¡t Ä‘á»™ng. 
			
	
	
# Deep Learning 	
	## Äá»‹nh nghÄ©a 
		Deep Learning lÃ  má»™t phÃ¢n nhÃ¡nh cá»§a machine Learning, láº¥y cáº£m há»©ng tá»« cáº¥u trÃºc vÃ  chá»©c nÄƒng cá»§a nÃ£o bá»™ con ngÆ°á»i, sá»­ dá»¥ng cÃ¡c máº¡ng neural nhiá»u lá»›p Ä‘á»ƒ há»c vÃ  trÃ­ch xuáº¥t Ä‘áº·c trÆ°ng tá»« dá»¯ liá»‡u 
		
	## Äáº·c Ä‘iá»ƒm chÃ­nh 
			
		### Máº¡ng nÆ¡-ron sÃ¢u(Deep Neural Networks )	
			Há»c sÃ¢u sá»­ dá»¥ng cÃ¡c máº¡ng nÆ¡ ron nháº­n táº¡o cÃ³ nhiá»u lá»›p áº©n(hidden layers) giá»¯a Ä‘áº§u vÃ o vÃ  Ä‘áº§u ra. Cáº¥u trÃºc nÃ y cho phÃ©p mÃ´ hÃ¬nh há»c cÃ¡c Ä‘áº·c Ä‘iá»ƒm phá»©c táº¡p tá»« dá»¯ liá»‡u. 
			Má»—i lá»›p trong máº¡ng há»c cÃ¡ch trÃ­ch xuáº¥t cÃ¡c Ä‘áº·c Ä‘iá»ƒm khÃ¡c nhau tá»« dá»¯ liá»‡u Ä‘áº§u vÃ o, vá»›i cÃ¡c lá»›p sÃ¢u hÆ¡n trÃ­ch xuáº¥t cÃ¡c Ä‘áº·c Ä‘iá»ƒm trá»«u tÆ°á»£ng vÃ  phá»©c táº¡p hÆ¡n 
			
		### Xá»­ lÃ½ dá»¯ liá»‡u phi cáº¥u trÃºc 
			Há»c sÃ¢u ráº¥t hiá»‡u quáº£ trong viá»‡c xá»­ lÃ½ dá»¯ liá»‡u phi cáº¥u trÃºc nhÆ° hÃ¬nh áº£nh, Ã¢m thanh, video, vÄƒn báº£n. Nhá» vÃ o kháº£ nÄƒng tá»± Ä‘á»™ng trÃ­ch xuáº¥t Ä‘áº·c trÆ°ng tá»« dá»¯ liá»‡u. DL cÃ³ thá»ƒ xá»­ lÃ½ cÃ¡c loáº¡i dá»¯ liá»‡u nÃ y mÃ  khÃ´ng cáº§n Ä‘áº¿n cÃ¡c bÆ°á»›c tiá»n xá»­ lÃ½ phá»©c táº¡p 
			
		### YÃªu cáº§u dá»¯ liá»‡u lá»›n vÃ  tÃ i nguyÃªn tÃ­nh toÃ¡n cao 
			Há»c sÃ¢u cáº§n má»™t lÆ°á»£ng lá»›n dá»¯ liá»‡u Ä‘á»ƒ huáº¥n luyá»‡n mÃ´ hÃ¬nh. Äiá»u nÃ y lÃ  do cÃ¡c máº¡ng nÆ¡-ron sÃ¢u cÃ³ hÃ ng triá»‡u hoáº·c hÃ ng tá»· tham sá»‘ cáº§n Ä‘Æ°á»£c tá»‘i Æ°u hÃ³a 
			CÃ¡c mÃ´ hÃ¬nh DL Ä‘Ã²i há»i tÃ i nguyÃªn tÃ­nh toÃ¡n máº¡nh máº½, thÆ°á»ng sá»­ dá»¥ng GPU(Graphics Processing Units ) hoáº·c TPU(Tensor Processing Units ) Ä‘á»ƒ xá»­ lÃ½ khá»‘i lÆ°á»£ng tÃ­nh toÃ¡n lá»›n trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n. 
			
		### Tá»± Ä‘á»™ng trÃ­ch xuáº¥t Ä‘áº·c trÆ°ng(Feature Learning )
			Má»™t trong nhá»¯ng Æ°u Ä‘iá»ƒm lá»›n cá»§a há»c sÃ¢u kháº£ nÄƒng tá»± Ä‘á»™ng há»c cÃ¡c Ä‘áº·c trÆ°ng tá»« dá»¯ liá»‡u mÃ  khÃ´ng cáº§n sá»± can thiá»‡p cá»§a con ngÆ°á»i. Äiá»u nÃ y khÃ¡c biá»‡t vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p há»c mÃ¡y truyá»n thá»‘ng, nÆ¡i mÃ  viá»‡c chá»n lá»±a vÃ  trÃ­ch xuáº¥t cÃ¡c Ä‘áº·c trÆ°ng thÆ°á»ng yÃªu cáº§u sá»± hiá»ƒu biáº¿t chuyÃªn mÃ´n. 
		
	## Kiá»ƒu kiáº¿n trÃºc phá»• biá»ƒn 
		
		### Máº¡ng neural tÃ­ch cháº­p(CNN )
			Sá»­ dá»¥ng cÃ¡c lá»›p tÃ­ch cáº­p(convolutional layers) Ä‘á»ƒ trÃ­ch xuáº¥t cÃ¡c Ä‘áº·c trÆ°ng tá»« dá»¯ liá»‡u hÃ¬nh áº£nh hoáº·c tÃ­n hiá»‡u. CÃ¡c lá»›p nÃ y Ã¡p dá»¥ng cÃ¡c bá»™ lá»c(filter )Ä‘á»ƒ nháº­n diá»‡n cÃ¡c Ä‘áº·c trÆ°ng nhÆ° cáº¡nh, gÃ³c, vÃ  cÃ¡c máº«u 
		
		### Máº¡ng neural há»“i quy(RNN )
			Sá»­ dá»¥ng cÃ¡c káº¿t ná»‘i há»“i quy Ä‘á»ƒ xá»­ lÃ½ dá»¯ liá»‡u chuá»—i vÃ  dá»¯ liá»‡u tuáº§n tá»± giÃºp duy trÃ¬ thÃ´ng tin tá»« cÃ¡c bÆ°á»›c trÆ°á»›c Ä‘Ã³ trong chuá»—i. 
			
		### Máº¡ng Transformer 
			Sá»­ dá»¥ng cÆ¡ cháº¿ tá»± chÃº Ã½ Ä‘á»ƒ há»c há»i má»‘i quan há»‡ giá»¯a cÃ¡c pháº§n cá»§a dá»¯ liá»‡u Ä‘áº§u vÃ o, giÃºp xá»­ lÃ½ dá»¯ liá»‡u tuáº§n tá»± mÃ  khÃ´ng cáº§n máº¡ng há»“i quy. 
			
		### Máº¡ng Ä‘á»‘i khÃ¡ng sinh(Generative Adversarial Networks - GANs)
			Gá»“m 2 máº¡ng nÆ¡-ron, má»™t máº¡ng sinh(generator) vÃ  má»™t máº¡ng phÃ¢n biá»‡t(discriminator), hoáº¡c Ä‘á»™ng Ä‘á»‘i khÃ¡ng vá»›i nhau. Máº¡ng sinh cá»‘ gáº¯ng táº¡o ra dá»¯ liá»‡u giáº£, trong khi máº¡ng phÃ¢n biá»‡t cá»‘ gáº¯ng phÃ¢n biá»‡t giá»¯a dá»¯ liá»‡u tháº­t vÃ  dá»¯ liá»‡u giáº£. 
			
	## á»¨ng dá»¥ng 
		###  Nháº­n diá»‡n hÃ¬nh áº£nh(Image Recognition)
			PhÃ¢n loáº¡i hÃ¬nh áº£nh trong cÃ¡c ná»n táº£ng máº¡ng xÃ£ há»™i, nháº­n diá»‡n khuÃ´n máº·t, há»‡ thá»‘ng giÃ¡m sÃ¡t an ninh. 
	
		### Nháº­n diá»‡n Ä‘á»‘i tÆ°á»£ng video  (Object Detection in Video) 
			GiÃ¡m sÃ¡t an ninh, phÃ¢n tÃ­ch hÃ nh vi vÃ  há»‡ thá»‘ng Ä‘iá»u khiá»ƒn giao thÃ´ng 
			
		### Xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn(Natural Language  Processing - NLP )
			Dá»‹ch mÃ¡y, táº¡o vÄƒn báº£n tá»± Ä‘á»™ng, phÃ¢n tÃ­ch cáº£m xÃºc vÃ  chatbot. 
		
		### Sinh vÄƒn báº£n (Text Generation ) 
			Viáº¿t bÃ i bÃ¡o, táº¡o ná»™i dung cho máº¡ng xÃ£ há»™i, vÃ  táº¡o cÃ¢u tráº£ lá»i cho cÃ¡c cÃ¢u há»i. 
			
		### Táº¡o hÃ¬nh áº£nh(Image Generation)
			Táº¡o hÃ¬nh áº£nh cho cÃ¡c thiáº¿t káº¿, sÃ¡ng táº¡o nghá»‡ thuáº­t, vÃ  táº¡o cÃ¡c máº«u hÃ¬nh áº£nh cho cÃ¡c á»©ng dá»¥ng giáº£i trÃ­. 
			
		### Nháº­n diá»‡n giá»ng nÃ³i(Speech Recognition)
			Há»‡ thá»‘ng nháº­n diá»‡n giá»ng nÃ³i, trá»£ lÃ½ áº£o vÃ  phá»¥ Ä‘á» tá»± Ä‘á»™ng 
		
		### Nháº­n diá»‡n chá»¯ viÃªt tay(Handwriting recognition)
			Sá»‘ hÃ³a tÃ i liá»‡u, nháº­n diá»‡n chá»¯ viáº¿t tay trong cÃ¡c form vÃ  há»— trá»£ thu tháº­p dá»¯ liá»‡u. 
			
	
# Káº¿t luáº­n 
	Máº·c dÃ¹ AI, Machine Learning vÃ  Deep Learning cÃ³ nhá»¯ng Ä‘iá»ƒm khÃ¡c biá»‡t, chÃºng Ä‘á»u Ä‘Ã³ng vai trÃ² quan trá»ng trong viá»‡c phÃ¡t triá»ƒn cÃ¡c há»‡ thá»‘ng thÃ´ng minh. AI cung cáº¥p khung tá»•ng thá»ƒ, ML mang láº¡i kháº£ nÄƒng há»c tá»« dá»¯ liá»‡u vÃ  DL Ä‘áº©y giá»›i háº¡n cá»§a viá»‡c há»c sÃ¢u vÃ  tá»± Ä‘á»™ng hÃ³a. Khi káº¿t há»£p, ba lÄ©nh vá»±c nÃ y táº¡o nÃªn ná»n táº£ng cho cÃ¡c á»©ng cÃ´ng nghá»‡ tiÃªn tiáº¿n, thay Ä‘á»•i cÃ¡ch chÃºng ta tÆ°Æ¡ng tÃ¡c vá»›i tháº¿ giá»›i xung quanh. 
	Hiá»ƒu rÃµ sá»± khÃ¡c biá»‡t giá»¯a AL, ML, DL khÃ´ng chá»‰ quan trá»ng Ä‘á»‘i vá»›i cÃ¡c nhÃ  phÃ¡t triá»ƒn vÃ  nghiÃªn cá»©u, mÃ  cÃ²n cáº§n thiáº¿t cho báº¥t ká»³ ai muá»‘n náº¯m báº¯t vÃ  táº­n dá»¥ng tiá»m nÄƒng cá»§a cÃ´ng nghá»‡ trong thá»i Ä‘áº¡i sá»‘, Khi cÃ¡c cÃ´ng nghá»‡ nÃ y tiáº¿p tá»¥c phÃ¡t triá»ƒn, chÃºng ta cÃ³ thá»ƒ mong Ä‘á»£i nhá»¯ng Ä‘á»™t phÃ¡ má»›i vÃ  á»©ng dá»¥ng sÃ¡ng táº¡o sáº½ Ä‘á»‹nh hÃ¬nh tÆ°Æ¡ng lai cá»§a nhiá»u ngÃ nh cÃ´ng nghá»‡p vÃ  lÄ©nh vá»±c cá»§a cuá»™c sá»‘ng.  
	
	
//======================== Tá»•ng quan 4 phÆ°Æ¡ng phÃ¡p há»c trÃ­ tuá»‡ nhÃ¢n táº¡o ========================// 
# 1 .  Giá»›i thiá»‡u vá» há»c mÃ¡y 
		
	Há»c mÃ¡y (Machine Learning )	 lÃ  má»™t nhÃ¡nh cá»§a trÃ­ tuá»‡ nhÃ¢n táº¡o AI, táº­p trung vÃ o viá»‡c phÃ¡t triá»ƒn cÃ¡c thuáº­t toÃ¡n vÃ  mÃ´ hÃ¬nh giÃºp mÃ¡y tÃ­nh cÃ³ thá»ƒ há»c tá»« dá»¯ liá»‡u vÃ  Ä‘Æ°a ra quyáº¿t Ä‘á»‹nh hoáº·c dá»± Ä‘oÃ¡n mÃ  khÃ´ng cáº§n láº­p trÃ¬nh cá»¥ thá»ƒ cho tá»«ng tÃ¡c vá»¥. 
	CÃ¡c phÆ°Æ¡ng phÃ¡p há»c mÃ¡y phá»• biáº¿n hiá»‡n nay bao gá»“m 
		- Supervised Learning (há»c mÃ¡y cÃ³ giÃ¡m sÃ¡t)
		- Unsupervised Learning(Há»c mÃ¡y khÃ´ng giÃ¡m sÃ¡t )
		- Semi-Supervised Learning (há»c mÃ¡y bÃ¡n giÃ¡m sÃ¡t )
		- Reinforcement Learning (Há»c tÄƒng cÆ°á»ng )
	
	Má»—i phÆ°Æ¡ng phÃ¡p cÃ³ nhá»¯ng Ä‘áº·c Ä‘iá»ƒm riÃªng, Ä‘Æ°á»£c sá»­ dá»¥ng trong cÃ¡c trÆ°á»ng há»£p khÃ¡c nhau, vÃ  cÃ³ Æ°u Ä‘iá»ƒm cÅ©ng nhÆ° nhÆ°á»£c Ä‘iá»ƒm riÃªng  
	

# 2. Supervised Learning (Há»c mÃ¡y cÃ³ giÃ¡m sÃ¡t) 
	## 2.1 Äá»‹nh nghÄ©a 
		Há»c mÃ¡y cÃ³ giÃ¡m sÃ¡t lÃ  phÆ°Æ¡ng phÃ¡p há»c mÃ¡y mÃ  trong Ä‘Ã³ mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn má»™t táº­p dá»¯ liá»‡u Ä‘Æ°á»£c gÃ¡n nhÃ£n sáºµn. Dá»¯ liá»‡u Ä‘áº§u vÃ o(input) Ä‘Æ°á»£c lÃªn káº¿t vá»›i cÃ¡c nhÃ£n Ä‘áº§u ra(output) mong muá»‘n, vÃ  má»¥c tiÃªu cá»§a mÃ´ hÃ¬nh há»c lÃ  cÃ¡ch Ã¡nh xáº¡ tá»« Ä‘áº§u vÃ o Ä‘áº¿n Ä‘áº§u ra dá»±a trÃªn cáº·p dá»¯ liá»‡u nÃ y 
			
	## 2.2 NguyÃªn lÃ½ hoáº¡t Ä‘á»™ng  
		- Dá»¯ liá»‡u huáº¥n luyá»‡n : Bao gá»“m cÃ¡c cáº·p dá»¯ liá»‡u Ä‘áº§u vÃ o vÃ  Ä‘áº§u ra (VÃ­ dá»¥ hÃ¬nh áº£nh cá»§a má»™t con chÃ³ vÃ  nhÃ£n "chÃ³")
		- QuÃ¡ trÃ¬nh huáº¥n luyá»‡n : MÃ´ hÃ¬nh há»c cÃ¡ch dá»± Ä‘oÃ¡n Ä‘áº§u ra tá»« Ä‘áº§u vÃ o báº±ng cÃ¡ch tá»‘i Æ°u hÃ³a má»™t hÃ m máº¥t mÃ¡t(loss function), thÆ°á»ng lÃ  sá»± khÃ¡c biá»‡t giá»¯a dá»± Ä‘oÃ¡n cá»§a mÃ´ hÃ¬nh vÃ  nhÃ£n thá»±c táº¿ 
		
		- Kiá»ƒm tra : Sau khi huáº¥n luyá»‡n, mÃ´ hÃ¬nh Ä‘Æ°á»£c kiá»ƒm tra trÃªn dá»¯ liá»‡u chÆ°a tá»«ng tháº¥y Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ Ä‘á»™ chÃ­nh xÃ¡c vÃ  kháº£ nÄƒng tá»•ng quÃ¡t hÃ³a . 
		
	## 2.3 Æ¯u Ä‘iá»ƒm vÃ  nhÆ°á»£c Ä‘iá»ƒm  
	
		### Æ¯u Ä‘iá»ƒm  
			- Hiá»‡u quáº£ cao vá»›i cÃ¡c bÃ i toÃ¡n cÃ³ dá»¯ liá»‡u gÃ¡n nhÃ£n Ä‘áº§y Ä‘á»§ 
			- Dá»… dÃ ng Ä‘Ã¡nh giÃ¡ hiá»‡u suáº¥t mÃ´ hÃ¬nh nhá» vÃ o cÃ¡c sá»‘ liá»‡u nhÆ° Ä‘á»™ chÃ­nh xÃ¡c(accuaracy) , F1-score vÃ  AUC 
			
		### NhÆ°á»£c Ä‘iá»ƒm 
			- YÃªu cáº§u lÆ°á»£ng lá»›n dá»¯ liá»‡u Ä‘Æ°á»£c gÃ¡n nhÃ£n, tá»‘n kÃ©m vÃ  máº¥t thá»i gian Ä‘á»ƒ thu tháº­p 
			- KhÃ´ng linh hoáº¡t vá»›i dá»¯ liá»‡u má»›i hoáº·c thay Ä‘á»•i 
			
	## 2.4 á»¨ng dá»¥ng 
		- PhÃ¢n loáº¡i hÃ¬nh áº£nh : PhÃ¢n loáº¡i hÃ¬nh áº£nh vÃ o cÃ¡c danh má»¥c nháº­n diá»‡n Ä‘á»‘i tÆ°á»£ng, phÃ¢n loáº¡i thÆ° má»¥c. 
		- Dá»± Ä‘oÃ¡n tÃ i chÃ­nh : Dá»± Ä‘oÃ¡n giÃ¡ cá»• phiáº¿u, rá»§i ro tÃ­n dá»¥ng dá»±a trÃªn dá»¯ liá»‡u lá»‹ch sá»­ 
		- Xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn(NLP) : PhÃ¢n loáº¡i email, phÃ¢n tÃ­ch cáº£m xÃºc, dá»‹ch mÃ¡y 
		
	
	
# 3. Há»c mÃ¡y khÃ´ng giÃ¡m sÃ¡t (Unsupervised Learning)
	
	##3.1 Äá»‹nh nghÄ©a 
		Há»c mÃ¡y khÃ´ng giÃ¡m sÃ¡t lÃ  phÆ°Æ¡ng phÃ¡p trong Ä‘Ã³ mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn má»™t táº­p dá»¯ liá»‡u khÃ´ng cÃ³ nhÃ£n. Má»¥c tiÃªu cá»§a mÃ´ hÃ¬nh lÃ  tÃ¬m ra cÃ¡c máº«u(pattenrns) hoáº·c cáº¥u trÃºc tiá»m áº©n trong dá»¯ liá»‡u mÃ  khÃ´ng cáº§n biáº¿t Ä‘áº§u ra mong muá»‘n  
		
	## 3.2 NguyÃªn lÃ½ hoáº¡t Ä‘á»™ng 
		- Dá»¯ liá»‡u huáº¥n luyá»‡n :  Chá»‰ bao gá»“m dá»¯ liá»‡u Ä‘áº§u vÃ o mÃ  khÃ´ng cÃ³ nhÃ£n 
		- QuÃ¡ trÃ¬nh huáº¥n luyá»‡n : MÃ´ hÃ¬nh cá»‘ gáº¯ng nhÃ³m cÃ¡c dá»¯ liá»‡u tÆ°Æ¡ng tá»± nhau(clustering) hoáº·c giáº£m sá»‘ chiá»u cá»§a dá»¯ liá»‡u Ä‘á»ƒ dá»… dÃ ng phÃ¢n tÃ­ch(dimensionality reduction)
		- Kiá»ƒm tra :  KhÃ´ng cÃ³ má»™t sá»‘ liá»‡u rÃµ rÃ ng Ä‘á»ƒ Ä‘Ã¡nh giÃ¡, thÆ°á»ng dá»±a trÃªn quan sÃ¡t cá»§a con ngÆ°á»i hoáº·c sá»­ dá»¥ng cÃ¡c biá»‡n phÃ¡p Ä‘Ã¡nh giÃ¡ nhÆ° silhouette score trong clustering 
		
	##3.3 Æ¯u Ä‘iá»ƒm vÃ  nhÆ°á»£c Ä‘iá»ƒm  
		### Æ¯u Ä‘iá»ƒm : 
			KhÃ´ng cáº§n dá»¯ liá»‡u Ä‘Æ°á»£c gÃ¡n nhÃ£n, do Ä‘Ã³ tiáº¿t kiá»‡m chi phÃ­ vÃ  thá»i gian 
			PhÃ¹ há»£p Ä‘á»ƒ khÃ¡m phÃ¡ dá»¯ liá»‡u vÃ  phÃ¡t hiá»‡n cÃ¡c máº«u má»›i mÃ  khÃ´ng bá»‹ áº£nh hÆ°á»Ÿng bá»Ÿi Ä‘á»‹nh kiáº¿n gÃ¡n nhÃ£n 
			
		### NhÆ°á»£c Ä‘iá»ƒm 
			KhÃ³ Ä‘Ã¡nh giÃ¡ hiá»‡u suáº¥t vÃ  cháº¥t lÆ°á»£ng cá»§a mÃ´ hÃ¬nh 
			Dá»… bá»‹ áº£nh hÆ°á»Ÿng bá»Ÿi nhiá»…u vÃ  sá»± Ä‘a dáº¡ng trong dá»¯ liá»‡u 
			
	## 3.4 á»¨ng dá»¥ng : 	
		- PhÃ¢n cá»¥m khÃ¡ch hÃ ng :PhÃ¢n nhÃ³m khÃ¡ch hÃ ng thÃ nh cÃ¡c phÃ¢n khÃºc Ä‘á»ƒ cÃ¡ nhÃ¢n hÃ³a chiáº¿n lÆ°á»£c marketing 
		- Giáº£m sá»‘ chiá»u dá»¯ liá»‡u : Sá»­ dá»¥ng phÆ°Æ¡ng phÃ¡p PCA (Principal Component Analysis) Ä‘á»ƒ giáº£m sá»‘ chiá»u cá»§a dá»¯ liá»‡u cho viá»‡c trá»±c quan hÃ³a vÃ  tÄƒng tá»‘c Ä‘á»™ xá»­ lÃ½ 
		- PhÃ¡t hiá»‡n báº¥t thÆ°á»ng : TÃ¬m ra cÃ¡c máº«u báº¥t thÆ°á»ng trong dá»¯ liá»‡u tÃ i chÃ­nh hoáº·c dá»¯ liá»‡u y táº¿ .
		
	
# 4. Há»c mÃ¡y bÃ¡n giÃ¡m sÃ¡t 

	## 4.1 Äá»‹nh nghÄ©a 
		Há»c mÃ¡y bÃ¡n giÃ¡m sÃ¡t lÃ  phÆ°Æ¡ng phÃ¡p káº¿t há»£p giá»¯a há»c mÃ¡y cÃ³ giÃ¡m sÃ¡t vÃ  khÃ´ng giÃ¡m sÃ¡t. Trong Ä‘Ã³, má»™t pháº§n nhá» cá»§a dá»¯ liá»‡u huáº¥n luyá»‡n Ä‘Æ°á»£c gÃ¡n nhÃ£n, cÃ²n láº¡i lÃ  dá»¯ liá»‡u khÃ´ng gÃ¡n nhÃ£n. Má»¥c tiÃªu lÃ  táº­n dá»¥ng thÃ´ng tin tá»« cáº£ dá»¯ liá»‡u Ä‘Æ°á»£c gÃ¡n nhÃ£n vÃ  khÃ´ng gÃ¡n nhÃ£n Ä‘á»ƒ cáº£i thiá»‡n Ä‘á»™ chÃ­nh xÃ¡c cá»§a mÃ´ hÃ¬nh. 
		
	## 4.2 NguyÃªn lÃ½ hoáº¡t Ä‘á»™ng  
		- Dá»¯ liá»‡u huáº¥n luyá»‡n : Bao gá»“m má»™t pháº§n nhá» dá»¯ liá»‡u Ä‘Æ°á»£c gÃ¡n nhÃ£n vÃ  pháº§n lá»›n dá»¯ liá»‡u khÃ´ng cÃ³ nhÃ£n. 
		- QuÃ¡ trÃ¬nh huáº¥n luyá»‡n : bao gá»“m mÃ´ hÃ¬nh há»c tá»« dá»¯ liá»‡u gÃ¡n nhÃ£n vÃ  khÃ´ng gÃ¡n nhÃ£n Ä‘á»ƒ tÃ¬m ra cáº¥u trÃºc tiá»m áº©n, sau Ä‘Ã³ cáº£i thiá»‡n dá»± Ä‘oÃ¡n cho cÃ¡c dá»¯ liá»‡u chÆ°a Ä‘Æ°á»£c gÃ¡n nhÃ£n.
		- Kiá»ƒm tra : ÄÆ°á»£c thá»±c hiá»‡n trÃªn táº­p dá»¯ liá»‡u khÃ´ng gÃ¡n nhÃ£n hoáº·c trÃªn má»™t táº­p dá»¯ liá»‡u kiá»ƒm tra riÃªng biá»‡t .
		
	
	## 4.3 Æ¯u Ä‘iá»ƒm vÃ  nhÆ°á»£c Ä‘iá»ƒm  
		### Æ¯u Ä‘iá»ƒm 	
		- Hiá»‡u quáº£ hÆ¡n so vá»›i há»c mÃ¡y cÃ³ giÃ¡m sÃ¡t khi chá»‰ cÃ³ má»™t lÆ°á»£ng nhá» dá»¯ liá»‡u Ä‘Æ°á»£c gÃ¡n nhÃ£n 
		- Táº­n dá»¥ng Ä‘Æ°á»£c thÃ´ng tin tá»« dá»¯ liá»‡u khÃ´ng gÃ¡n nhÃ£n Ä‘á»ƒ cáº£i thiá»‡n hiá»‡u suáº¥t mÃ´ hÃ¬nh 
		
		### NhÆ°á»£c Ä‘iá»ƒm 
			- Phá»©c táº¡p hÆ¡n vá» máº·t triá»ƒn khai so vá»›i há»c mÃ¡y cÃ³ giÃ¡m sÃ¡t 
			-  ÄÃ²i há»i sá»± cÃ¢n nháº¯c tháº­n trá»ng trong viá»‡c lá»±a chá»n dá»¯ liá»‡u vÃ  phÆ°Æ¡ng phÃ¡p huáº¥n luyá»‡n.
			
	## 4.4 á»¨ng dá»¥ng 
		### PhÃ¢n loáº¡i vÄƒn báº£n 
			- DÃ¹ng má»™t sá»‘ lÆ°á»£ng nhá» tÃ i liá»‡u Ä‘Æ°á»£c gÃ¡n nhÃ£n Ä‘á»ƒ phÃ¢n loáº¡i toÃ n bá»™ cÆ¡ sá»Ÿ dá»¯ liá»‡u vÄƒn báº£n 
			
		### Nháº­n diá»‡n hÃ¬nh áº£nh  
			- Sá»­ dá»¥ng má»™t táº­p nhá» hÃ¬nh áº£nh Ä‘Æ°á»£c gÃ¡n nhÃ£n Ä‘á»ƒ huáº¥n luyá»‡n mÃ´ hÃ¬nh vÃ  sau Ä‘Ã³ Ã¡p dá»¥ng lÃªn má»™t láº­p lá»›n hÃ¬nh áº£nh khÃ´ng gÃ¡n nhÃ£n 
			
		### PhÃ¢n tÃ­ch y há»c  
			- PhÃ¢n loáº¡i hÃ¬nh áº£nh y táº¿ vá»›i má»™t sá»‘ lÆ°á»£ng nhá» cÃ¡c hÃ¬nh áº£nh Ä‘Æ°á»£c chuyÃªn gia y táº¿ gÃ¡n nhÃ£n  
			


# 5. Há»c tÄƒng cÆ°á»ng(Reinforcement Learning)
	
	## 5.1 Äá»‹nh nghÄ©a 
		Há»c tÄƒng cÆ°á»ng(Reinforcement Learning - RL) lÃ  má»™t phÆ°Æ¡ng phÃ¡p há»c mÃ¡y, trong Ä‘Ã³ má»™t tÃ¡c nhÃ¢n(agent) há»c cÃ¡ch thá»±c hiá»‡n cÃ¡c hÃ nh Ä‘á»™ng trong má»™t mÃ´i trÆ°á»ng Ä‘á»ƒ tá»‘i Æ°u hÃ³a pháº§n thÆ°á»Ÿng nháº­n Ä‘Æ°á»£c theo thá»i gian. TÃ¡c nhÃ¢n khÃ´ng Ä‘Æ°á»£c cung cáº¥p sáºµn nhÃ£n cho tá»«ng hÃ nh Ä‘á»™ng, mÃ  thay vÃ o Ä‘Ã³, nÃ³ pháº£i khÃ¡m phÃ¡ mÃ´i trÆ°á»ng vÃ  há»c tá»« nhá»¯ng kinh nghiá»‡m thÃ´ng qua tÆ°Æ¡ng tÃ¡c. 
		
	## 5.2 NguyÃªn lÃ½ hoáº¡t Ä‘á»™ng 
		Há»c tÄƒng cÆ°á»ng hoáº¡t Ä‘á»™ng dá»±a trÃªn má»™t chu trÃ¬nh láº·p láº¡i giá»¯a tÃ¡c nhÃ¢n vÃ  mÃ´i trÆ°á»ng. TÃ¡c nhÃ¢n thá»±c hiá»‡n má»™t hÃ nh Ä‘á»™ng dá»±a trÃªn tráº¡ng thÃ¡i hiá»‡n táº¡i cá»§a mÃ´i trÆ°á»ng, vÃ  nháº­n Ä‘Æ°á»£c pháº§n thÆ°á»Ÿng cÃ¹ng vá»›i tráº¡ng thÃ¡i má»›i. Má»¥c tiÃªu cá»§a tÃ¡c nhÃ¢n lÃ  tá»‘i Ä‘a hÃ³a tá»•ng pháº§n thÆ°á»Ÿng nháº­n Ä‘Æ°á»£c trong dÃ i háº¡n. Má»™t sá»‘ thuáº­t toÃ¡n phá»• biáº¿n trong há»c tÄƒng cÆ°á»ng bao gá»“m Q-learning, SARSA, Deep Q-Network(DQN)
		
	## 5.3 Æ¯u Ä‘iá»ƒm vÃ  nhÆ°á»£c Ä‘iá»ƒm 
		### Æ¯u Ä‘iá»ƒm : 
			- TÃ­nh tá»± Ä‘á»™ng hÃ³a : Há»c tÄƒng cÆ°á»ng cÃ³ kháº£ nÄƒng há»c vÃ  Ä‘iá»u chá»‰nh chiáº¿n lÆ°á»£c mÃ  khÃ´ng cáº§n can thiá»‡p cá»§a con ngÆ°á»i sau khi Ä‘Æ°á»£c thiáº¿t láº­p. 
			
			- Kháº£ nÄƒng thÃ­ch á»©ng : TÃ¡c nhÃ¢n cÃ³ thá»ƒ thÃ­ch á»©ng vá»›i cÃ¡c thay Ä‘á»•i trong mÃ´i trÆ°á»ng vÃ  tá»‘i Æ°u hÃ³a chiáº¿n lÆ°á»£c Ä‘á»ƒ Ä‘áº¡t hiá»‡u suáº¥t cao hÆ¡n 
			
			- á»¨ng dá»¥ng rá»™ng rÃ£i :  RL cÃ³ thá»ƒ Ä‘Æ°á»£c Ã¡p dá»¥ng trong nhiá»u lÄ©nh vÆ°á»£c khÃ¡c nhau nhÆ° trÃ² chÆ¡i, robot, vÃ  tá»‘i Æ°u hÃ³a cÃ¡c há»‡ thá»‘ng phá»©c táº¡p 
			
		### NhÆ°á»£c Ä‘iá»ƒm : 
			- Thá»i gian há»c lÃ¢u :  Äá»ƒ Ä‘áº¡t Ä‘Æ°á»£c chiáº¿n lÆ°á»£c tá»‘i Æ°u, RL thÆ°á»ng Ä‘Æ°á»£c yÃªu cáº§u thá»i gian há»c ráº¥t dÃ i, Ä‘áº·c biá»‡t lÃ  trong cÃ¡c mÃ´i trÆ°á»ng phá»©c táº¡p 
			
			- Äá»™ khÃ³ trong viá»‡c thiáº¿t láº­p  : Viá»‡c thiáº¿t láº­p cÃ¡c tham sá»‘ vÃ  mÃ´i trÆ°á»ng há»c tÄƒng cÆ°á»ng cÃ³ thá»ƒ phá»©c táº¡p, Ä‘Ã²i há»i nhiá»u kinh nghiá»‡m 
			
			- Rá»§i ro khÃ¡m phÃ¡ :  Trong quÃ¡ trÃ¬nh khÃ¡m phÃ¡ mÃ´i trÆ°á»ng, tÃ¡c nhÃ¢n cÃ³ thá»ƒ thá»±c hiá»‡n cÃ¡c hÃ nh Ä‘á»™ng gÃ¢y háº¡i hoáº·c khÃ´ng hiá»‡u quáº£ trÆ°á»›c khi tÃ¬m Ä‘Æ°á»£c chiáº¿n lÆ°á»£c tá»‘i Æ°u .
			
	## á»¨ng dá»¥ng  
		Há»c tÄƒng cÆ°á»ng Ä‘Æ°á»£c á»©ng dá»¥ng thÃ nh cÃ´ng trong nhiá»u lÄ©nh vá»±c nhÆ° 
		
		- TrÃ² chÆ¡i Ä‘iá»‡n tá»­ : RL Ä‘Ã£ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ phÃ¡t triá»ƒn cÃ¡c tÃ¡c nhÃ¢n chÆ¡i game cÃ³ kháº£ nÄƒng vÆ°á»£t qua con ngÆ°á»i, nhÆ° AlphaGO cá»§a Google DeepMind 
		
		- Robot há»c : CÃ¡c robot cÃ³ thá»ƒ sá»­ dá»¥ng RL Ä‘á»ƒ há»c cÃ¡ch di chuyá»ƒn, tÆ°Æ¡ng tÃ¡c vá»›i mÃ´i trÆ°á»ng vÃ  hoÃ n thÃ nh cÃ¡c nhiá»‡m vá»¥ phá»©c táº¡p. 
		
		- Tá»‘i Æ°u hÃ³a há»‡ thá»‘ng : RL Ä‘Æ°á»£c Ã¡p dá»¥ng Ä‘á»ƒ tá»‘i Æ°u hÃ³a cÃ¡c há»‡ thá»‘ng phá»©c táº¡p nhÆ° máº¡ng lÆ°á»›i giao thÃ´ng, quáº£n lÃ½ nÄƒng lÆ°á»£ng vÃ  tÃ i chÃ­nh. 
		
		
# 6. Káº¿t luáº­n 
	Há»c mÃ¡y cÃ³ giÃ¡m sÃ¡t, khÃ´ng giÃ¡m sÃ¡t, bÃ¡n giÃ¡m sÃ¡t vÃ  há»c tÄƒng cÆ°á»ng Ä‘á»u lÃ  nhá»¯ng phÆ°Æ¡ng phÃ¡p há»c quan trá»ng trong lÄ©nh vá»±c há»c mÃ¡y, má»—i phÆ°Æ¡ng phÃ¡p cÃ³ Æ°u vÃ  nhÆ°á»£c Ä‘iá»ƒm riÃªng vÃ  phÃ¹ há»£p vá»›i cÃ¡c loáº¡i dá»¯ liá»‡u vÃ  á»©ng dá»¥ng khÃ¡c nhau. Viá»‡c hiá»ƒu rÃµ sá»± khÃ¡c biá»‡t giá»¯a chÃºng sáº½ giÃºp báº¡n chá»n phÆ°Æ¡ng phÃ¡p phÃ¹ há»£p nháº¥t vá»›i cÃ¡c bÃ i toÃ n cá»¥ thá»ƒ, tá»« Ä‘Ã³ tá»‘i Æ°u hÃ³a hiá»‡u suáº¥t vÃ  hiá»‡u quáº£ cá»§a cÃ¡c há»‡ thá»‘ng AI .
			
		
		
		
//==================================== Há»“i quy tuyáº¿n tÃ­nh =============================// 

# 1. Giá»›i thiá»‡u 
	
		Há»“i quy tuyáº¿n tÃ­nh lÃ  má»™t ká»¹ thuáº­t thá»‘ng kÃª cÆ¡ báº£n Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i trong phÃ¢n tÃ­ch dá»¯ liá»‡u vÃ  há»c mÃ¡y. Má»¥c tiÃªu cá»§a há»“i quy tuyáº¿n tÃ­nh lÃ  xÃ¢y dá»±ng má»™t mÃ´ hÃ¬nh toÃ¡n há»c Ä‘á»ƒ dá»± Ä‘oÃ¡n giÃ¡ trá»‹ cá»§a má»™t biáº¿n phá»¥ thuá»™c (output) dá»±a trÃªn má»™t hoáº·c nhiá»u biáº¿n Ä‘á»™c láº­p(input). MÃ´ hÃ¬nh há»“i quy tuyáº¿n tÃ­nh giáº£ Ä‘á»‹nh ráº±ng má»‘i quan há»‡ giá»¯a cÃ¡c biáº¿n nÃ y lÃ  tuyáº¿n tÃ­nh, nghÄ©a lÃ  cÃ³ thá»ƒ biá»ƒu diá»…n dÆ°á»›i dáº¡ng má»™t Ä‘Æ°á»ng tháº³ng trong khÃ´ng gian sá»‘ liá»‡u. 
		https://aicandy.vn/wp-content/uploads/2024/09/aicandy_hoiquytuyentinh.jpg
		
		
		SimpleLinearRegression : Há»“i quy tuyáº¿n tÃ­nh Ä‘Æ¡n giáº£n 
		Multiple Linear Regression : Há»“i quy tuyáº¿n tÃ­nh Ä‘a biáº¿n 
		
		
# 2.  PhÃ¢n loáº¡i : 
	## 2.1 Há»“i quy tuyáº¿n tÃ­nh Ä‘Æ¡n giáº£n 
		Há»“i quy tuyáº¿n tÃ­nh Ä‘Æ¡n giáº£m mÃ´ hÃ¬nh hÃ³a má»‘i quan há»‡ giá»¯a má»™t biáº¿n Ä‘á»™c láº­p X vÃ  má»™t biáº¿n phá»¥ thuá»™c Y . CÃ´ng thá»©c cá»§a mÃ´ hÃ¬nh há»“i quy tuyáº¿n tÃ­nh Ä‘Æ¡n giáº£n lÃ  : 
						
						Y = b0 + b1X + e 
		
		Trong Ä‘Ã³ : 
			- Y lÃ  biáº¿n phá»¥ thuá»™c (output) <mjx-c class="mjx-c1D44C TEX-I"></mjx-c>
			- X lÃ  biáº¿n Ä‘á»™c láº­p (input)		<mjx-c class="mjx-c1D44B TEX-I"></mjx-c>
			- B0 lÃ  háº±ng sá»‘ (intercept)  <mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D6FD TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c30"></mjx-c></mjx-mn></mjx-script></mjx-msub>
			- B1 lÃ  há»‡ sá»‘ há»“i quy (slope) , Ä‘áº¡i diá»‡n cho má»©c thay Ä‘á»•i cá»§a Y khi X thay Ä‘á»•i má»™t Ä‘Æ¡n vá»‹   <mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D6FD TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-script></mjx-msub>
			- e lÃ  sai sá»‘(error tern), biá»ƒu thá»‹ pháº§n biáº¿n Ä‘á»™ng cá»§a Y khÃ´ng Ä‘Æ°á»£c giáº£i thÃ­ch bá»Ÿi X  <mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D716 TEX-I"></mjx-c></mjx-mi></mjx-math>
			
		#### Trong toÃ¡n há»c 
			- e lÃ  pháº§n chÃªnh lá»‡ch giá»¯a giÃ¡ trá»‹ thá»±c táº¿ y vÃ  giÃ¡ trá»‹ dá»± Ä‘oÃ¡n y^ mÃ  mÃ´ hÃ¬nh tÃ­nh Ä‘Æ°á»£c 
			- NÃ³ biá»ƒu thá»‹ nhá»¯ng yáº¿u tá»‘ khÃ´ng Ä‘Æ°á»£c mÃ´ hÃ¬nh hÃ³a hoáº·c cÃ¡c yáº¿u tá»‘ ngáº«u nhiÃªn khÃ´ng náº±m trong dá»¯ liá»‡u X 
			
		#### Trong AI 
			- Sai sá»‘ e lÃ  má»™t thÆ°á»›c Ä‘o Ä‘á»ƒ mÃ´ hÃ¬nh biáº¿t nÃ³ dá»± Ä‘oÃ¡n kÃ©m chÃ­nh xÃ¡c á»Ÿ má»©c nÃ o 
			- Má»¥c tiÃªu khi huáº¥n luyá»‡n má»™t mÃ´ hÃ¬nh lÃ  lÃ m cho tá»•ng e hoáº·c má»™t dáº¡ng khÃ¡c cá»§a sai sá»‘ nhÆ° bÃ¬nh phÆ°Æ¡ng cá»§a e cÃ ng nhá» cÃ ng tá»‘t, tá»©c lÃ  lÃ m mÃ´ hÃ¬nh khá»›p tá»‘t nháº¥t vá»›i dá»¯ liá»‡u. 
			
		### LiÃªn quan Ä‘áº¿n MachineLearning 
			- Trong há»c mÃ¡y, sai sá»‘ nÃ y Ä‘Æ°á»£c tá»‘i Æ°u hÃ³a qua cÃ¡c phÆ°Æ¡ng phÃ¡p nhÆ° gradient sescent. MÃ´ hÃ¬nh sáº½ Ä‘iá»u chá»‰nh cÃ¡c tham sá»‘ b0, b1 Ä‘á»ƒ giáº£m sai sá»‘ e 
			- Náº¿u e quÃ¡ lá»›n hoáº·c phÃ¢n phá»‘i khÃ´ng ngáº«u nhiÃªn(VÃ­ dá»¥ cÃ³ máº«u hÃ¬nh láº·p láº¡i), Ä‘iá»u nÃ y cho tháº¥y mÃ´ hÃ¬nh chÆ°a phÃ¹ há»£p hoáº·c dá»¯ liá»‡u cÃ³ váº¥n Ä‘á»   
	
	## 2.2  Há»“i quy tuyáº¿n tÃ­nh Ä‘a biáº¿n 
		Há»“i quy tuyáº¿n tÃ­nh Ä‘a biáº¿n má»Ÿ rá»™ng mÃ´ hÃ¬nh trÃªn báº±ng cÃ¡ch sá»­ dá»¥ng nhiá»u biáº¿n Ä‘á»™c láº­p Ä‘á»ƒ dá»± Ä‘oÃ¡n biáº¿n phá»¥ thuá»™c. CÃ´ng thá»©c tá»•ng quÃ¡t cá»§a mÃ´ hÃ¬nh há»“i quy tuyáº¿n tÃ­nh Ä‘a biáº¿n lÃ  : 
			
				Y = B0 + B1.X1 + B2.X2 + ..... + Bn.Xn + e 
				
			- Trong Ä‘Ã³ : 
				X1 , X2 , .... Xn lÃ  cÃ¡c biáº¿n Ä‘á»™c láº­p 
				B1,B2,Bn lÃ  cÃ¡c há»‡ sá»‘ há»“i quy tÆ°Æ¡ng á»©ng, biá»ƒu thá»‹ má»©c thay Ä‘á»•i cá»§a Y má»—i khi Xi thay Ä‘á»•i má»™t Ä‘Æ¡n vá»‹ 
				b0 : LÃ  giÃ¡ trá»‹ y khi cáº£ x1 vÃ  x2 Ä‘á»u báº±ng 0 , Ä‘Ã¢y lÃ  Ä‘iá»ƒm khá»Ÿi Ä‘áº§u trÃªn trá»¥c y 
				b1: lÃ  há»‡ sá»‘ há»“i quy cá»§a x1. NÃ³ cho biáº¿t khi x1 tÄƒng lÃªn 1 Ä‘Æ¡n vá»‹, y sáº½ thay Ä‘á»•i bao nhiÃªu 
				b2 : LÃ  há»‡ sá»‘ há»“i quy cá»§a x2. NÃ³ cho biáº¿t khi x2 tÄƒng lÃªn 1 Ä‘Æ¡n vá»‹ (vá»›i x1 giá»¯ nguyÃªn), y sáº½ thay Ä‘á»•i bao nhiÃªu 
				
			-> MÃ´ hÃ¬nh nÃ y cá»‘ gáº¯ng tÃ¬m ra má»‘i quan há»‡ giá»¯a 2 biáº¿n Ä‘áº§u vÃ o (x1,x2)vÃ  má»™t biáº¿n Ä‘áº§u ra(y). NÃ³ tÃ­nh toÃ¡n Ä‘á»ƒ tÃ¬m ra má»™t máº·t pháº³ng trong khÃ´ng gian 3d sao cho máº·t pháº³ng nÃ y phÃ¹ há»£p nháº¥t vá»›i cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u  
				
# 3 CÃ¡ch tÃ­nh há»‡ sá»‘ há»“i quy 
	
	Há»‡ sá»‘ há»“i quy B Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh báº±ng cÃ¡ch tá»‘i hiá»ƒu hÃ³a tá»•ng bÃ¬nh phÆ°Æ¡ng sai sá»‘(Residual Sum of Squares - RSS ) 
	giá»¯a giÃ¡ trá»‹ dá»± Ä‘oÃ¡n vÃ  giÃ¡ trá»‹ thá»±c táº¿. PhÆ°Æ¡ng phÃ¡p phá»• biáº¿n nháº¥t Ä‘á»ƒ Æ°á»›c lÆ°á»£ng cÃ¡c há»‡ sá»‘ há»“i quy lÃ  phÆ°Æ¡ng phÃ¡p bÃ¬nh phÆ°Æ¡ng tá»‘i thiá»ƒu (Ordinary Least Squares - OLS )
	
	## PhÆ°Æ¡ng phÃ¡p bÃ¬nh phÆ°Æ¡ng tá»‘i thiá»ƒu 
		CÃ´ng thá»©c Ä‘á»ƒ tÃ­nh há»‡ sá»‘ há»“i quy trong há»“i quy tuyáº¿n tÃ­nh Ä‘Æ¡n giáº£n : 
		### 1. B1 há»‡ sá»‘ gÃ³c 
			<MathJax Original Source>
			\hat{\beta_1} = \frac{\sum_{i=1}^{n} (X_i â€“ \bar{X})(Y_i â€“ \bar{Y})}{\sum_{i=1}^{n} (X_i â€“ \bar{X})^2}
			
			
		- Trong Ä‘Ã³ : 
			b1  vÃ  b0 lÃ  cÃ¡c giÃ¡ trá»‹ Æ°á»›c lÆ°á»£ng cá»§a B1 vÃ  B0. 
			Xi  vÃ  Yi lÃ  cÃ¡c giÃ¡ trá»‹ biáº¿n Ä‘á»™c láº­p cá»§a biáº¿n phá»¥ thuá»™c táº¡i Ä‘iá»ƒm dá»¯ liá»‡u thá»© i 
			X_ vÃ  Y_ lÃ  giÃ¡ trá»‹ trung bÃ¬nh cá»§a X vÃ  Y 
 
â€‹	

	### 2. B0 : Há»‡ sá»‘ cháº·n  
				b0 = y_ - b1 x_
	
		
#4. MÃ´ hÃ¬nh há»“i quy tuyáº¿n tÃ­nh vá»›i Python 
	## 4.1 Thá»±c hiá»‡n há»“i quy tuyáº¿n tÃ­nh Ä‘Æ¡n giáº£n báº±ng Python ()
		
		### e.g VÃ­ dá»¥ vá» cÃ¡ch thá»±c hiá»‡n há»“i quy tuyáº¿n tÃ­nh Ä‘Æ¡n giáº£n báº±ng Python sá»­ dá»¥ng thÆ° viÃªn scikit-learn Ä‘á»ƒ dá»± Ä‘oÃ¡n má»™t biáº¿n y dá»±a trÃªn biáº¿n x 
					import numpy as np
					import matplotlib.pyplot as plt 
					from sklearn.linear_model import LinearRegression 
					
					''' VÃ­ dá»¥ vá» cÃ¡ch thá»±c hiá»‡n há»“i quy tuyáº¿n tÃ­nh Ä‘Æ¡n giáº£n báº±ng Python sá»­ dá»¥ng thÆ° viÃªn scikit-learn'''
					
					# Dá»¯ liá»‡u máº«u 
					X = np.array([1,2,3,4,5]).reshape(-1,1)
					y = np.array([1,3,3,2,5])
					
					# Khá»Ÿi táº¡o mÃ´ hÃ¬nh 
					model = LinearRegression()
					
					# Huáº¥n luyá»‡n mÃ´ hÃ¬nh 
					model.fit(X,y)
					
					# Dá»± Ä‘oÃ¡n 
					y_pred =  model.predict(X)
					
					# Há»‡ sá»‘ há»“i quy 
					print(f"Há»‡ sá»‘ há»“i quy : {model.coef_[0]}")
					print(f"Há»‡ sá»‘ há»“i quy : {model.intercept_}")
					
					# Váº½ Ä‘á»“ thá»‹ 
					plt.scatter(X , y,color='blue')
					plt.plot(X , y_pred , color='red')
					plt.xlabel('X')
					plt.ylabel('Y')
					plt.title('Há»“i quy tuyáº¿n tÃ­nh Ä‘Æ¡n giáº£n')
					plt.show()

			
		### Giáº£i thÃ­ch 
			1. ThÆ° viá»‡n sá»­ dá»¥ng 
				numpy : Xá»­ lÃ½ dá»¯ liá»‡u dÆ°á»›i dáº¡ng máº£ng sá»‘ há»c 
				matplotlib.pyplot:Váº½ Ä‘á»“ thá»‹ Ä‘á»ƒ trá»±c quan hÃ³a dá»¯ liá»‡u 
				sklearn.linear_model.LinearRegression : Cung cáº¥p mÃ´ hÃ¬nh há»“i quy tuyáº¿n tÃ­nh 
			
			2. Dá»¯ liá»‡u máº«u : 
				X = np.array([1,2,3,4,5]).reshape(-1,1): ÄÃ¢y lÃ  biáº¿n Ä‘á»™c láº­p X, Ä‘Æ°á»£c Ä‘á»‹nh dáº¡ng thÃ nh máº£ng cá»™t (vÃ¬ scikit-learn yÃªu cáº§u Ä‘áº§u vÃ o pháº£i cÃ³ Ä‘á»‹nh dáº¡ng nÃ y )
				
				y = np.array([1,3,3,2,5]): LÃ  biáº¿n phá»¥ thuá»™c y mÃ  báº¡n muá»‘n dá»± Ä‘oÃ¡n  
			
			3. Khá»Ÿi táº¡o mÃ´ hÃ¬nh huáº¥n luyá»‡n : 
				model = LinearRegression(): Khá»Ÿi táº¡o má»™t mÃ´ hÃ¬nh há»“i quy tuyáº¿n tÃ­nh  
				
				model.fit(X,y) : Huáº¥n luyá»‡n mÃ´ hÃ¬nh báº±ng cÃ¡ch tÃ¬m ra cÃ¡c há»‡ sá»‘ há»“i quy b0 vÃ  b1 dá»±a trÃªn dá»¯ liá»‡u X vÃ  y 
				
			4. Dá»± Ä‘oÃ¡n 
				y_pred = model.predict(X): Dá»± Ä‘oÃ¡n giÃ¡ trá»‹ y dá»±a trÃªn dá»¯ liá»‡u X 
				
			5. In cÃ¡c há»‡ sá»‘ há»“i quy : 
				model.coef_ : Há»‡ sá»‘ gÃ³c b1, cho biáº¿t má»©c Ä‘á»™ thay Ä‘á»•i cá»§a y khi x thay Ä‘á»•i 
				
				model.intercept_ : Há»‡ sá»‘ cháº·n b0, giÃ¡ trá»‹ y khi x = 0 
				
			6. Váº½ Ä‘á»“ thá»‹ : 
				plt.scatter(X , y , color = 'blue') : Váº½ cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u thá»±c táº¿(X , y).
				
				plt.plot(X , y_pred, color = 'red') : váº½ Ä‘Æ°á»ng há»“i quy tuyáº¿n tÃ­nh 
				
				plt.xlabel  , plt.ylabel , plt.title : Äáº·t nhÃ£n vÃ  tiÃªu Ä‘á» cho Ä‘á»“ thá»‹. 
				
			--
			
				
			
			
	# 4.2 Thá»±c hiá»‡n há»“i quy tuyáº¿n tÃ­nh Ä‘a biáº¿n báº±ng Python 
		
			import numpy as np
			from sklearn.linear_model import LinearRegression
			import matplotlib.pyplot as plt
			from mpl_toolkits.mplot3d import Axes3D
			
			# Dá»¯ liá»‡u máº«u
			X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])
			y = np.dot(X, np.array([1, 2])) + 3
			
			# Khá»Ÿi táº¡o mÃ´ hÃ¬nh
			model = LinearRegression()
			
			# Huáº¥n luyá»‡n mÃ´ hÃ¬nh
			model.fit(X, y)
			
			# Dá»± Ä‘oÃ¡n
			y_pred = model.predict(X)
			
			# Há»‡ sá»‘ há»“i quy
			print(f"Há»‡ sá»‘ há»“i quy: {model.coef_}")
			print(f"Intercept: {model.intercept_}")
			
			# Váº½ Ä‘á»“ thá»‹ 3D
			fig = plt.figure()
			ax = fig.add_subplot(111, projection='3d')
			
			# Dá»¯ liá»‡u gá»‘c (cháº¥m xanh)
			ax.scatter(X[:, 0], X[:, 1], y, color='blue', label='Dá»¯ liá»‡u thá»±c táº¿')
			
			# Dá»¯ liá»‡u dá»± Ä‘oÃ¡n (máº·t pháº³ng há»“i quy)
			x1_range = np.linspace(X[:, 0].min(), X[:, 0].max(), 10)
			x2_range = np.linspace(X[:, 1].min(), X[:, 1].max(), 10)
			x1_grid, x2_grid = np.meshgrid(x1_range, x2_range)
			y_grid = model.intercept_ + model.coef_[0] * x1_grid + model.coef_[1] * x2_grid
			ax.plot_surface(x1_grid, x2_grid, y_grid, alpha=0.5, color='red', label='Máº·t pháº³ng há»“i quy')
			
			# GÃ¡n nhÃ£n cho cÃ¡c trá»¥c
			ax.set_xlabel('X1')
			ax.set_ylabel('X2')
			ax.set_zlabel('Y')
			ax.set_title('Há»“i quy tuyáº¿n tÃ­nh Ä‘a biáº¿n')
			ax.legend()
			
			plt.show()
		
		
		
		### Giáº£i thÃ­ch vá» cÃ¡c biáº¿n 
		
		- X : ÄÃ¢y lÃ  dá»¯ liá»‡u Ä‘áº§u vÃ o biáº¿n Ä‘á»™c láº­p, cÃ³ 4 hÃ ng vÃ  2 cá»™t : 
			- Má»—i hÃ ng Ä‘áº¡i diá»‡n cho má»™t quan sÃ¡t, vÃ  má»—i cá»™t lÃ  má»™t biáº¿n Ä‘á»™c láº­p(feature)
			
		
		
		- y : ÄÃ¢y lÃ  dá»¯ liá»‡u Ä‘áº§u ra(Biáº¿n phá»¥ thuá»™c), Ä‘Æ°á»£c tÃ­nh báº±ng cÃ´ng thá»©c  y = X .[1,2] + 3
			
		-  np.dot(X , np.array([1,2])) thá»±c hiá»‡n phÃ©p ma tráº­n giá»¯a X vÃ  [1,2]
		 
		-  Sau Ä‘Ã³ cá»™ng thÃªm 3 vÃ o tá»«ng pháº§n tá»­, káº¿t quáº£ lÃ  : 
					y = [6,8,10,13]
		 
	
		- model   = LinearRegression()  : Khá»Ÿi táº¡o má»™t mÃ´ hÃ¬nh há»“i quy tuyáº¿n tÃ­nh 
		
		- model.fit(X , y): Huáº¥n luyá»‡n mÃ´ hÃ¬nh trÃªn dá»¯ liá»‡u X vÃ  y. 
			+ MÃ´ hÃ¬nh há»c cÃ¡ch tÃ¬m ra cÃ¡c tham sá»‘ b0(intercept) vÃ  b1,b2(cÃ¡c há»‡ sá»‘ há»“i quy sao cho):  y = b0 + b1.x1 + b2.x2 
			+ Trong trÆ°á»ng há»£p nÃ y, mÃ´ hÃ¬nh sáº½ tÃ¬m ra b0 = 3 , b1 = 1 vÃ  b2 = 2  
			
		- y_pred = model.predict(X) : TÃ­nh toÃ¡n giÃ¡ trá»‹ y dá»± Ä‘oÃ¡n(giÃ¡ trá»‹ mÃ´ hÃ¬nh Æ°á»›c lÆ°á»£ng) tá»« dá»¯ liá»‡u Ä‘áº§u vÃ o X 
			+ Káº¿t quáº£ : Ypred = [6,8,10,13]
			
		
		- Máº·t pháº³ng há»“i quy cho tháº¥y mÃ´ hÃ¬nh cá»§a báº¡n Ä‘Ã£ há»c Ä‘Æ°á»£t quy luáº­t : Khi x1 hoáº·c x2 tÄƒng, y cÅ©ng tÄƒng theo má»™t tá»· lá»‡ nháº¥t Ä‘á»‹nh. 
		

#. 5 Há»“i quy tuyáº¿n tÃ­nh sá»­ dá»¥ng PyTorch 
	-> Náº¿u báº¡n muá»‘n há»“i quy tuyáº¿n tÃ­nh trong mÃ´i trÆ°á»ng há»c mÃ¡y nÃ¢ng cao, báº¡n cÃ³ thá»ƒ sá»­ dá»¥ng PyTorch 
	
#5.1 Há»“i quy tuyáº¿n tÃ­nh Ä‘Æ¡n giáº£n vá»›i PyTorch 
	
		import torch
		import torch.nn as nn
		import torch.optim as optim
		import matplotlib.pyplot as plt
		
		# Dá»¯ liá»‡u máº«u
		X = torch.tensor([[1.0], [2.0], [3.0], [4.0]])
		y = torch.tensor([[2.0], [4.0], [6.0], [8.0]])
		
		# MÃ´ hÃ¬nh há»“i quy tuyáº¿n tÃ­nh
		model = nn.Linear(1, 1)  # 1 Ä‘áº§u vÃ o, 1 Ä‘áº§u ra
		
		# HÃ m máº¥t mÃ¡t vÃ  tá»‘i Æ°u hÃ³a
		criterion = nn.MSELoss()  # Sai sá»‘ bÃ¬nh phÆ°Æ¡ng trung bÃ¬nh
		optimizer = optim.SGD(model.parameters(), lr=0.01)  # Gradient Descent
		
		# Huáº¥n luyá»‡n mÃ´ hÃ¬nh
		losses = []  # Danh sÃ¡ch lÆ°u giÃ¡ trá»‹ loss Ä‘á»ƒ váº½ biá»ƒu Ä‘á»“
		
		for epoch in range(1000):  # Huáº¥n luyá»‡n trong 1000 epoch
			model.train()
		
			# Dá»± Ä‘oÃ¡n
			y_pred = model(X)
		
			# TÃ­nh toÃ¡n máº¥t mÃ¡t
			loss = criterion(y_pred, y)
			losses.append(loss.item())  # LÆ°u giÃ¡ trá»‹ loss
		
			# Tá»‘i Æ°u hÃ³a
			optimizer.zero_grad()  # XÃ³a gradient cÅ©
			loss.backward()  # TÃ­nh toÃ¡n gradient
			optimizer.step()  # Cáº­p nháº­t trá»ng sá»‘
		
			# In thÃ´ng tin má»—i 100 epoch
			if (epoch + 1) % 100 == 0:
				print(f'Epoch: {epoch + 1}, Loss: {loss.item()}')
		
		# Há»‡ sá»‘ há»“i quy vÃ  intercept
		print(f'Há»‡ sá»‘ há»“i quy (slope): {model.weight.item()}')
		print(f'Intercept (Ä‘á»™ dá»i): {model.bias.item()}')
		
		# Trá»±c quan hÃ³a dá»¯ liá»‡u vÃ  Ä‘Æ°á»ng há»“i quy
		plt.figure(figsize=(10, 5))
		
		# Váº½ dá»¯ liá»‡u gá»‘c
		plt.scatter(X.numpy(), y.numpy(), color='blue', label='Dá»¯ liá»‡u gá»‘c')
		
		# Váº½ Ä‘Æ°á»ng há»“i quy
		x_line = torch.linspace(0, 5, 100).reshape(-1, 1)  # Táº¡o cÃ¡c Ä‘iá»ƒm x
		y_line = model(x_line).detach().numpy()  # Dá»± Ä‘oÃ¡n giÃ¡ trá»‹ y
		plt.plot(x_line.numpy(), y_line, color='red', label='ÄÆ°á»ng há»“i quy')
		
		plt.title('Há»“i quy tuyáº¿n tÃ­nh Ä‘Æ¡n giáº£n')
		plt.xlabel('X')
		plt.ylabel('y')
		plt.legend()
		plt.grid(True)
		plt.show()
		
		# Váº½ biá»ƒu Ä‘á»“ máº¥t mÃ¡t (loss) theo epoch
		plt.figure(figsize=(10, 5))
		plt.plot(range(1, 1001), losses, label='Loss')
		plt.title('GiÃ¡ trá»‹ Loss theo Epoch')
		plt.xlabel('Epoch')
		plt.ylabel('Loss')
		plt.grid(True)
		plt.legend()
		plt.show()


	

	# Giáº£i thÃ­ch code 
	1. Dá»¯ liá»‡u máº«u : 
		Táº¡o má»™t táº­p dá»¯ liá»‡u Ä‘áº§u vÃ o lÃ  cÃ¡c máº£ng Ä‘a chiá»u(tensor) mÃ´ phá»ng má»‘i quan há»‡ tuyáº¿n tÃ­nh y = 2x 
		
	2. Sá»­ dá»¥ng mÃ´ hÃ¬nh há»“i quy tuyáº¿n tÃ­nh : 
		Sá»­ dá»¥ng nn.Linear(1,1) Ä‘á»ƒ táº¡o má»™t mÃ´ hÃ¬nh há»“i quy tuyáº¿n tÃ­nh Ä‘Æ¡n giáº£n vá»›i : 
			1 Ä‘áº§u vÃ o(input feature)
			1 Ä‘áº§u ra (output feature)
	3. HÃ m máº¥t mÃ¡t : 
		nn.MSELoss() dÃ¹ng Ä‘á»ƒ tÃ­nh toÃ¡n sai sá»‘ bÃ¬nh phÆ°Æ¡ng trÃ¬nh bÃ¬nh MSE giá»¯a giÃ¡ trá»‹ dá»± Ä‘oÃ¡n vÃ  thá»±c táº¿. 
	
	4. Thuáº­t toÃ¡n tá»‘i Æ°u : 
		optim.SGD(Stochastic Gradient Descent ) Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ cáº­p nháº­t trá»ng sá»‘ dá»±a trÃªn gradient 
		
	5. VÃ²ng láº·p huáº¥n luyá»‡n : 
		
		### Trong má»—i epoch 
			- Dá»± Ä‘oÃ¡n giÃ¡ trá»‹ y_pred = model(X).
			- TÃ­nh toÃ¡n máº¥t mÃ¡t loss = criterion(y_pred , y)
			- Tá»‘i Æ°u hÃ³a trá»ng sá»‘ 
							optimizer.zero_grad()     //  XÃ³a gradient cÅ© 
							loss.backward()           // TÃ­nh gradient má»›i 
							optimizer.step()          // Cáº­p nháº­t trá»ng sá»‘ 
			
	6. Káº¿t quáº£ : 
		Sau khi huáº¥n luyá»‡n mÃ´ hÃ¬nh sáº½ tráº£ vá» há»‡ sá»‘ há»“i quy(slope) vÃ  intercept Ä‘á»™ dá»i: 


# 5.1 Há»“i quy tuyáº¿n tÃ­nh Ä‘a biáº¿n vá»›i PyTorch
	
			import torch 
			import torch.nn as nn
			import torch.optim as optim 
			import matplotlib.pyplot as plt 
			
			#Dá»¯ liá»‡u máº«u 
			# Giáº£ sá»­ cÃ³ 2 biáº¿n Ä‘á»™c láº­p vÃ  má»™t biáº¿n phá»¥ thuojc 
			X = torch.tensor([ [1.0 , 2.0] , [2.0,3.0] , [3.0,4.0] , [4.0,5.0] ]) 
			y = torch.tensor( [ [3.0] , [5.0] , [7.0] , [9.0] ])
			
			# MÃ´ hÃ¬nh há»“i quy tuyáº¿n tÃ­nh Ä‘a biáº¿n 
			model = nn.Linear(2,1)
			
			# HÃ m máº¥t mÃ¡t vÃ  tá»‘i Æ°u hÃ³a 
			criterion = nn.MSELoss()
			optimizer = optim.SGD(model.parameters() , lr = 0.01)
			
			#LÆ°u trá»¯ giÃ¡ trá»‹ máº¥t mÃ¡t Ä‘á»ƒ váº½ Ä‘á»“ thá»‹ 
			losses = []
			
			#Huáº¥n luyá»‡n mÃ´ hÃ¬nh 
			for epoch in range(1000):
				model.train()
			
				#Dá»± Ä‘oÃ¡n 
				y_pred = model(X)
				#TÃ­nh toÃ¡n máº¥t mÃ¡t 
				loss = criterion(y_pred , y)
				losses.append(loss.item())
				# Tá»‘i Æ°u hÃ³a
				optimizer.zero_grad()
				loss.backward()
				optimizer.step()
			
				if(epoch + 1) % 100 == 0:
					print(f'Epoch {epoch+1}, Loss: {loss.item()}')
			
			#Há»‡ sá»‘ há»“i quy vÃ  intercept
			print(f'Há»‡ sá»‘ há»“i quy: {model.weight.data}')
			print(f'Intercept: {model.bias.data}')
			
			# Váº½ Ä‘á»“ thá»‹ máº¥t mÃ¡t qua cÃ¡c epoch (log scale cho trá»¥c Y)
			plt.plot(losses)
			plt.title('QuÃ¡ trÃ¬nh giáº£m máº¥t mÃ¡t (Log Scale)')
			plt.xlabel('Epoch')
			plt.ylabel('Loss')
			plt.yscale('log')  # Chuyá»ƒn trá»¥c Y sang dáº¡ng logarit
			plt.grid(True, which="both", linestyle="--", linewidth=0.5)
			plt.show()
			#Kiá»ƒm tra dá»± Ä‘oÃ¡n 
			with torch.no_grad():
				y_test = model(X)
				print(f'Dá»± Ä‘oÃ¡n: {y_test}')


		### Giáº£i thÃ­ch : 
			- Sá»­ dá»¥ng torch.tensor Ä‘á»ƒ táº¡o dá»¯ liá»‡u Ä‘áº§u vÃ o X vÃ  Ä‘áº§u ra y 
			- MÃ´ hÃ¬nh há»“i quy tuyáº¿n tÃ­nh Ä‘a biáº¿n Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a vá»›i 2 biáº¿n Ä‘áº§u vÃ o vÃ  1 biáº¿n Ä‘áº§u ra báº±ng cÃ¡ch sá»­ dá»¥ng lá»›p nn.Linear 
			- ChÃºng ta sá»­ dá»¥ng hÃ m máº¥t mÃ¡t nn.MSELoss Ä‘á»ƒ tÃ­nh toÃ¡n sai sá»‘ bÃ¬nh phÆ°Æ¡ng trung bÃ¬nh vÃ  tá»‘i Æ°u hÃ³a mÃ´ hÃ¬nh báº±ng optim.SGD 
			- ChÃºng ta huáº¥n luyá»‡n mÃ´ hÃ¬nh qua nhiá»u epoch vÃ  in ra cÃ¡c há»‡ sá»‘ há»“i quy vÃ  intercept sau khi huáº¥n luyá»‡n 


#6. á»¨ng dá»¥ng cá»§a há»“i quy tuyáº¿n tÃ­nh trong há»c mÃ¡y 
Há»“i quy tuyáº¿n tÃ­nh cÃ³ ráº¥t nhiá»u á»©ng dá»¥ng trong cÃ¡c lÄ©nh vá»±c khÃ¡c nhau, tá»« kinh táº¿, tÃ i chÃ­nh Ä‘áº¿n y há»c, ká»¹ thuáº­t vÃ  hÆ¡n tháº¿ ná»¯a. DÆ°á»›i Ä‘Ã¢y lÃ  má»™t sá»‘ vÃ­ dá»¥ Ä‘iá»ƒn hÃ¬nh : 


	##6.1 Dá»± Ä‘oÃ¡n giÃ¡ nhÃ  
		Há»“i quy tuyáº¿n tÃ­nh cÃ³ thá»ƒ sá»­ dá»¥ng Ä‘á»ƒ dá»± Ä‘oÃ¡n giÃ¡ nhÃ  dá»±a trÃªn cÃ¡c yáº¿u tá»‘ nhÆ° diá»‡n tÃ­ch, sá»‘ phÃ²ng ngá»§,vÃ  vá»‹ trÃ­. Báº¡n cÃ³ thá»ƒ xÃ¢y dá»±ng mÃ´ hÃ¬nh há»“i quy tuyáº¿n tÃ­nh Ä‘á»ƒ dá»± Ä‘oÃ¡n giÃ¡ nhÃ  dá»±a trÃªn cÃ¡c Ä‘áº·c Ä‘iá»ƒm nÃ y 
		
	## 6.2  PhÃ¢n tÃ­ch má»—i quan há»‡ giá»¯a cÃ¡c biáº¿n kinh táº¿ 
		Trong kinh táº¿ há»c, há»“i quy tuyáº¿n tÃ­nh cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ phÃ¢n tÃ­ch má»‘i quan há»‡ giá»¯a cÃ¡c chá»‰ sá»‘ kinh táº¿ nhÆ° GDP, tá»· lá»‡ tháº¥t nghiá»‡p vÃ  láº¡m phÃ¡t. VÃ­ dá»¥, báº¡n cÃ³ thá»ƒ nghiÃªn cá»©u áº£nh hÆ°á»Ÿng cá»§a tá»· lá»‡ tháº¥t nghiá»‡p Ä‘áº¿n tÄƒng trÆ°á»Ÿng GDP 
		
	## 6.3 Dá»± Ä‘oÃ¡n nguy cÆ¡ bá»‡nh táº­t 
		Há»“i quy tuyáº¿n tÃ­nh cÅ©ng cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng trong y há»c Ä‘á»ƒ dá»± Ä‘oÃ¡n nguy cÆ¡ máº¯c bá»‡nh dá»±a trÃªn cÃ¡c chá»‰ sá»‘ sá»©c khá»e. VÃ­ dá»¥, mÃ´ hÃ¬nh há»“i quy tuyáº¿n tÃ­nh cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ phÃ¢n tÃ­ch má»‘i quan há»‡ giá»¯a huyáº¿t Ã¡p vÃ  nguy cÆ¡ máº¯c bá»‡nh tim máº¡ch 
		

# Káº¿t luáº­n  
	Há»“i quy tuyáº¿n tÃ­nh lÃ  má»™t ká»¹ thuáº­t cÆ¡ báº£n nhÆ°ng cá»±c ká»³ máº¡nh máº½ trong phÃ¢n tÃ­ch dá»¯ liá»‡u vÃ  há»c mÃ¡y. NÃ³ cung cáº¥p má»™t ná»n táº£ng quan trá»ng cho cÃ¡c mÃ´ hÃ¬nh phá»©c táº¡p hÆ¡n vÃ  cÃ³ nhiá»u á»©ng dá»¥ng thá»±c tiá»…n trong cÃ¡c lÄ©nh vá»±c khÃ¡c nhau. Viá»‡c hiá»ƒu rÃµ cÃ¡ch thá»©c hoáº¡t Ä‘á»™ng vÃ  á»©ng dá»¥ng cá»§a há»“i quy tuyáº¿n tÃ­nh khÃ´ng chá»‰ giÃºp báº¡n phÃ¢n tÃ­ch dá»¯ liá»‡u tá»‘t hÆ¡n mÃ  cÃ²n má»Ÿ ra nhiá»u cÆ¡ há»™i trong phÃ¡t triá»ƒn cÃ¡c mÃ´ hÃ¬nh há»c mÃ¡y hiá»‡u quáº£. 

	
		
// ============================= Biá»ƒu Ä‘á»“ máº¥t mÃ¡t (loss )================= 
Biá»ƒu Ä‘á»“ Loss theo Epoch cho tháº¥y quÃ¡ trÃ¬nh há»c táº­p cá»§a mÃ´ hÃ¬nh qua tá»«ng bÆ°á»›c (epoch). NÃ³ Ä‘o xem mÃ´ hÃ¬nh cá»§a báº¡n Ä‘ang há»c tá»‘t hÆ¡n hay tá»‡ hÆ¡n khi cá»‘ gáº¯ng giáº£m sai sá»‘ giá»¯a dá»± Ä‘oÃ¡n vÃ  thá»±c táº¿ 

# 1. # Ã nghÄ©a cá»§a Loss khi huáº¥n luyá»‡n 
	- Loss lÃ  cÃ¡ch Ä‘o lÆ°á»ng sai sá»‘ cá»§a mÃ´ hÃ¬nh : 
		+ Loss cao : MÃ´ hÃ¬nh dá»± Ä‘oÃ¡n khÃ´ng tá»‘t(xa thá»±c táº¿)
		+ Loss tháº¥p : MÃ´ hÃ¬nh dá»± Ä‘oÃ¡n tá»‘t hÆ¡n(gáº§n thá»±c táº¿).
		
	- Trong huáº¥n luyá»‡n, má»¥c tiÃªu cá»§a mÃ´ hÃ¬nh lÃ  giáº£m Loss dáº§n qua cÃ¡c epoch báº±ng cÃ¡ch cáº£i thá»‡n cÃ¡c tham sá»‘ bÃªn trong(trá»ng sá»‘ vÃ  Ä‘á»™ dá»i) 
	
# 2. # VÃ­ dá»¥ minh há»a dá»… hiá»ƒu 
	- Khi dáº¡y má»™t Ä‘á»©a tráº» nÃ©m bÃ³ng trÃºng rá»• 
		+ Ban Ä‘áº§u(epoch Ä‘áº§u tiÃªn) : Tráº» nÃ©m bÃ¡o xa rá»• , sai ráº¥t nhiá»u -> Loss cao 
		+ Trong quÃ¡ trÃ¬nh luyá»‡n táº­p :  Tráº» báº¯t Ä‘áº§u Ä‘iá»u chá»‰nh ká»¹ thuáº­t: dÃ¹ng lá»±c Ä‘Ãºng hÆ¡n, nháº¯m chÃ­nh xÃ¡c hÆ¡n
		+ Sau má»—i láº§n táº­p, sai sá»‘(khoáº£ng cÃ¡ch bÃ³ng Ä‘áº¿n rá»• )giáº£m dáº§n -> Loss nhá» dáº§n 
		+ Káº¿t quáº£(epoch cuá»‘i): Tráº» nÃ©m bÃ³ng vÃ o rá»• chÃ­nh xÃ¡c -> Loss gáº§n báº±ng 0 
		
	- Biá»ƒu Ä‘á»“ :  
		+ Náº¿u tráº» há»c tá»‘t, biá»ƒu Ä‘á»“ sáº½ giáº£m dáº§n Ä‘á»u 
		+ Náº¿u tráº» há»c khÃ´ng tá»‘t hoáº·c gáº·p váº¥n Ä‘á», biá»ƒu Ä‘á»“ cÃ³ thá»ƒ pháº³ng, tÄƒng lÃªn, khÃ´ng giáº£m  
		

# 3 . PhÃ¢n tÃ­ch biá»ƒu Ä‘á»“ Loss trong mÃ´ hÃ¬nh AI 
	## 1.  Biá»ƒu Ä‘á»“ Loss giáº£m dáº§n : 
		+ ÄÃ¢y lÃ  dáº¥u hiá»‡u tá»‘t. NÃ³ cho tháº¥y mÃ´ hÃ¬nh Ä‘ang há»c cÃ¡ch dá»± Ä‘oÃ¡n ngÃ y cÃ ng chÃ­nh xÃ¡c hÆ¡n 
				Epoch 1: Loss = 10.0
				Epoch 100: Loss = 1.5
				Epoch 1000: Loss = 0.01
		
	## 2. Loss khÃ´ng giáº£m hoáº·c tÄƒng lÃªn : 
		+ CÃ³ thá»ƒ mÃ´ hÃ¬nh gáº·p váº¥n Ä‘á» : 
			+ Tá»‘c Ä‘á»™ há»c(learning rate) quÃ¡ lá»›n hoáº·c quÃ¡ nhá» 
			+ Dá»¯ liá»‡u khÃ´ng Ä‘á»§ tá»‘t hoáº·c khÃ´ng phÃ¹ há»£p 
			+ MÃ´ hÃ¬nh quÃ¡ Ä‘Æ¡n giáº£n (underfitting) hoáº·c quÃ¡ phá»©c táº¡p(overfitting)
			
	## 3. Loss giáº£m máº¡nh rá»“i chá»¯ng láº¡i : 
		+ ÄÃ¢y lÃ  Ä‘iá»u bÃ¬nh thÆ°á»ng. Sau khi há»c Ä‘Æ°á»£c nhiá»u, mÃ´ hÃ¬nh cáº§n nhiá»u ná»— lá»±c hÆ¡n Ä‘á»ƒ cáº£i thiá»‡n tiáº¿p. 
			
			
# 4. VÃ­ dá»¥ báº±ng hÃ¬nh áº£nh 
	
	+ Trá»¥c X (epoch) : Sá»‘ láº§n mÃ´ hÃ¬nh há»c(epoch )
	+ Trá»¥c Y (loss) : Sai sá»‘ táº¡i má»—i epoch 


	1. Biá»ƒu Ä‘á»“ tá»‘t : 
		+ Giáº£m dáº§n Ä‘á»u : MÃ´ hÃ¬nh Ä‘ang há»c tá»‘t, ngÃ y cÃ ng chÃ­nh xÃ¡c hÆ¡n 
	2. Biá»ƒu Ä‘á»“ khÃ´ng giáº£m : 
		+ Khoogn thay Ä‘á»•i : CÃ³ lá»—i hoáº·c mÃ´ hÃ¬nh khÃ´ng há»c Ä‘Æ°á»£c 
		
	3. Loss dao Ä‘á»™ng : 
		+ Dao Ä‘á»™ng tháº¥t thÆ°á»ng : CÃ³ thá»ƒ do tá»‘c Ä‘á»™ há»c quÃ¡ lá»›n hoáº·c dá»¯ liá»‡u khÃ´ng á»•n Ä‘á»‹nh 
		

//====================== K-Means Clustering =============================// 
#1 Giá»›i thiá»‡u : 
	K-Means Clustering lÃ  má»™t thuáº­n toÃ¡n phÃ¢n cá»¥m phá»• biáº¿n, thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng trong cÃ¡c á»©ng dá»¥ng há»c mÃ¡y (Machine Learning) vÃ  khai thÃ¡c dá»¯ liá»‡u(Data Mining). Má»¥c tiÃªu chÃ­nh cá»§a K-Means lÃ  chi má»™t táº­p há»£p vá»›i cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u k cá»¥m(clusters) sao cho cÃ¡c Ä‘iá»ƒm trong cÃ¹ng má»™t cá»¥m cÃ³ sá»± tÆ°Æ¡ng Ä‘á»“ng cao nháº¥t vá»›i nhau vÃ  khÃ¡c biá»‡t tá»‘i Ä‘a vá»›i cÃ¡c cá»¥m khÃ¡c. 
	
	Trong thuáº­t toÃ¡n K-Means, má»—i cá»¥m Ä‘áº¡i diá»‡n bá»Ÿi má»™t centroid, lÃ  giÃ¡ trá»‹ trung bÃ¬nh cá»§a cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u trong cá»¥m Ä‘Ã³. Thuáº­t toÃ¡n sáº½ liÃªn tá»¥c Ä‘iá»u chá»‰nh cÃ¡c centroid vÃ  táº¡i phÃ¢n cá»¥m cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u cho Ä‘áº¿n khi Ä‘áº¡t Ä‘Æ°á»£c sá»± há»™i tá»¥(convergence).
	https://aicandy.vn/wp-content/uploads/2024/09/aicandy_kmeans_arch.jpg
	
#2 CÃ¡ch hoáº¡t Ä‘á»™ng 
	K-Means Clustering lÃ  má»™t thuáº­t toÃ¡n phÃ¢n cá»¥m khÃ´ng giÃ¡m sÃ¡t phá»• biáº¿n, dÃ¹ng Ä‘á»ƒ nhÃ³m cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u thÃ nh K cá»¥m dá»±a trÃªn cÃ¡c Ä‘áº·c tÃ­nh tÆ°Æ¡ng tá»±. Thuáº­t toÃ¡n nÃ y hoáº¡t Ä‘á»™ng dá»±a trÃªn Ã½ tÆ°á»Ÿng lÃ  giáº£m thiá»ƒu tá»•ng khoáº£ng cÃ¡ch bÃ¬nh phÆ°Æ¡ng giá»¯a cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u vÃ  trung tÃ¢m cá»§a cá»¥m chÃºng thuá»™c vá». 

	## 2.1 Khá»Ÿi táº¡o cÃ¡c trung tÃ¢m cá»¥m(Centroids )
		BÆ°á»›c Ä‘áº§u tiÃªn cá»§a K-Means lÃ  xÃ¡c Ä‘á»‹nh sá»‘ lÆ°á»£ng cá»¥m K mÃ  báº¡n muá»‘n tÃ¬m. Sau Ä‘Ã³, chá»n ngáº«u nhiÃªn K Ä‘iá»ƒm tá»« táº­p dá»¯ liá»‡u lÃ m cÃ¡c trung tÃ¢m cá»¥m ban Ä‘áº§u, Ä‘Æ°á»£c gá»i lÃ  cÃ¡c centroid 
		
	## 2.2 GÃ¡n má»—i Ä‘iá»ƒm dá»¯ liá»‡u vÃ o cá»¥m gáº§n nháº¥t 
		Má»—i Ä‘iá»ƒm dá»¯ liá»‡u Ä‘Æ°á»£c gÃ¡n vÃ o cá»¥m cÃ³ centroid gáº§n nháº¥t, Ä‘Æ°á»£c tÃ­nh báº±ng khoáº£ng cÃ¡ch Euclidean. Khoáº£ng cÃ¡ch giá»¯a má»™t Ä‘iá»ƒm dá»¯ liá»‡u x1 vÃ  má»™t centroid cj Ä‘Æ°á»£c tÃ­nh báº±ng cÃ´ng thá»©c  
				d(x_i, c_j) = \sqrt{\sum_{k=1}^{n}(x_{ik} â€“ c_{jk})^2}


		Trong Ä‘Ã³ : 
			x1 lÃ  Ä‘iá»ƒm dá»¯ liá»‡u thá»© i 
			cj lÃ  centroid cá»§a cá»¥m thá»© j 
			n lÃ  sá»‘ chiá»u cá»§a dá»¯ liá»‡u 
			
		VÃ­ dá»¥ : Giáº£ sá»­ ta cÃ³ má»™t táº­p dá»¯ liá»‡u gá»“m ba Ä‘iá»ƒm trong khÃ´ng gian 2 chiá»u(1,2) , (2,3) , (3,4) vÃ  báº¡n khá»Ÿi táº¡o hai centroid c1 = (1,1) vÃ  c2 = (4,4). Khoáº£ng cÃ¡ch Euclidean giá»¯a má»—i Ä‘iá»ƒm vÃ  centroid Ä‘Æ°á»£c tÃ­nh nhÆ° sau  
			d((1, 2), c_1) = \sqrt{(1-1)^2 + (2-1)^2} = 1
			d((1, 2), c_2) = \sqrt{(1-4)^2 + (2-4)^2} = \sqrt{13}

		VÃ¬ khoáº£ng cÃ¡ch tá»« Ä‘iá»ƒm (1,2) Ä‘áº¿n centroid c1 nhá» hÆ¡n, nÃªn Ä‘iá»ƒm nÃ y sáº½ Ä‘Æ°á»£c gÃ¡n vÃ o cá»¥m vá»›i centroid c1 
		
	## 2.3 Cáº­p nháº­t cÃ¡c trung tÃ¢m cá»¥m 
		Sau khi táº¥t cáº£ cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c gÃ¡n vÃ o má»™t cá»¥m, cÃ¡c centroid Ä‘Æ°á»£c tÃ­nh láº¡i báº±ng cÃ¡ch láº¥y trung bÃ¬nh cá»™ng cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u trong má»—i cá»¥m.
				c_j = \frac{1}{|C_j|} \sum_{x_i \in C_j} x_i
				
		Trong Ä‘Ã³ : 
			c_j lÃ  táº­p há»£p cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u trong cá»¥m i 
			|C_j| lÃ  sá»‘ lÆ°á»£ng Ä‘iá»ƒm dá»¯ liá»‡u trong cá»¥m i 
			
		### VÃ­ dá»¥ tiáº¿p theo : Giáº£ sá»­ sau khi gÃ¡n cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u, cá»¥m C1 bao gá»“m cÃ¡c Ä‘iá»ƒm(1,2) vÃ  (2,3) thÃ¬ centroid má»›i c1 sáº½ Ä‘Æ°á»£c tÃ­nh nhÆ° sau : 
			c_1 = \frac{1}{2} \left( (1, 2) + (2, 3) \right) = (1.5, 2.5)
			
			
	## 2.4 Láº·p láº¡i quÃ¡ trÃ¬nh 
		CÃ¡c bÆ°á»›c gÃ¡n cá»¥m vÃ  cáº­p nháº­t centroid Ä‘Æ°á»£c láº·p láº¡i cho Ä‘áº¿n khi cÃ¡c centroid khÃ´ng cÃ²n thay Ä‘á»•i Ä‘Ã¡ng ká»ƒ hoáº·c Ä‘áº¡t Ä‘áº¿n sá»‘ láº§n láº·p tá»‘i Ä‘a. Thuáº­t toÃ¡n há»™i tá»¥ khi khÃ´ng cÃ³ sá»± thay Ä‘á»•i trong gÃ¡n cá»¥m cá»§a cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u hoáº·c sá»± thay Ä‘á»•i ráº¥t nhá». 
		
	## 2.5 ÄÃ¡nh giÃ¡ cháº¥t lÆ°á»£ng phÃ¢n cá»¥m  
		Cháº¥t lÆ°á»£ng cá»§a viá»‡c phÃ¢n cá»¥m cÃ³ thá»ƒ Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ báº±ng tá»•ng bÃ¬nh phÆ°Æ¡ng sai sá»‘ trong cá»¥m(Within-Cluster Sum of Squared - WCSSS ) Ä‘Æ°á»£c tÃ­nh báº±ng cÃ´ng thá»©c 
			WCSS = \sum_{j=1}^{K} \sum_{x_i \in C_j} d(x_i, c_j)^2
		Thuáº­t toÃ¡n cá»‘ gáº¯ng giáº£m thiá»ƒu giÃ¡ trá»‹ WCSS Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c sá»± phÃ¢n cá»¥m tá»‘t nháº¥t. 
			
		### VÃ­ dá»¥ minh há»a 
			0> Giáº£ sá»­ ta cÃ³ dá»¯ liá»‡u sau Ä‘Ã¢y: 
				Äiá»ƒm dá»¯ liá»‡u : (1, 1), (2, 1), (4, 3), (5, 4)
				Khá»Ÿi táº¡o K = 2 vá»›i cÃ¡c centroid ban Ä‘áº§u c1 = (1,1) vÃ  c2 = (5,4)
				
			1> GÃ¡n cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u vÃ o cá»¥m
				Äiá»ƒm (1,1) vÃ  (2,1) cáº§n centroid c1 hÆ¡n nÃªn thuá»™c cá»¥m 1
				Äiá»ƒm(4,3) vÃ  (5,4) gáº§n centroid c2 hÆ¡n, nÃªn thuá»™c cá»¥m 2.
				
			2> Cáº­p nháº­t cÃ¡c centroid 
				Centroid má»›i cá»§a cá»¥m 1 lÃ  c1 = (1,5 , 1)
				Centroid má»›i cá»§a cá»¥m 2 lÃ  c2 = (4.5 , 3.5)
				
			3> Láº·p láº¡i bÆ°á»›c 1 vÃ  bÆ°á»›c 2 cho Ä‘áº¿n khi cÃ¡c centroid khÃ´ng thay Ä‘á»•i. 

# 3 á»¨ng dá»¥ng 			
	
	K-means Clustering Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i trong nhiá»u lÄ©nh vá»±c khÃ¡c nhau, bao gá»“m :
	
	## PhÃ¢n tÃ­ch khÃ¡ch hÃ ng  
	K-Means cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ phÃ¢n nhÃ³m khÃ¡ch hÃ ng dá»±a trÃªn cÃ¡c hÃ nh vi mua sáº¯m, tá»« Ä‘Ã³ giÃºp doanh nghiá»‡p hiá»ƒu rÃµ hÆ¡n vá» cÃ¡c phÃ¢n khÃºc khÃ¡ch hÃ ng khÃ¡c nhau. 
	
    
	## PhÃ¢n tÃ­ch hÃ¬nh áº£nh 
		Trong xá»­ lÃ½ hÃ¬nh áº£nh, K-Means cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ phÃ¢n cá»¥m cÃ¡c Ä‘iá»ƒm cÃ³ mÃ u sáº¯c tÆ°Æ¡ng Ä‘á»“ng, giÃºp trong viá»‡c nÃ©n áº£nh hoáº·c phÃ¢n Ä‘oáº¡n áº£nh. 
		
	## PhÃ¢n tÃ­ch vÄƒn báº£n 
		K-Means cÃ³ thá»ƒ phÃ¢n nhÃ³m cÃ¡c tÃ i liá»‡u hoáº·c tá»« ngá»¯ dá»±a trÃªn cÃ¡c Ä‘áº·c trÆ°ng chung, giÃºp tá»• chá»©c dá»¯ liá»‡u vÄƒn báº£n hoáº·c tÃ¬m kiáº¿m thÃ´ng tin.  
	
# 4 Æ¯u Ä‘iá»ƒm, nhÆ°á»£c Ä‘iá»ƒm cá»§a K-Means Clustering  
	## 4.1 Æ¯u Ä‘iá»ƒm cá»§a K-Means Clustering 
		### ÄÆ¡n giáº£n dá»… hiá»ƒu 
			K-Means lÃ  má»™t trong nhá»¯ng thuáº­t toÃ¡n phÃ¢n cá»¥m dá»… hiá»ƒu nháº¥t, vá»›i quy trÃ¬nh hoáº¡t Ä‘á»™ng rÃµ rÃ ng vÃ  dá»… triá»ƒn khai 
		
		### Hiá»‡u quáº£ 
			Thuáº­t toÃ¡n nÃ y cÃ³ thá»ƒ xá»­ lÃ½ má»™t lÆ°á»£ng lá»›n dá»¯ liá»‡u vá»›i chi phÃ­ tÃ­nh toÃ¡n tháº¥p, Ä‘áº·c biá»‡t lÃ  khi sá»‘ lÆ°á»£ng cá»¥m k nhá». 
			
		### Kháº£ nÄƒng má»Ÿ rá»™ng 
			K-Means cÃ³ thá»ƒ má»Ÿ rá»™ng tá»‘t vá»›i dá»¯ liá»‡u lá»›n, nhá» vÃ o tÃ­nh cháº¥t tuyáº¿n tÃ­nh cá»§a nÃ³. NÃ³ cÃ³ thá»ƒ Ä‘Æ°á»£c Ã¡p dá»¥ng trÃªn cÃ¡c táº­p dá»¯ liá»‡u lá»›n vá»›i hÃ ng triá»‡u Ä‘iá»ƒm dá»¯ liá»‡u. 
		
		### Linh hoáº¡t 
			CÃ³ thá»ƒ Ã¡p dá»¥ng K-Means cho nhiá»u láº¡i dá»¯ liá»‡u khÃ¡c nhau, bao gá»“m cáº£ dá»¯ liá»‡u sá»‘ vÃ  dá»¯ liá»‡u danh má»¥c(categorical ) 
	
	## 4.2 NhÆ°á»£c Ä‘iá»ƒm : 
		### Sá»‘ cá»¥m pháº£i Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh trÆ°á»›c  
			Má»™t trong nhá»¯ng háº¡n cháº¿ lá»›n nháº¥t cá»§a K-Means lÃ  yÃªu cáº§u ngÆ°á»i dÃ¹ng pháº£i xÃ¡c Ä‘á»‹nh sá»‘ lÆ°á»£ng cá»¥m k trÆ°á»›c khi cháº¡y thuáº­t toÃ¡n. Äiá»u nÃ y cÃ³ thá»ƒ khÃ³ khÄƒn khi khÃ´ng biáº¿t trÆ°á»›c sá»‘ lÆ°á»£ng cá»¥m tá»‘i Æ°u 
			
		### Nháº¡y cáº£m vá»›i vá»‹ trÃ­ khá»Ÿi táº¡o centroid 
			Káº¿t quáº£ cá»§a K-Means phá»¥ thuá»™c ráº¥t nhiá»u vÃ o viá»‡c khá»Ÿi táº¡o centroid ban Ä‘áº§u. Khá»Ÿi táº¡o kÃ©m cÃ³ thá»ƒ dáº«n Ä‘áº¿n há»™i tá»¥ táº¡i má»™t cá»±c trá»‹ cá»¥c bá»™ khÃ´ng tá»‘i Æ°u 
		
		### Chá»‰ nháº­n diá»‡n cÃ¡c cá»¥m hÃ¬nh cáº§u  
			K-Means hoáº¡t Ä‘á»™t tá»‘t vá»›i cÃ¡c cá»¥m cÃ³ hÃ¬nh dáº¡ng gáº§n nhÆ° cáº§u vÃ  kÃ­ch thÆ°á»›c tÆ°Æ¡ng Ä‘á»“ng. NÃ³ khÃ´ng thá»ƒ xá»­ lÃ½ tá»‘t cÃ¡c cá»¥m cÃ³ hÃ¬nh dáº¡ng phi tuyáº¿n hoáº·c khÃ´ng Ä‘á»“ng Ä‘á»u. 
			
		### Nháº¡y cáº£m vá»›i nhiá»…u  
			K-Means ráº¥t nháº¡y cáº£m vá»›i cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u ngoáº¡i lai(outliers), vÃ¬ cÃ¡c outliers cÃ³ thá»ƒ kÃ©o dÃ i centroid ra khá»i vá»‹ trÃ­ trung tÃ¢m cá»§a cá»¥m thá»±c sá»±. 
			
	## 4.3 Khi nÃ o nÃªn sá»­ dá»¥ng K-Means Clustering? 
		K-Means Clustering lÃ  má»™t lá»±a chá»n tá»‘t trong cÃ¡c trÆ°á»ng há»£p sau : 
		
		### Dá»¯ liá»‡u cÃ³ cáº¥u trÃºc Ä‘Æ¡n giáº£n 
			Khi dá»¯ liá»‡u cÃ³ cáº¥u trÃºc phÃ¢n cá»¥m Ä‘Æ¡n giáº£n vá»›i cÃ¡c cá»¥m hÃ¬nh cáº§u hoáº·c gáº§n hÃ¬nh cáº§u, K-Means lÃ  má»™t cÃ´ng cá»¥ máº¡nh máº½.
			
		### Sá»‘ lÆ°á»£ng cá»¥m Ä‘Æ°á»£c biáº¿t trÆ°á»›c 
			Khi báº¡n Ä‘Ã£ biáº¿t hoáº·c cÃ³ thá»ƒ Æ°á»›c lÆ°á»£ng chÃ­nh xÃ¡c sá»‘ cá»¥m k, K-Means cÃ³ thá»ƒ nhanh chÃ³ng phÃ¢n nhÃ³m dá»¯ liá»‡u. 
			
		### PhÃ¢n tÃ­ch dá»¯ liá»‡u sÆ¡ bá»™ 
			Khi cáº§n phÃ¢n tÃ­ch dá»¯ liá»‡u sÆ¡ bá»™ Ä‘á»ƒ khÃ¡m phÃ¡ cÃ¡c máº«u áº©n, K-Means cÃ³ thá»ƒ cung cáº¥p cÃ¡i nhÃ¬n nhanh chÃ³ng vÃ  rÃµ rÃ ng vá» cáº¥u trÃºc cá»§a dá»¯ liá»‡u. 
			
		### YÃªu cáº§u thá»i gian tÃ­nh toÃ¡n tháº¥p 
			Khi thá»i gian vÃ  tÃ i nguyÃªn háº¡n cháº¿, K-Means cung cáº¥p giáº£i phÃ¡p nhanh chÃ³ng vá»›i hiá»‡u suáº¥t tÃ­nh toÃ¡n cao. 
			
# 5 VÃ­ dá»¥ K-Means Clustering trÃªn Python  

	## 5.1 K-Means Clustering vá»›i scikit-learn 
		... files 
		
	Trong vÃ­ dá»¥ nÃ y, chÃºng ta táº¡o má»™t táº­p dá»¯ liá»‡u ngáº«u nhiÃªn vÃ  sá»­ dá»¥ng thuáº­t toÃ¡n K-Means Ä‘á»ƒ phÃ¢n cá»¥m dá»¯ liá»‡u thÃ nh 3 cá»¥m. Káº¿t quáº£ Ä‘Æ°á»£c trá»±c quan hÃ¡o báº±ng biá»ƒu Ä‘á»“ vá»›i cÃ¡c centroid cá»§a má»—i cá»¥m Ä‘Æ°á»£c hiá»ƒn thá»‹ dÆ°á»›i dáº¡ng cÃ¡c Ä‘iá»ƒm mÃ u Ä‘á»  



	## 5.2 K-Means Clustering vá»›i scikit-learn PyTorch 
		... files 
		
	Trong vÃ­ dá»¥ nÃ y, chÃºng ta triá»ƒn khai thuáº­t toÃ¡n K-Means tá»« Ä‘áº§u báº±ng PyTorch. ChÃºng ta thá»±c hiá»‡n viá»‡c khá»Ÿi táº¡o centroid ngáº«u nhiÃªn, tÃ­nh toÃ¡n khoáº£ng cÃ¡ch, gÃ¡n nhÃ£n cho cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u vÃ  cáº­p nháº­t cÃ¡c centroid theo phÆ°Æ¡ng phÃ¡p láº·p 
	
# Káº¿t luáº­n 
	K-Means Clustering lÃ  má»™t thuáº­t toÃ¡n máº¡nh máº½ vÃ  linh hoáº¡t trong phÃ¢n cá»¥m dá»¯ liá»‡u, Ä‘áº·c biá»‡t lÃ  trong cÃ¡c trÆ°á»ng há»£p dá»¯ liá»‡u cÃ³ cáº¥u trÃºc Ä‘Æ¡n giáº£n. Tuy nhiÃªn, ngÆ°á»i dÃ¹ng cáº§n lÆ°u Ã½ cÃ¡c nhÆ°á»£c Ä‘iá»ƒm cá»§a thuáº­t toÃ¡n, nhÆ° sá»± nháº¡y cáº£m vá»›i viá»‡c khá»Ÿi táº¡o centroid vÃ  kháº£ nÄƒng hoáº¡t Ä‘á»™ng kÃ©m vá»›i cÃ¡c cá»¥m phá»©c táº¡p. Viá»‡c hiá»ƒu rÃµ khi nÃ o nÃªn sá»­ dá»¥ng K-Means vÃ  cÃ¡ch cÃ i Ä‘áº·t nÃ³ báº±ng Python hoáº·c PyTorch sáº½ giÃºp báº¡n Ã¡p dá»¥ng thuáº­t toÃ¡n nÃ y má»™t cÃ¡ch hiá»‡u quáº£ trong cÃ¡c bÃ i toÃ¡n há»c mÃ¡y cá»§a mÃ¬nh  




//================================ K-nearest neighbors cho phÃ¢n loáº¡i vÃ  há»“i quy ====================

# KhÃ¡i niá»‡m 
	K-nearest neightbors(KNN) lÃ  má»™t trong nhá»¯ng thuáº­t toÃ¡n há»c mÃ¡y cÆ¡ báº£n nháº¥t nhÆ°ng vÃ´ cÃ¹ng máº¡nh máº½ trong cáº£ bÃ i toÃ¡n phÃ¢n loáº¡i vÃ  há»“i quy. 
	
	KNN lÃ  má»™t thuáº­t toÃ¡n há»c mÃ¡y khÃ´ng tham sá»‘(non-parametric) vÃ  há»c giÃ¡m sÃ¡t(supervised learning). Ã tÆ°á»Ÿng chÃ­nh lÃ  tÃ¬m kiáº¿m k Ä‘iá»ƒm dá»¯ liá»‡u gáº§n nháº¥t trong táº­p huáº¥n luyá»‡n, sau Ä‘Ã³ sá»­ dá»¥ng chÃºng Ä‘á»ƒ dá»± Ä‘oÃ¡n nhÃ£n hoáº·c giÃ¡ trá»‹ cá»§a Ä‘iá»ƒm dá»¯ liá»‡u má»›i. 
		https://aicandy.vn/wp-content/uploads/2024/11/aicandy_knn_2-1.jpg
		
# NguyÃªn lÃ½ hoáº¡t Ä‘á»™ng cá»§a KNN 
	## CÃ¡c bÆ°á»›c thá»±c hiá»‡n  
		
		
		1. Chá»n giÃ¡ trá»‹ cá»§a k 
			k lÃ  sá»‘ lÆ°á»£ng hÃ ng xÃ³m gáº§n nháº¥t sáº½ Ä‘Æ°á»£c sá»­ dá»¥ng 
			
		2. TÃ­nh khoáº£ng cÃ¡ch 
			Khoáº£ng cÃ¡ch giá»¯a Ä‘iá»ƒm cáº§n dá»± Ä‘oÃ¡n vÃ  cÃ¡c Ä‘iá»ƒm trong táº­p huáº¥n luyá»‡n Ä‘Æ°á»£c tÃ­nh toÃ¡n. Khoáº£ng cÃ¡ch Euclidean lÃ  phá»• biáº¿n nháº¥t 
			
						d(x, y) = \sqrt{\sum_{i=1}^{n}(x_i â€“ y_i)^2}
						
		3. Chá»n k hÃ ng xÃ³m gáº§n nháº¥t 
			Chá»n ra k Ä‘iá»ƒm gáº§n nháº¥t tá»« táº­p huáº¥n luyá»‡n 
			
		4. Dá»± Ä‘oÃ¡n 
			PhÃ¢n loáº¡i : Ä‘Æ°a ra nhÃ£n cá»§a Ä‘iá»ƒm má»›i dá»±a trÃªn Ä‘a sá»‘ phiáº¿u tá»« k hÃ ng xÃ³m 
			
		5. Há»“i quy : Dá»± Ä‘oÃ¡n giÃ¡ trá»‹ liÃªn tá»¥c báº±ng cÃ¡ch trung bÃ¬nh(hoáº·c trung bÃ¬nh cÃ³ trá»ng sá»‘) cá»§a cÃ¡c giÃ¡ trá»‹ hÃ ng xÃ³m 
		
		
		
	## BÃ i toÃ¡n phÃ¢n loáº¡i 
		Trong bÃ i toÃ¡n phÃ¢n loáº¡i, KNN hoáº¡t Ä‘á»™ng dá»±a trÃªn  nguyÃªn lÃ½ Ä‘a sá»‘ phiáº¿u(majority voting). VÃ­ dá»¥, giáº£ sá»­ cÃ³ má»™t táº­p dá»¯ liá»‡u vá»›i hai lá»›p, vÃ  chÃºng ta muá»‘n phÃ¢n loáº¡i má»™t Ä‘iá»ƒm má»›i vá»›i k = 3 
		
		Giáº£ sá»­ ba hÃ ng xÃ³m gáº§n nháº¥t cÃ³ nhÃ£n láº§n lÆ°á»£t lÃ  {A,B,A}. Do lá»›p A xuáº¥t hiá»‡n nhiá»u hÆ¡n, nÃªn Ä‘iá»ƒm má»›i sáº½ Ä‘Æ°á»£c phÃ¢n vÃ o lá»›p A 
	
	## CÃ´ng thá»©c tÃ­nh Ä‘a sá»‘ phiáº¿u 
		CÃ´ng thá»©c tÃ­nh xÃ¡c suáº¥t cho P(A) cho má»™t Ä‘iá»ƒm má»›i thuá»™c lá»›p A : 
					P(A) = \frac{N_A}{k}
					
				Trong Ä‘Ã³, N_A lÃ  sá»‘ lÆ°á»£ng hÃ ng xÃ³m thuá»™c lá»›p A, k lÃ  tá»•ng sá»‘ hÃ ng xÃ³m(á»Ÿ Ä‘Ã¢y lÃ  3)
				
	## VÃ­ dá»¥ thá»±c táº¿ vá»›i PyTorch 
	
			file... 
			
			Trong vÃ­ dá»¥ nÃ y, chÃºng ta táº¡o má»™t táº­p dá»¯ liá»‡u giáº£ láº­p vá»›i hai lá»›p. Sau Ä‘Ã³, sá»­ dá»¥ng hÃ m knn_classification Ä‘á»ƒ dá»± Ä‘oÃ¡n nhÃ£n cá»§a cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u trong táº­p kiá»ƒm tra. Cuá»‘i cÃ¹ng, tÃ­nh Ä‘á»™ chÃ­nh xÃ¡c cá»§a mÃ´ hÃ¬nh.


# 2.4. BÃ i toÃ¡n há»“i quy
	Trong bÃ i toÃ¡n há»“i quy, KNN dá»± Ä‘oÃ¡n giÃ¡ trá»‹ liÃªn tá»¥c cho Ä‘iá»ƒm má»›i dá»±a trÃªn trung bÃ¬nh(hoáº·c trung bÃ¬nh cÃ³ trá»ng sá»‘)	 cá»§a cÃ¡c giÃ¡ trá»‹ thuá»™c hÃ ng xÃ³m gáº§n nháº¥t. 
	
	## CÃ´ng thá»©c tÃ­nh giÃ¡ trá»‹ dá»± Ä‘oÃ¡n 
		
		GIáº£ sá»­ giÃ¡ trá»‹ cáº§n dá»± Ä‘oÃ¡n lÃ  y , vÃ  y_1, y_2 , .... y_k lÃ  giÃ¡ trá»‹ cá»§a k hÃ ng xÃ³m gáº§n nháº¥t, giÃ¡ trá»‹ dá»± Ä‘oÃ¡n Ä‘Æ°á»£c tÃ­nh theo cÃ´ng thá»©c  
				\hat{y} = \frac{1}{k} \sum_{i=1}^{k} y_i
				
		Náº¿u sá»­ dá»¥ng trung bÃ¬nh trá»ng sá»‘, giÃ¡ trá»‹ dá»± Ä‘oÃ¡n sáº½ lÃ  : 
			 
				
	##VÃ­ dá»¥ thá»±c táº¿ vá»›i PyTorch
	DÆ°á»›i Ä‘Ã¢y lÃ  vÃ­ dá»¥ sá»­ dá»¥ng PyTorch Ä‘á»ƒ triá»ƒn khai KNN cho bÃ i toÃ¡n há»“i quy:
			
			
			files... 
			K-nearestNeightborsHoiQuyWithLibPyTorch.py 
			
			
			
VÃ­ dá»¥ nÃ y minh há»a cÃ¡ch sá»­ dá»¥ng KNN Ä‘á»ƒ thá»±c hiá»‡n há»“i quy trÃªn má»™t táº­p dá»¯ liá»‡u giáº£ láº­p. Káº¿t quáº£ dá»± Ä‘oÃ¡n Ä‘Æ°á»£c so sÃ¡nh vá»›i giÃ¡ trá»‹ thá»±c táº¿ báº±ng cÃ¡ch tÃ­nh toÃ¡n Mean Squared Error (MSE).


# 3. Chá»n giÃ¡ trá»‹ cá»§a k
	
	## Táº§m quan trá»ng cá»§a viá»‡c chá»n giÃ¡ trá»‹ k 
		Trong thuáº­t toÃ¡n KNN, giÃ¡ trá»‹ k (sá»‘ lÆ°á»£ng hÃ ng xÃ³m gáº§n nháº¥t) lÃ  má»™t tham sá»‘ quan trá»ng quyáº¿t Ä‘á»‹nh hiá»‡u suáº¥t cá»§a mÃ´ hÃ¬nh. Chá»n k quÃ¡ nhá» cÃ³ thá»ƒ khiáº¿n mÃ´ hÃ¬nh trá»Ÿ nÃªn quÃ¡ nháº¡y cáº£m vá»›i cÃ¡c nhiá»…u vÃ  Ä‘áº·c Ä‘iá»ƒm riÃªng cá»§a dá»¯ liá»‡u, dáº«n Ä‘áº¿n overfitting. NgÆ°á»£c láº¡i, chá»n k quÃ¡ lá»›n cÃ³ thá»ƒ lÃ m máº¥t Ä‘i cÃ¡c chi tiáº¿t quan trá»ng trong dá»¯ liá»‡u, dáº«n Ä‘áº¿n hiá»‡n tÆ°á»£ng underfiting. Do Ä‘Ã³, viá»‡c chá»n k cáº§n Ä‘Æ°á»£c thá»±c hiá»‡n cáº©n tháº­n vÃ  dá»±a trÃªn nhiá»u yáº¿u tá»‘ khÃ¡c nhau. 
		
	## áº¢nh hÆ°á»Ÿng cá»§a giÃ¡ trá»‹ k Ä‘áº¿n hiá»‡u suáº¥t mÃ´ hÃ¬nh  
		### GiÃ¡ trá»‹ k nhá»(k = 1,2,3...	) 
			Khi k ráº¥t nhá», mÃ´ hÃ¬nh KNN chá»‰ dá»±a vÃ o má»™t hoáº·c má»™t vÃ i Ä‘iá»ƒm gáº§n nháº¥t Ä‘á»ƒ dá»± Ä‘oÃ¡n káº¿t quáº£. Äiá»u nÃ y cÃ³ thá»ƒ dáº«n Ä‘áº¿n viá»‡c mÃ´ hÃ¬nh bá»‹ áº£nh hÆ°á»Ÿng máº¡nh bá»Ÿi cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u nhiá»…u hoáº·c lá»—i(outliers). Vá»›i k = 1, KNN trá»Ÿ thÃ nh má»™t loáº¡i mÃ´ hÃ¬nh cá»±c ká»³ cá»¥c bá»™, vÃ  dá»… bá»‹ overfiting, tá»©c lÃ  mÃ´ hÃ¬nh sáº½ dá»± Ä‘oÃ¡n ráº¥t tá»‘t trÃªn dá»¯ liá»‡u huáº¥n luyá»‡n nhÆ°ng cÃ³ thá»ƒ dá»± Ä‘oÃ¡n kÃ©m trÃªn dá»¯ liá»‡u kiá»ƒm tra hoáº·c dá»¯ liá»‡u má»›i  
			
		## GiÃ¡ trá»‹ k lá»›n : 
			Khi k tÄƒng, mÃ´ hÃ¬nh trá»Ÿ nÃªn tá»•ng quÃ¡t hÆ¡n vÃ¬ nÃ³ sáº½ xem xÃ©t nhiá»u Ä‘iá»ƒm dá»¯ liá»‡u Ä‘á»ƒ Ä‘Æ°a ra quyáº¿t Ä‘á»‹nh. Äiá»u nÃ y giÃºp giáº£m thiá»ƒu áº£nh hÆ°á»Ÿng cá»§a cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u nhiá»…u, nhÆ°ng Ä‘á»“ng thá»i cÅ©ng cÃ³ nguy cÆ¡ lÃ m máº¥t Ä‘i cÃ¡c Ä‘áº·c Ä‘iá»ƒm quan trá»ng cá»§a dá»¯ liá»‡u cá»¥c bá»™. Náº¿u k quÃ¡ lá»›n, mÃ´ hÃ¬nh cÃ³ thá»ƒ trá»Ÿ nÃªn quÃ¡ tá»•ng quÃ¡t(underfiting), dáº«n Ä‘áº¿n viá»‡c dá»± Ä‘oÃ¡n trá»Ÿ nÃªn khÃ´ng chÃ­nh xÃ¡c vÃ¬ nÃ³ khÃ´ng pháº£n Ã¡nh Ä‘Ãºng cÃ¡c má»‘i quan há»‡ cá»¥c bá»™ trong dá»¯ liá»‡u 
			
	## CÃ¡c phÆ°Æ¡ng phÃ¡p chá»n giÃ¡ trá»‹ k 
		Viá»‡c xÃ¡c Ä‘á»‹nh giÃ¡ trá»‹ k tá»‘i Æ°u cÃ³ thá»ƒ Ä‘Æ°á»£c thá»±c hiá»‡n thÃ´ng qua má»™t sá»‘ phÆ°Æ¡ng phÃ¡p phá»• biáº¿n : 
			
		### Cross-validation 
			Cross-validation lÃ  phÆ°Æ¡ng phÃ¡p phá»• biáº¿n nháº¥t Ä‘á»ƒ chá»n k. Vá»›i cross-valication, táº­p dá»¯ liá»‡u Ä‘Æ°á»£c chia thÃ nh nhiá»u pháº§n(folds), vÃ  mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn má»™t sá»‘ pháº§n trong khi kiá»ƒm tra trÃªn pháº§n cÃ²n láº¡i. QuÃ¡ trÃ¬nh nÃ y Ä‘Æ°á»£c láº·p láº¡i nhiá»u láº§n vá»›i cÃ¡c giÃ¡ trá»‹ k khÃ¡c nhau, vÃ  giÃ¡ trá»‹ k tá»‘i Æ°u Ä‘Æ°á»£c chá»n dá»±a trÃªn hiá»‡u suáº¥t trung bÃ¬nh trÃªn táº¥t cáº£ cÃ¡c fold 
			
			Cross-validation khÃ´ng chá»‰ giÃºp tÃ¬m ra giÃ¡ trá»‹ k tá»‘t nháº¥t mÃ  cÃ²n giÃºp Ä‘Ã¡nh giÃ¡ Ä‘á»™ á»•n Ä‘á»‹nh vÃ  kháº£ nÄƒng tá»•ng quÃ¡t hÃ³a cá»§a mÃ´ hÃ¬nh 
			
		### NguyÃªn táº¯c thumb rule(quy táº¯c ngÃ³n tay cÃ¡i)
			Má»™t nguyÃªn táº¯c thumb rule Ä‘Æ¡n giáº£n Ä‘á»ƒ chá»n k lÃ  chá»n giÃ¡ trá»‹ k báº±ng cÄƒn báº­c hai cá»§a sá»‘ lÆ°á»£ng Ä‘iá»ƒm dá»¯ liá»‡u trong táº­p huáº¥n luyá»‡n : k=nk = \sqrt{n}k=nâ€‹ . Trong Ä‘Ã³ n lÃ  sá»‘ lÆ°á»£ng máº«u trong táº­p huáº¥n luyá»‡n. ÄÃ¢y lÃ  má»™t phÆ°Æ¡ng phÃ¡p Ä‘Æ¡n giáº£n vÃ  nhanh chÃ³ng Ä‘á»ƒ báº¯t Ä‘áº§u, nhÆ°ng nÃ³ khÃ´ng pháº£i lÃºc nÃ o cÅ©ng cho káº¿t quáº£ tá»‘i Æ°u. ThÃ´ng thÆ°á»ng, phÆ°Æ¡ng phÃ¡p nÃ y Ä‘Æ°á»£c sá»­ dá»¥ng nhÆ° má»™t Ä‘iá»ƒm khá»Ÿi Ä‘áº§u, vÃ  sau Ä‘Ã³ giÃ¡ trá»‹ k cÃ³ thá»ƒ Ä‘Æ°á»£c tinh chá»‰nh thÃ´ng qua cross-validation hoáº·c cÃ¡c phÆ°Æ¡ng phÃ¡p khÃ¡c 
			
		### Grid search 
			Grid search lÃ  má»™t phÆ°Æ¡ng phÃ¡p há»‡ thá»‘ng Ä‘á»ƒ tÃ¬m kiáº¿m giÃ¡ trá»‹ k tá»‘i Æ°u báº±ng cÃ¡ch thá»­ táº¥t cáº£ cÃ¡c giÃ¡ trá»‹ trong má»™t khoáº£ng Ä‘Æ°á»£c Ä‘á»‹nh trÆ°á»›c. VÃ­ dá»¥, báº¡n cÃ³ thá»ƒ thá»­ cÃ¡c giÃ¡ trá»‹ k tá»« 1 Ä‘áº¿n 20 vÃ  chá»n giÃ¡ trá»‹ cÃ³ hiá»‡u suáº¥t tá»‘t nháº¥t dá»±a trÃªn má»™t sá»‘ chá»‰ sá»‘ Ä‘Ã¡nh giÃ¡ nhÆ° Ä‘á»™ chÃ­nh xÃ¡c(accuracy), F1-score hoáº·c mean squred error (MSE)
			
			Grid search cÃ³ thá»ƒ káº¿t há»£p vá»›i cross-validation Ä‘á»ƒ Ä‘áº£m báº£o ráº±ng giÃ¡ trá»‹ k chá»n Ä‘Æ°á»£c khÃ´ng chá»‰ tá»‘t trÃªn táº­p dá»¯ liá»‡u hiá»‡n táº¡i mÃ  cÃ²n cÃ³ kháº£ nÄƒng tá»•ng quÃ¡t hÃ³a tá»‘t 
			
		
		### Random Search 
			Random search lÃ  má»™t phiÃªn báº£n Ä‘Æ¡n giáº£n hÆ¡n cá»§a grid search, trong Ä‘Ã³ cÃ¡c giÃ¡ trá»‹ k Ä‘Æ°á»£c chá»n ngáº«u nhiÃªn tá»« má»™t khoáº£ng giÃ¡ trá»‹ nháº¥t Ä‘á»‹nh. Random search thÆ°á»ng Ã­t tá»‘n kÃ©m hÆ¡n vá» máº·t tÃ­nh toÃ¡n so vá»›i grid search, vÃ  trong má»™t sá»‘ trÆ°á»ng há»£p cÃ³ thá»ƒ tÃ¬m Ä‘Æ°á»£c giÃ¡ trá»‹ k tá»‘i Æ°u hÆ¡n hoáº·c gáº§n tá»‘i Æ°u mÃ  khÃ´ng cáº§n pháº£i kiá»ƒm tra táº¥t cáº£ cÃ¡c giÃ¡ trá»‹ cá»¥ thá»ƒ 
			
		### Sá»­ dá»¥ng phÆ°Æ¡ng phÃ¡p loáº¡i bá» nhiá»…u 
			Khi dá»¯ liá»‡u chá»©a nhiá»u nhiá»u, viá»‡c giáº£m áº£nh hÆ°á»Ÿng cá»§a cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u lá»—i cÃ³ thá»ƒ giÃºp chá»n k tá»‘t hÆ¡n. CÃ¡c ká»¹ thuáº­t nhÆ° loáº¡i bá» cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u náº±m ngoÃ i má»™t khoáº£ng xÃ¡c Ä‘á»‹nh(outlier removal ) hoáº·c giáº£m thiá»ƒu nhiá»…u báº±ng cÃ¡c phÆ°Æ¡ng phÃ¡p lá»c trÆ°á»›c khi Ã¡p dá»¥ng KNN cÃ³ thá»ƒ cáº£i thiá»‡n viá»‡c chá»n k 
			
	## CÃ¡c yáº¿u tá»‘ áº£nh hÆ°á»Ÿng Ä‘áº¿n viá»‡c chá»n giÃ¡ trá»‹ k 
		
		### KÃ­ch thÆ°á»›c táº­p dá»¯ liá»‡u 
			KÃ­ch thÆ°á»›c cá»§a táº­p dá»¯ liá»‡u huáº¥n luyá»‡n áº£nh hÆ°á»Ÿng trá»±c tiáº¿p Ä‘áº¿n viá»‡c chá»n k. Vá»›i táº­p dá»¯ liá»‡u lá»›n, giÃ¡ trá»‹ k cÃ³ thá»ƒ lá»›n hÆ¡n Ä‘á»ƒ Ä‘áº£m báº£o ráº±ng dá»± Ä‘oÃ¡n khÃ´ng bá»‹ áº£nh hÆ°á»Ÿng quÃ¡ nhiá»u bá»Ÿi nhiá»u. NgÆ°á»£c láº¡i, vá»›i cÃ¡c táº­p dá»¯ liá»‡u nhá», k nhá» hÆ¡n cÃ³ thá»ƒ phÃ¹ há»£p hÆ¡n  
			
		### Chiá»u cá»§a dá»¯ liá»‡u(Dimensionality )
			Khi sá»‘ lÆ°á»£ng Ä‘áº·c trÆ°ng(features) cá»§a dá»¯ liá»‡u tÄƒng lÃªn, khÃ´ng gian dá»¯ liá»‡u trá»Ÿ nÃªn thÆ°a thá»›t hÆ¡n, vÃ  khoáº£ng cÃ¡ch giá»¯a cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u trá»Ÿ nÃªn Ã­t phÃ¢n biá»‡t hÆ¡n(hiá»‡n tÆ°á»£ng "curse of dimensionality"). Trong trÆ°á»ng há»£p nÃ y, má»™t giÃ¡ trá»‹ k lá»›n cÃ³ thá»ƒ cáº§n thiáº¿t Ä‘á»ƒ giáº£m thiá»ƒu áº£nh hÆ°á»Ÿng cá»§a chiá»u cao dá»¯ liá»‡u 
		
		## PhÃ¢n phá»‘i dá»¯ liá»‡u : 
			PhÃ¢n phá»‘i cá»§a dá»¯ liá»‡u cÅ©ng lÃ  má»™t yáº¿u tá»‘ quan trá»ng. Náº¿u dá»¯ liá»‡u cÃ³ cÃ¡c cá»¥m riÃªng biá»‡t(clusters), má»™t giÃ¡ trá»‹ k nhá» cÃ³ thá»ƒ giÃºp phÃ¡t hiá»‡n cÃ¡c cá»¥m nÃ y má»™t cÃ¡ch chÃ­nh xÃ¡c hÆ¡n. NgÆ°á»£c láº¡i, náº¿u dá»¯ liá»‡u phÃ¢n phá»‘i ngáº«u nhiÃªn, k lá»›n hÆ¡n cÃ³ thá»ƒ mang láº¡i káº¿t quáº£ tá»‘t hÆ¡n 
			
	## Tá»‘i Æ°u hÃ³a giÃ¡ trá»‹ k cho tá»«ng bÃ i toÃ¡n cá»¥ thá»ƒ 
		
		## BÃ i toÃ¡n phÃ¢n loáº¡i 
			Trong bÃ i toÃ¡n phÃ¢n loáº¡i, viá»‡c chá»n k phÃ¹ há»£p giÃºp cÃ¢n báº±ng giá»¯a viá»‡c duy trÃ¬ sá»± chÃ­nh xÃ¡c cá»§a mÃ´ hÃ¬nh vÃ  kháº£ nÄƒng tá»•ng quÃ¡t hÃ³a cho dá»¯ liá»‡u má»›i. Má»™t giÃ¡ trá»‹ k tá»‘i Æ°u sáº½ giÃºp mÃ´ hÃ¬nh nháº­n dáº¡ng Ä‘Ãºng cÃ¡c lá»›p vÃ  giáº£m thiá»ƒu lá»—i phÃ¢n loáº¡i 
			
		## BÃ i toÃ¡n há»“i quy 
			Äá»‘i vá»›i bÃ i toÃ¡n há»“i quy, viá»‡c chá»n k khÃ´ng chá»‰ áº£nh hÆ°á»Ÿng Ä‘áº¿n Ä‘á»™ chÃ­nh xÃ¡c cá»§a dá»± Ä‘oÃ¡n mÃ  cÃ²n quyáº¿t Ä‘á»‹nh má»©c Ä‘á»™ "mÆ°á»£t" cá»§a cÃ¡c dá»± Ä‘oÃ¡n. GiÃ¡ trá»‹ k quÃ¡ nhá» cÃ³ thá»ƒ dáº«n Ä‘áº¿n cÃ¡c dá»± Ä‘oÃ¡n biáº¿n Ä‘á»™ng máº¡nh,trong khi k quÃ¡ lá»›n cÃ³ thá»ƒ lÃ m máº¥t Ä‘i sá»± nháº¡y cáº£m cá»§a mÃ´ hÃ¬nh vá»›i cÃ¡c thay Ä‘á»•i nhá» trong dá»¯ liá»‡u  
			

# Æ¯u vÃ  nhÆ°á»£c Ä‘iá»ƒm cá»§a KNN trong há»“i quy 
		
		
	## Æ¯u Ä‘iá»ƒm 	
		### Dá»… hiá»ƒu vÃ  dá»… triá»ƒn khai 
			KNN lÃ  má»™t trong nhá»¯ng thuáº­t toÃ¡n Ä‘Æ¡n giáº£n nháº¥t trong há»c mÃ¡y. KhÃ´ng cáº§n xÃ¢y dá»±ng mÃ´ hÃ¬nh phá»©c táº¡p hoáº·c tÃ¬m kiáº¿m cÃ¡c tham sá»‘ phá»©c táº¡p, KNN chá»‰ dá»±a vÃ o tÃ­nh toÃ¡n khoáº£ng cÃ¡ch vÃ  tÃ¬m kiáº¿m k hÃ ng xÃ³m gáº§n nháº¥t. 
			Äiá»u nÃ y lÃ m cho KNN trá»Ÿ nÃªn dá»… hiá»ƒu vÃ  dá»… triá»ƒn khai, ngay cáº£ khi Ä‘á»‘i vá»›i nhá»¯ng ngÆ°á»i má»›i báº¯t Ä‘áº§u trong lÄ©nh vá»±c há»c mÃ¡y 
			
		### KhÃ´ng yÃªu cáº§u giáº£ Ä‘á»‹nh vá» phÃ¢n phá»‘i dá»¯ liá»‡u 
			KNN khÃ´ng dá»±a vÃ  báº¥t ká»³ giÃ¡ trá»‹ giáº£ Ä‘á»‹nh nÃ o vá» phÃ¢n phá»‘i dá»¯ cá»§a dá»¯ liá»‡u, Ä‘iá»u nÃ y ráº¥t há»¯u Ã­ch trong cÃ¡c trÆ°á»ng há»£p mÃ  báº¡n khÃ´ng biáº¿t rÃµ vá» hÃ¬nh dáº¡ng cá»§a dá»¯ liá»‡u 
			Äiá»u nÃ y Ä‘áº·c biá»‡t quan trá»ng khi dá»¯ liá»‡u cÃ³ tÃ­nh cháº¥t phi tuyáº¿n tÃ­nh hoáº·c khÃ´ng tuÃ¢n theo cÃ¡c phÃ¢n phá»‘i thÃ´ng thÆ°á»ng(nhÆ° phÃ¢n phá»‘i chuáº©n)
			
		### Linh hoáº¡t vá»›i cÃ¡c loáº¡i dá»¯ liá»‡u khÃ¡c nhau 
			KNN cÃ³ thá»ƒ Ã¡p dá»¥ng cho nhiá»u loáº¡i dá»¯ liá»‡u khÃ¡c nhau, tá»« dá»¯ liá»‡u sá»‘, dá»¯ liá»‡u phÃ¢n loáº¡i Ä‘áº¿n dá»¯ liá»‡u dáº¡ng vÄƒn báº£n hoáº·c hÃ¬nh áº£nh, chá»‰ cáº§n cÃ³ má»™t cÃ¡ch Ä‘á»ƒ Ä‘o khoáº£ng cÃ¡ch giá»¯a cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u 
			
		### KhÃ´ng cáº§n giai Ä‘oáº¡n huáº¥n luyá»‡n 
			KNN khÃ´ng yÃªu cáº§u huáº¥n luyá»‡n mÃ´ hÃ¬nh trÆ°á»›c khi sá»­ dá»¥ng. toÃ n bá»™ quÃ¡ trÃ¬nh diá»…n ra trong quÃ¡ trÃ¬nh dá»± Ä‘oÃ¡n. Äiá»u nÃ y cÃ³ thá»ƒ tiáº¿t kiá»‡m thá»i gian khi xá»­ lÃ½ cÃ¡c táº­p dá»¯ liá»‡u nhá» hoáº·c vá»«a pháº£i. 
			NgoÃ i ra, KNN cÃ³ thá»ƒ thÃ­ch á»©ng nhanh chÃ³ng vá»›i nhá»¯ng thay Ä‘á»•i trong dá»¯ liá»‡u mÃ  khÃ´ng cáº§n huáº¥n luyá»‡n láº¡i toÃ n bá»™ mÃ´ hÃ¬nh . 
			
	## NhÆ°á»£c Ä‘iá»ƒm 
	
		### Äá»™ phá»©c táº¡p tÃ­nh toÃ¡n cao 
			KNN cáº§n pháº£i tÃ­nh toÃ¡n khoáº£ng cÃ¡ch giá»¯a cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u má»›i vÃ  táº¥t cáº£ cÃ¡c Ä‘iá»ƒm trong táº­p huáº¥n luyá»‡n, Ä‘iá»u nÃ y dáº«n Ä‘áº¿n Ä‘á»™ phá»©c táº¡p tÃ­nh toÃ¡n lÃ  O(n)O(n)O(n), vá»›i n lÃ  sá»‘ lÆ°á»£ng Ä‘iá»ƒm trong táº­p huáº¥n luyá»‡n.
			Khi sá»‘ lÆ°á»£ng máº«u hoáº·c chiá»u cá»§a dá»¯ liá»‡u lá»›n, viá»‡c tÃ­nh toÃ¡n nÃ y trá»Ÿ nÃªn ráº¥t tá»‘n kÃ©m vá» thá»i gian vÃ  tÃ i nguyÃªn, khiáº¿n KNN khÃ´ng phÃ¹ há»£p vá»›i cÃ¡c á»©ng dá»¥ng yÃªu cáº§u thá»i gian thá»±c hoáº·c xá»­ lÃ½ dá»¯ liá»‡u lá»›n. 
			
		### Nháº¡y cáº£m vá»›i nhiá»…u vÃ  dá»¯ liá»‡u khÃ´ng liÃªn quan 
			KNN ráº¥t nháº¡y cáº£m vá»›i nhiá»…u(noise) trong dá»¯ liá»‡u, Ä‘áº·c biá»‡t lÃ  cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u lá»—i(outliers) hoáº·c cÃ¡c Ä‘áº·c trÆ°ng khÃ´ng liÃªn quan. 
			Náº¿u dá»¯ liá»‡u chá»©a nhiá»u nhiá»u, cÃ¡c Ä‘iá»ƒm nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c coi lÃ  hÃ ng xÃ³m gáº§n nháº¥t vÃ  gÃ¢y áº£nh hÆ°á»Ÿng Ä‘áº¿n káº¿t quáº£ dá»± Ä‘oÃ¡n, dáº«n Ä‘áº¿n sai sá»‘ cao hÆ¡n. Äiá»u nÃ y yÃªu cáº§u pháº£i cÃ³ bÆ°á»›c tiá»n xá»­ lÃ½ dá»¯ liá»‡u ká»¹ lÆ°á»¡ng trÆ°á»›c khi Ã¡p dá»¥ng KNN 
			
		### Lá»±a chá»n giÃ¡ trá»‹ k khÃ³ khÄƒn  
			Viá»‡c chá»n Ä‘Ãºng giÃ¡ trá»‹ k lÃ  ráº¥t quan trá»ng nhÆ°ng khÃ´ng pháº£i lÃºc nÃ o cÅ©ng Ä‘Æ¡n giáº£n. GiÃ¡ trá»‹ k quÃ¡ nhá» cÃ³ thá»ƒ dáº«n Ä‘áº¿n overfitting, nÆ¡i mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n ráº¥t tá»‘t trÃªn táº­p huáº¥n luyá»‡n nhÆ°ng kÃ©m hiá»‡u quáº£ trÃªn dá»¯ liá»‡u má»›i 
			NgÆ°á»£c láº¡i, giÃ¡ trá»‹ k quÃ¡ lá»›n cÃ³ thá»ƒ dáº«n Ä‘áº¿n underfitting, nÆ¡i mÃ´ hÃ¬nh trá»Ÿ nÃªn quÃ¡ tá»•ng quÃ¡t vÃ  máº¥t Ä‘i tÃ­nh Ä‘áº·c trÆ°ng cá»§a cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u gáº§n ká». Äá»ƒ giáº£i quyáº¿t váº¥n Ä‘á» nÃ y, cÃ¡c ká»¹ thuáº­t nhÆ° cross-validation thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng, nhÆ°ng Ä‘iá»u nÃ y láº¡i tÄƒng thÃªm Ä‘á»™ phá»©c táº¡p vÃ  chi phÃ­ tÃ­nh toÃ¡n. 
			
		### KhÃ³ má»Ÿ rá»™ng cho dá»¯ liá»‡u lá»›n  
			Do Ä‘áº·c tÃ­nh pháº£i lÆ°u trá»¯ toÃ n bá»™ táº­p dá»¯ liá»‡u huáº¥n luyá»‡n vÃ  tÃ­nh toÃ¡n khoáº£ng cÃ¡ch cho má»—i dá»± Ä‘oÃ¡n, KNN khÃ´ng dá»… dÃ ng má»Ÿ rá»™ng cho cÃ¡c táº­p dá»¯ liá»‡u lá»›n. Äiá»u nÃ y cÃ³ thá»ƒ gÃ¢y ra cÃ¡c váº¥n Ä‘á» vá» bá»™ nhá»› vÃ  hiá»‡u suáº¥t, Ä‘áº·c biá»‡t lÃ  khi triá»ƒn khai trong cÃ¡c há»‡ thÃ´ng thá»±c táº¿. 
			
			NgoÃ i ra , khi lÃ m viá»‡c vá»›i dá»¯ liá»‡u cÃ³ sá»‘ lÆ°á»£ng lá»›n Ä‘áº·c trÆ°ng(hight-dimensional data) KNN cÃ³ thá»ƒ gáº·p pháº£i hiá»‡n tÆ°á»£ng "curse of dimensionality", nÆ¡i khoáº£ng cÃ¡ch giá»¯a cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u trá»Ÿ nÃªn gáº§n báº±ng nhau, lÃ m giáº£m hiá»‡u quáº£ cá»§a thuáº­t toÃ¡n  .
			
# á»¨ng dá»¥ng cá»§a KNN trong thá»±c táº¿		
		
	## PhÃ¢n loáº¡i vÄƒn báº£n 
		KNN lÃ  má»™t trong nhá»¯ng thuáº­t toÃ¡n phá»• biáº¿n nháº¥t Ä‘Æ°á»£c sá»­ dá»¥ng trong lÄ©nh vá»±c phÃ¢n loáº¡i vÄƒn báº£n. Vá»›i kháº£ nÄƒng xá»­ lÃ½ dá»¯ liá»‡u phi cáº¥u trÃºc nhÆ° vÄƒn báº£n, KNN cÃ³ thá»ƒ Ä‘Æ°á»£c Ã¡p dá»¥ng vÃ o nhiá»u á»©ng dá»¥ng khÃ¡c nhau, bao gá»“m 
		
		### PhÃ¢n loáº¡i tÃ i liá»‡u  (Document Classification)
			Trong phÃ¢n loáº¡i tÃ i liá»‡u, KNN cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ gÃ¡n nhÃ£n cho cÃ¡c vÄƒn báº£n dá»±a trÃªn ná»™i dung cá»§a chÃºng. VÃ­ dá»¥, KNN cÃ³ thá»ƒ giÃºp phÃ¢n loáº¡i tÃ i liá»‡u thÃ nh cÃ¡c thá»ƒ loáº¡i nhÆ° thá»ƒ thao, chÃ­nh trá»‹, cÃ´ng nghá»‡, vvv báº±ng cÃ¡ch so sÃ¡nh vÄƒn báº£n cáº§n phÃ¢n loáº¡i vá»›i cÃ¡c vÄƒn báº£n Ä‘Ã£ biáº¿t nhÃ£n trÆ°á»›c Ä‘Ã³. 
		
			Khi sá»­ dá»¥ng KNN cho phÃ¢n loáº¡i vÄƒn báº£n, cÃ¡c Ä‘áº·c trÆ°ng(feature) thÆ°á»ng Ä‘Æ°á»£c biá»ƒu diá»…n dÆ°á»›i dáº¡ng vector tá»«(word vectors) hoáº·c sá»­ dá»¥ng phÆ°Æ¡ng phÃ¡p TF-IDF Ä‘á»ƒ cÃ¢n nháº¯c táº§n suáº¥t tá»« vÃ  táº§m quan trá»ng cá»§a chÃºng trong vÄƒn báº£n. 
		
		### PhÃ¡t hiá»‡n thÆ° rÃ¡c(Spam Detection )
			KNN cÅ©ng Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i trong viá»‡c phÃ¡t hiá»‡n thÆ° rÃ¡c. Thuáº­t toÃ¡n nÃ y cÃ³ thá»ƒ phÃ¢n loáº¡i email lÃ  thÆ° rÃ¡c hay khÃ´ng dá»±a trÃªn cÃ¡c Ä‘áº·c trÆ°ng cá»§a email nhÆ° táº§n suáº¥t xuáº¥t hiá»‡n cá»§a tá»« ngá»¯, tiÃªu Ä‘á» email, hoáº·c cÃ¡c tá»« khÃ¡c Ä‘áº·c biá»‡t lÃªn quan Ä‘áº¿n thÆ° rÃ¡c. 
			Khi Ã¡p dá»¥ng vÃ o phÃ¡t hiá»‡n thÆ° rÃ¡c, KNN cÃ³ thá»ƒ táº­n dá»¥ng cÃ¡c táº­p dá»¯ liá»‡u lá»›n tá»« cÃ¡c email Ä‘Ã£ Ä‘Æ°á»£c phÃ¢n loáº¡i trÆ°á»›c Ä‘Ã³, vÃ  qua Ä‘Ã³ há»c cÃ¡ch nháº­n diá»‡n cÃ¡c máº«u thÆ° rÃ¡c má»›i má»™t cÃ¡ch hiá»‡u quáº£ 
		

	## Nháº­n dáº¡ng hÃ¬nh áº£nh 
		Nháº­n dáº¡ng hÃ¬nh áº£nh lÃ  má»™t trong nhá»¯ng lÄ©nh vá»±c mÃ  KNN Ä‘Æ°á»£c sá»­ dá»¥ng ráº¥t phá»• biáº¿n, Ä‘áº·c biá»‡t trong cÃ¡c á»©ng dá»¥ng yÃªu cáº§u Ä‘á»™ chÃ­nh xÃ¡c cao vÃ  kháº£ nÄƒng pháº£n á»©ng nhanh 
		
		### Nháº­n dáº¡ng khuÃ´n máº·t(Face Recognition)
			KNN ÄÆ°á»£c sá»­ dá»¥ng trong há»‡ thá»‘ng nháº­n dáº¡ng khuÃ´n máº·t Ä‘á»ƒ phÃ¢n loáº¡i vÃ  nháº­n diá»‡n cÃ¡c hÃ¬nh áº£nh khuÃ´n máº·t. Khi má»™t hÃ¬nh áº£nh khuÃ´n máº·t má»›i Ä‘Æ°á»£c cung cáº¥p, KNN sáº½ tÃ¬m kiáº¿m trong táº­p dá»¯ liá»‡u cÃ¡c khuÃ´n máº·t Ä‘Ã£ biáº¿t Ä‘á»ƒ xÃ¡c Ä‘á»‹nh khuÃ´n máº·t nÃ o cÃ³ Ä‘áº·c Ä‘iá»ƒm tÆ°Æ¡ng tá»± nháº¥t 
			
			CÃ¡c Ä‘áº·c trÆ°ng cá»§a khuÃ´n máº·t thÆ°á»ng Ä‘Æ°á»£c trÃ­ch xuáº¥t sá»­ dá»¥ng cÃ¡c ká»¹ thuáº­t xá»­ lÃ½ hÃ¬nh áº£nh hoáº·c cÃ¡c mÃ´ hÃ¬nh há»c sÃ¢u, sau Ä‘Ã³ sá»­ dá»¥ng KNN Ä‘á»ƒ so sÃ¡nh cÃ¡c Ä‘áº·c trÆ°ng nÃ y vÃ  Ä‘Æ°a ra quyáº¿t Ä‘á»‹nh nháº­n dáº¡ng 
			
		### Nháº­n dáº¡ng chá»¯ viáº¿t tay(Handwritten Digit Recognition)
			KNN cÅ©ng Ä‘Æ°á»£c sá»­ dá»¥ng trong nháº­n dáº¡ng chá»¯ viáº¿t tay, má»™t á»©ng dá»¥ng phá»• biáº¿n trong cÃ¡c há»‡ thá»‘ng nháº­n dáº¡ng chá»¯ sá»‘ nhÆ° kiá»ƒm tra bÃ i thi tráº¯c nghiá»‡m tá»± Ä‘á»™ng hoáº·c xá»­ lÃ½ tÃ i liá»‡u sá»‘ hÃ³a. 
			Trong á»©ng dá»¥ng nÃ y, KNN sáº½ so sÃ¡nh hÃ¬nh áº£nh chá»¯ sá»‘ viáº¿t tay vá»›i cÃ¡c hÃ¬nh áº£nh máº«u Ä‘Ã£ biáº¿t Ä‘á»ƒ xÃ¡c Ä‘á»‹nh chá»¯ sá»‘ Ä‘Ã³. 
			VÃ­ dá»¥ ná»•i tiáº¿ng nháº¥t lÃ  viá»‡c sá»­ dá»¥ng KNN trÃªn táº­p dá»¯ liá»‡u MNIST, má»™t táº­p dá»¯ liá»‡u tiÃªu chuáº©n gá»“m cÃ¡c chá»¯ sá»‘ viáº¿t tay tá»« 0 Ä‘áº¿n 9, nÆ¡i KNN Ä‘Ã£ chá»©ng tá» lÃ  má»™t phÆ°Æ¡ng phÃ¡p Ä‘Æ¡n giáº£n nhÆ°ng hiá»‡u quáº£ trong viá»‡c phÃ¢n loáº¡i cÃ¡c chá»¯ sá»‘ nÃ y 
			
			
	## Dá»± Ä‘oÃ¡n lÄ©nh vá»±c tÃ i chÃ­nh 
		Trong lÄ©nh vá»±c tÃ i chÃ­nh, KNN cÃ³ nhiá»u á»©ng dá»¥ng khÃ¡c nhau, tá»« dá»± Ä‘oÃ¡n giÃ¡ cá»• phiáº¿u Ä‘áº¿n phÃ¢n tÃ­ch rá»§i ro vÃ  quáº£n lÃ½ danh má»¥c Ä‘áº§u tÆ° : 
			
			### Dá»± Ä‘oÃ¡n giÃ¡ cá»• phiáº¿u(Stock Price Prediction): 
				Máº·c dÃ¹ cÃ¡c phÆ°Æ¡ng phÃ¡p dá»± Ä‘oÃ¡n cá»• phiáº¿u phá»©c táº¡p nhÆ° máº¡ng nÆ¡-ron hoáº·c mÃ´ hÃ¬nh chuá»—i thá»i gian thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng, KNN váº«n cÃ³ thá»ƒ Ä‘Æ°á»£c Ã¡p dá»¥ng Ä‘á»ƒ dá»± Ä‘oÃ¡n giÃ¡ cá»• phiáº¿u báº±ng cÃ¡ch xem xÃ©t cÃ¡c Ä‘áº·c Ä‘iá»ƒm tÆ°Æ¡ng tá»± giá»¯a cÃ¡c ngÃ y giao dá»‹ch. 
				KNN cÃ³ thá»ƒ sá»­ dá»¥ng thÃ´ng tin tá»« cÃ¡c ngÃ y giao dá»‹ch trÆ°á»›c Ä‘á»ƒ dá»± Ä‘oÃ¡n giÃ¡ cá»• phiáº¿u vÃ o ngÃ y hiá»‡n táº¡i hoáº·c tÆ°Æ¡ng láº¡i gáº§n báº±ng cÃ¡ch tÃ¬m kiáº¿m cÃ¡c ngÃ y cÃ³ cÃ¡c Ä‘áº·c Ä‘iá»ƒm tÆ°Æ¡ng tá»±(nhÆ° giÃ¡ má»Ÿ cá»­a, giÃ¡ Ä‘Ã³ng cá»­a, khá»‘i lÆ°á»£ng giao dá»‹ch) vÃ  dá»± Ä‘oÃ¡n dá»±a trÃªn cÃ¡c ngÃ y Ä‘Ã³ 
				
			### PhÃ¢n tÃ­ch rá»§i ro tÃ­n dá»¥ng(Credit Risk Analysis): 
				Trong phÃ¢n tÃ­ch rá»§i ro tÃ­n dá»¥ng, KNN cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ phÃ¢n loáº¡i khÃ¡ch hÃ ng thÃ nh cÃ¡c nhÃ³m rá»§i ro khÃ¡c nhau dá»±a trÃªn há»“ sÆ¡ tÃ­n dá»¥ng cá»§a há». CÃ¡c Ä‘áº·c trÆ°ng cÃ³ thá»ƒ bao gá»“m lá»‹ch sá»­ tÃ­n dá»¥ nháº­p, ná»£ náº§n, vÃ  cÃ¡c thÃ´ng tin nhÃ¢n kháº©u há»c khÃ¡c. ng, thu
				Khi má»™t khÃ¡ch hÃ ng má»›i yÃªu cáº§u tÃ­n dá»¥ng, KNN sáº½ so sÃ¡nh há» vá»›i cÃ¡c khÃ¡ch hÃ ng trÆ°á»›c Ä‘Ã³ trong cÃ¹ng nhÃ³m rá»§i ro vÃ  dá»± Ä‘oÃ¡n kháº£ nÄƒng tráº£ ná»£ cá»§a há» dá»±a trÃªn káº¿t quáº£ cá»§a nhá»¯ng khÃ¡c hÃ ng tÆ°Æ¡ng tá»± 
				
	## Y táº¿ vÃ  chuáº©n Ä‘oÃ¡n bá»‡nh 
		KNN cÅ©ng cÃ³ á»©ng  dá»¥ng rá»™ng rÃ£i trong lÄ©nh vá»±c y táº¿, Ä‘áº·c biá»‡t trong viá»‡c há»— trá»£ chuáº©n Ä‘oÃ¡n bá»‡nh vÃ  phÃ¢n loáº¡i cÃ¡c loáº¡i bá»‡nh: 
			
			
		### Chuáº©n Ä‘oÃ¡n bá»‡nh dá»±a trÃªn hÃ¬nh áº£nh y táº¿(Medical Image Diagnosis)
			Trong y táº¿, KNN cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ phÃ¢n loáº¡i cÃ¡c hÃ¬nh áº£nh y táº¿, cháº³ng háº¡n nhÆ° hÃ¬nh áº£nh X-quang, MRI, hoáº·c siÃªu Ã¢m. VÃ­ dá»¥ KNN cÃ³ thá»ƒ giÃºp phÃ¢n loáº¡i cÃ¡c khá»‘i u trong hÃ¬nh áº£nh X-quang thÃ nh lÃ nh tÃ­nh hoáº·c Ã¡c tÃ­nh dá»±a trÃªn cÃ¡c Ä‘áº·c Ä‘iá»ƒm hÃ¬nh áº£nh. 
			KNN Ä‘áº·c biá»‡t há»¯u Ã­ch khi dá»¯ liá»‡u vá» cÃ¡c trÆ°á»ng há»£p bá»‡nh lÃ½ tÆ°Æ¡ng tá»± cÃ³ sáºµn, giÃºp mÃ´ hÃ¬nh há»c tá»« nhá»¯ng trÆ°á»ng há»£p trÆ°á»›c Ä‘Ã³ Ä‘á»ƒ dá»± Ä‘oÃ¡n káº¿t quáº£ cho trÆ°á»ng há»£p má»›i  
			
		### Dá»± Ä‘oÃ¡n bá»‡nh tiá»ƒu Ä‘Æ°á»ng (Diabetes Prediction)
			KNN cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ dá»± Ä‘oÃ¡n kháº£ nÄƒng máº¯c bá»‡nh tiá»ƒu Ä‘Æ°á»ng dá»±a trÃªn cÃ¡c Ä‘áº·c Ä‘iá»ƒm sinh há»c nhÆ° tuá»•i tÃ¡c, chá»‰ sá»‘ BMI, huyáº¿t Ã¡p, vÃ  cÃ¡c thÃ´ng sá»‘ khÃ¡c. Báº±ng cÃ¡ch so sÃ¡nh bá»‡nh nhÃ¢n má»›i vá»›i cÃ¡c bá»‡nh nhÃ¢n Ä‘Ã£ biáº¿t tÃ¬nh tráº¡ng bá»‡nh, KNN cÃ³ thá»ƒ Æ°á»›c tÃ­nh xÃ¡c suáº¥t máº¯c bá»‡nh cá»§a há» 
			
			Trong á»©ng dá»¥ng nÃ y, KNN cáº§n má»™t táº­p dá»¯ liá»‡u lá»›n vÃ  Ä‘a dáº¡ng Ä‘á»ƒ Ä‘áº£m báº£o ráº±ng cÃ¡c dá»± Ä‘oÃ¡n cÃ³ Ä‘á»™ chÃ­nh xÃ¡c cao, Ä‘áº·c biá»‡t khi phÃ¢n tÃ­ch trÃªn cÃ¡c nhÃ³m cÃ³ dÃ¢n sá»‘ khÃ¡c nhau 

	## PhÃ¢n tÃ­ch thá»‹ trÆ°á»ng vÃ  Ä‘á» xuáº¥t sáº£n pháº©m  
		Trong thÆ°Æ¡ng máº¡i Ä‘iá»‡n tá»­ vÃ  phÃ¢n tÃ­ch thá»‹ trÆ°á»ng, KNN cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ phÃ¢n tÃ­ch hÃ nh vi ngÆ°á»i tiÃªu dÃ¹ng vÃ  Ä‘á» xuáº¥t cÃ¡c sáº£n pháº©m phÃ¹ há»£p 
		
		### Há»‡ thá»‘ng gá»£i Ã½ sáº£n pháº©m(Product Recommendation Systems):
			KNN Ä‘Æ°á»£c sá»­ dá»¥ng trong cÃ¡c há»‡ thá»‘ng gá»£i Ã½ Ä‘á»ƒ Ä‘á»ƒ xuáº¥t sáº£n pháº©m cho ngÆ°á»i dÃ¹ng dá»±a trÃªn lá»‹ch sá»­ mua sáº¯m hoáº·c duyá»‡t web cá»§a há». VÃ­ dá»¥, má»™t há»‡ thá»‘ng cÃ³ thá»ƒ Ä‘Æ°á»£c gá»£i Ã½ sáº£n pháº©m tÆ°Æ¡ng tá»± nhÆ° nhá»¯ng gÃ¬ ngÆ°á»i dÃ¹ng Ä‘Ã£ mua hoáº·c xem trÆ°á»›c Ä‘Ã³ báº±ng cÃ¡ch tÃ¬m kiáº¿m cÃ¡c ngÆ°á»i dÃ¹ng cÃ³ hÃ nh vi tÆ°Æ¡ng tá»±. 
			Há»‡ thá»‘ng gá»£i Ã½ dá»±a trÃªn KNN thÆ°á»ng sá»­ dá»¥ng cÃ¡c Ä‘áº·c trÆ°ng nhÆ° lá»‹ch sá»­ giao dá»‹ch, táº§n suáº¥t mua hÃ ng, hoáº·c Ä‘iá»ƒm sá»‘ xáº¿p háº¡ng sáº£n pháº©m Ä‘á»ƒ Ä‘Æ°a ra cÃ¡c gá»£i Ã½ cÃ³ tÃ­nh cÃ¡ nhÃ¢n hÃ³a cao . 
			
		### PhÃ¢n Ä‘oáº¡n thá»‹ trÆ°á»ng(Market Segmentaion): 
			Trong phÃ¢n Ä‘oáº¡n thá»‹ trÆ°á»ng, KNN cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ nhÃ³m cÃ¡c khÃ¡ch hÃ ng thÃ nh cÃ¡c phÃ¢n khÃºc khÃ¡c nhau dá»±a trÃªn hÃ nh vi mua sáºµm, sá»Ÿ thÃ­ch, vÃ  cÃ¡c Ä‘áº·c Ä‘iá»ƒm nhÃ¢n kháº©u há»c khÃ¡c. Äiá»u nÃ y giÃºp cÃ¡c cÃ´ng ty táº¡o ra cÃ¡c chiáº¿n lÆ°á»£c tiáº¿p thá»‹ vÃ  sáº£n pháº©m phÃ¹ há»£p vá»›i tá»«ng phÃ¢n khÃºc khÃ¡ch hÃ ng. 
			CÃ¡c Ä‘áº·c trÆ°ng phá»• biáº¿n trong phÃ¢n Ä‘oáº¡n thá»‹ trÆ°á»ng bao gá»“m thu nháº­p, Ä‘á»™ tuá»•i, khu vá»±c Ä‘á»‹a lÃ½, tuáº§n suáº¥t mua sáº¯m, vÃ  loáº¡i sáº£n pháº©m Æ°a thÃ­ch .
			
	## Káº¿t luáº­n  
		KNN lÃ  má»™t thuáº­t toÃ¡n máº¡nh máº½ vÃ  linh hoáº¡t, cÃ³ thá»ƒ Ä‘Æ°á»£c Ã¡p dá»¥ng cho nhiá»u bÃ i toÃ¡n phÃ¢n loáº¡i vÃ  há»“i quy khÃ¡c nhau. Máº·c dÃ¹ cÃ³ má»™t sá»‘ háº¡n cháº¿ nhÆ° Ä‘á»™ phá»©c táº¡p tÃ­nh toÃ¡n cao vÃ  nháº¡y cáº£m vá»›i nhiá»…u, KNN váº«n lÃ  lá»±a chá»n tá»‘t nháº¥t khi cáº§n má»™t phÆ°Æ¡ng phÃ¡p Ä‘Æ¡n giáº£n, dá»… hiá»ƒu vÃ  cÃ³ thá»ƒ triá»ƒn khai nhanh chÃ³ng. 
		
		Äá»ƒ triá»ƒn khai KNN hiá»‡u quáº£, cáº§n pháº£i lá»±a chá»n giÃ¡ trá»‹ k má»™t cÃ¡ch cáº©n tháº­n vÃ  Ä‘áº£m báº£o ráº±ng dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c tiá»n xá»­ lÃ½ tá»‘t Ä‘á»ƒ loáº¡i bá» cÃ¡c Ä‘áº·c trÆ°ng khÃ´ng liÃªn quan hoáº·c nhiá»…u 
		
			
				
//========================================== PhÃ¢n loáº¡i dá»¯ liá»‡u =========================================

#KhÃ¡i niá»‡m 
		PhÃ¢n loáº¡i dá»¯ liá»‡u lÃ  má»™t ká»¹ thuáº­t quan trá»ng trong há»c mÃ¡y(machine learning) vÃ  trÃ­ tuá»‡ nhÃ¢n táº¡o, giÃºp chia cÃ¡c Ä‘á»‘i tÆ°á»£ng hoáº·c máº«u dá»¯ liá»‡u vÃ o cÃ¡c nhÃ³m hoáº·c nhÃ£n cá»¥ thá»ƒ. PhÃ¢n loáº¡i Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i trong nhiá»u á»©ng dá»¥ng thá»±c táº¿, cháº³ng háº¡n nhÆ° phÃ¢n loáº¡i email spam, nháº­n diá»‡n khuÃ´n máº·t, chuáº©n Ä‘oÃ¡n y khoa vÃ  nhiá»u hÆ¡n tháº¿ ná»¯a. 
		
		QuÃ¡ trÃ¬nh phÃ¢n loáº¡i bao gá»“m viá»‡c xÃ¢y dá»±ng mÃ´ hÃ¬nh dá»±a trÃªn táº­p dá»¯ liá»‡u huáº¥n luyá»‡n, sau Ä‘Ã³ sá»­ dá»¥ng mÃ´ hÃ¬nh nÃ y Ä‘á»ƒ dá»± Ä‘oÃ¡n nhÃ£n cho cÃ¡c dá»¯ liá»‡u má»›i. CÃ¡c mÃ´ hÃ¬nh phÃ¢n loáº¡i phá»• biáº¿n nhÆ° phÃ¢n loáº¡i nhá»‹ phÃ¢n, phÃ¢n loáº¡i Ä‘a lá»›p, phÃ¢n loáº¡i nhiá»u nhÃ£n, phÃ¢n loáº¡i thá»© tá»±, phÃ¢n loáº¡i chuá»—i thá»i gian, phÃ¢n loáº¡i Ä‘á»“ thá»‹. 
		
		
		
# PhÃ¢n loáº¡i nhá»‹ phÃ¢n (Binary Classification)

	## KhÃ¡i niá»‡m 
		PhÃ¢n loáº¡i nhá»‹ phÃ¢n lÃ  trÆ°á»ng há»£p Ä‘Æ¡n giáº£n nháº¥t cá»§a phÃ¢n loáº¡i, nÆ¡i dá»¯ liá»‡u Ä‘Æ°á»£c phÃ¢n thÃ nh hai nhÃ³m. VÃ­ dá»¥ trong má»™t há»‡ thá»‘ng phÃ¡t hiá»‡n email spam, má»—i email Ä‘Æ°á»£c phÃ¢n loáº¡i vÃ o trong má»™t trong hai nhÃ³m spam hoáº·c khÃ´ng spam 
		
		
	## MÃ´ hÃ¬nh toÃ¡n há»c 
		MÃ´ hÃ¬nh phÃ¢n loáº¡i nhá»‹ phÃ¢n cÃ³ thá»ƒ Ä‘Æ°á»£c biá»ƒu diá»…n báº±ng hÃ m tuyáº¿n tÃ­nh nhÆ° sau : 
		
					y = o(w^T . x + b )
				
				x lÃ  vector Ä‘áº·c trÆ°ng cá»§a máº«u dá»¯ liá»‡u 
				w lÃ  vector trá»ng sá»‘ cá»§a mÃ´ hÃ¬nh  
				b lÃ  háº±ng sá»‘ Ä‘iá»u chá»‰nh (bias )
				o  lÃ  hÃ m sigmoid, dÃ¹ng Ä‘á»ƒ chuáº©n hÃ³a Ä‘áº§u ra thÃ nh xÃ¡c suáº¥t tá»« 0 Ä‘áº¿n 1 
				
	## á»¨ng dá»¥ng vá»›i Pytorch 
			PhanLoaiDuLieuByPytorch.py
		
			Trong vÃ­ dá»¥ nÃ y , chÃºng ta xÃ¢y dá»±ng má»™t mÃ´ hÃ¬nh Ä‘Æ¡n giáº£n Ä‘á»ƒ phÃ¢n loáº¡i dá»¯ liá»‡u XOR thÃ nh hai lá»›p. Dá»¯ liá»‡u XOR lÃ  má»™t vÃ­ dá»¥ kinh Ä‘iá»ƒn,nÆ¡i mÃ´ hÃ¬nh tuyáº¿n tÃ­nh Ä‘Æ¡n giáº£n khÃ´ng thá»ƒ phÃ¢n loáº¡i chÃ­nh xÃ¡c. Tuy nhiÃªn vá»›i káº¿t ná»‘i Ä‘áº§y Ä‘á»§ vÃ  hÃ m kÃ­ch hoáº¡t sigmoid, mÃ´ hÃ¬nh cÃ³ thá»ƒ há»c cÃ¡ch phÃ¢n loáº¡i Ä‘Ãºng sau khi Ä‘Æ°á»£c huáº¥n luyá»‡n 
			
			
			
# PhÃ¢n loáº¡i Ä‘a lá»›p(Multiclass Classification)
	## KhÃ¡i niá»‡m  
		PhÃ¢n loáº¡i Ä‘a lá»›p lÃ  má»™t má»Ÿ rá»™ng cá»§a phÃ¢n loáº¡i nhá»‹ phÃ¢n, nÆ¡i dá»¯ liá»‡u cÃ³ thá»ƒ Ä‘Æ°á»£c phÃ¢n loáº¡i vÃ o nhiá»u nhÃ³m hoáº·c nhÃ£n khÃ¡c nhau. VÃ­ dá»¥ trong bÃ i toÃ¡n phÃ¢n loáº¡i hÃ¬nh áº£nh, chÃºng ta cÃ³ thá»ƒ cáº§n phÃ¢n loáº¡i cÃ¡c bá»©c áº£nh thÃ nh nhiá»u loáº¡i khÃ¡c nhau, cháº¯ng háº¡n nhÆ° chÃ³ , mÃ¨o ,  chim 
		
		
	## MÃ´ hÃ¬nh toÃ¡n há»c 
		HÃ m máº¥t mÃ¡t Ä‘Æ°á»£c sá»­ dá»¥ng phá»• biáº¿n nháº¥t trong phÃ¢n loáº¡i Ä‘a lá»›p lÃ  Cross- Entropy Loss, Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a nhÆ° sau  
				
			L = â€“ \sum_{i=1}^{C} y_i \log(\hat{y}_i)
			
			C lÃ  sá»‘ lÆ°á»£ng cÃ¡c lá»›p  (classes)
			y_i lÃ  nhÃ£n thá»±c táº¿  (dáº¡ng one-hot)
			\hat{y}_i lÃ  xÃ¡c suáº¥t dá»± Ä‘oÃ¡n cho lá»›p y 
		
	## á»¨ng dá»¥ng vá»›i Pytorch  
		ÄÃ¢y lÃ  vÃ­ dá»¥ vá» cÃ¡ch xÃ¢y dá»±ng mÃ´ hÃ¬nh phÃ¢n loáº¡i Ä‘a lá»›p báº±ng PyTorch 
			MulticlassClassificationPlotByPyTorch.py

# PhÃ¢n loáº¡i nhiá»u nhÃ£n(Multi-label Classication)
	
	## KhÃ¡i niá»‡m  
		PhÃ¢n loáº¡i nhiá»u nhÃ£n lÃ  trÆ°á»ng há»£p mÃ  má»—i máº«u dá»¯ liá»‡u cÃ³ thá»ƒ Ä‘Æ°á»£c gÃ¡n vÃ o nhiá»u nhÃ£n cÃ¹ng má»™t lÃºc. Äiá»u nÃ y khÃ¡c vá»›i phÃ¢n loáº¡i Ä‘a lá»›p, nÆ¡i má»—i máº«u chá»‰ Ä‘Æ°á»£c gÃ¡n vÃ o má»™t lá»›p. VÃ­ dá»¥, má»™t bÃ i bÃ¡o cÃ¡o cÃ³ thá»ƒ Ä‘Æ°á»£c gÃ¡n nhiá»u nhÃ£n nhÆ° "thá»ƒ thao" , "bÃ³ng Ä‘Ã¡" , "quá»‘c táº¿"
		
		
	## MÃ´ hÃ¬nh toÃ¡n há»c 
		MÃ´ hÃ¬nh phá»• biáº¿n cho phÃ¢n loáº¡i nhiá»u nhÃ£n lÃ  huáº¥n luyá»‡n má»™t bá»™ phÃ¢n loáº¡i nhá»‹ phÃ¢n cho tá»«ng nhÃ£n má»™t cÃ¡ch Ä‘á»™c láº­p. Cho tá»«ng nhÃ£n l, ta xÃ¢y dá»±ng má»™t hÃ m phÃ¢n loáº¡i nhá»‹ phÃ¢n  
			
				f_l: \mathbb{R}^d \rightarrow \{0, 1\}

			f_l(x) = 1 náº¿u máº«u x thuá»™c vá» lá»›p l 
			f_l(x) = 0 náº¿u máº«u x khÃ´ng thuá»™c vá» lá»›p l 
			
			
		Do Ä‘Ã³, mÃ´ hÃ¬nh phÃ¢n loáº¡i nhiá»u nhÃ£n cÃ³ thá»ƒ Ä‘Æ°á»£c biá»ƒu diá»…n dÆ°á»›i dáº¡ng: 
			
			\hat{Y} = f(x) = \left\{ l \in \{1, 2, \dots, L\} \mid f_l(x) = 1 \right\}
			
			
	## á»¨ng dá»¥ng vá»›i Pytorch  
			file  
				MultilabelClassificationPlotByPyTorch.py



# PhÃ¢n loáº¡i thá»© tá»±(Ordinal Classification)

	## KhÃ¡i niá»‡m  
		Trong lÄ©nh vá»±c há»c mÃ¡y vÃ  trÃ­ tuá»‡ nhÃ¢n táº¡o, phÃ¢n loáº¡i thá»© tá»±(Ordinal Classication) lÃ  má»™t phÆ°Æ¡ng phÃ¡p phÃ¢n loáº¡i Ä‘áº·c biá»‡t, nÆ¡i cÃ¡c nhÃ£n(labels) khÃ´ng chá»‰ Ä‘Æ¡n thuáº§n lÃ  cÃ¡c lá»›p con riÃªng biá»‡t mÃ  cÃ²n cÃ³ má»™t thá»© tá»± rÃµ rÃ ng giá»¯a chÃºng. KhÃ¡c vá»›i phÃ¢n loáº¡i Ä‘a lá»›p(multiclass classification), nÆ¡i cÃ¡c lá»›p khÃ´ng cÃ³ thá»© tá»± cá»‘ Ä‘á»‹nh, phÃ¢n loáº¡i thá»© tá»± Ä‘Ã²i há»i pháº£i tÃ´n trong má»—i quan há»‡ thá»© tá»± giá»¯a cÃ¡c lá»›p. Äiá»u nÃ y cÃ³ nghÄ©a lÃ  cÃ¡c lá»›p cÃ³ má»‘i liÃªn há»‡ vá» máº·t Ä‘á»‹nh lÆ°á»£ng, cháº³ng háº¡n nhÆ° "tháº¥p" , "trung bÃ¬nh" ,"cao"
		
	## MÃ´ hÃ¬nh toÃ¡n há»c 
		Giáº£ sá»­ chÃºng ta cÃ³ cÃ¡c lá»›p thá»© tá»± {1,2,...,L} nÆ¡i lá»›p 1 lÃ  tháº¥p nháº¥t vÃ  lá»›p L lÃ  cao nháº¥t. Äá»ƒ mÃ´ hÃ¬nh hÃ³a bÃ i toÃ¡n phÃ¢n loáº¡i thá»© tá»±, ta thÆ°á»ng sá»­ dá»¥ng cÃ¡c mÃ´ hÃ¬nh há»“i quy hoáº·c mÃ´ hÃ¬nh phÃ¢n loáº¡i vá»›i má»™t sá»‘ Ä‘iá»u chá»‰nh Ä‘á»ƒ pháº£n Ã¡nh thá»© tá»± cá»§a cÃ¡c lá»›p. 


	## HÃ m máº¥t mÃ¡t (Loss Function)
		
		HÃ m máº¥t mÃ¡t cho phÃ¢n loáº¡i thá»© tá»± cÃ³ thá»ƒ Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a Ä‘á»ƒ pháº£n Ã¡nh thá»© tá»± giá»¯a cÃ¡c lá»›p. Má»™t vÃ­ dá»¥ lÃ  hÃ m máº¥t mÃ¡t Ä‘á»™ chÃ­nh xÃ¡c trung bÃ¬nh (mean absolute error) giá»¯a dá»± Ä‘oÃ¡n vÃ  thá»±c táº¿ 
				\mathcal{L}(\theta) = \frac{1}{N} \sum_{i=1}^{N} \left| \hat{y}_i â€“ y_i \right|
				
				
			Trong Ä‘Ã³  
				\hat{y}_i lÃ  dá»± Ä‘oÃ¡n cá»§a mÃ´ hÃ¬nh cho máº«u i 
				y_i			lÃ  lá»›p thá»±c táº¿ cá»§a máº«u i , vÃ  nÃ³ Ä‘Æ°á»£c Ã¡nh xáº¡ thÃ nh giÃ¡ trá»‹ sá»‘ Ä‘á»ƒ pháº£n Ã¡nh thá»© tá»± 
				
				
	## HÃ m kÃ­ch hoáº¡t(Actication Function )
		CÃ¡c mÃ´ hÃ¬nh phÃ¢n loáº¡i thá»© tá»± cÃ³ thá»ƒ sá»­ dá»¥ng hÃ m kÃ­ch hoáº¡t(actication function) nhÆ° hÃ m softmax Ä‘á»ƒ dá»± Ä‘oÃ¡n xÃ¡c suáº¥t cá»§a tá»«ng lá»›p, sau Ä‘Ã³ Ã¡p dá»¥ng cÃ¡c Ä‘iá»u chá»‰nh Ä‘á»ƒ pháº£n Ã¡nh thá»© tá»±. HÃ m softmax cho xÃ¡c suáº¥t cá»§a tá»«ng lá»›p Ä‘Æ°á»£c tÃ­nh nhÆ° sau  
			\hat{p}_l = \frac{e^{z_l}}{\sum_{k=1}^{L} e^{z_k}}

			Trong Ä‘Ã³  
				z_l	lÃ  Ä‘áº§u ra cá»§a mÃ´ hÃ¬nh cho lá»›p l 
				\hat{p}_l	lÃ  xÃ¡c suáº¥t dá»± Ä‘oÃ¡n cá»§a lá»›p  l 
				

	## Sá»­ dá»¥ng mÃ´ hÃ¬nh há»“i quy 
		MÃ´ hÃ¬nh há»“i quy thá»© tá»± cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ dá»± Ä‘oÃ¡n Ä‘iá»ƒm sá»‘ hoáº·c giÃ¡ trá»‹ liÃªn tá»¥c cho má»—i máº«u, sau Ä‘Ã³ phÃ¢n loáº¡i Ä‘iá»ƒm sá»‘ Ä‘Ã³ thÃ nh cÃ¡c lá»›p thá»© tá»±. VÃ­ dá»¥, má»™t mÃ´ hÃ¬nh há»“i quy cÃ³ thá»ƒ dá»± Ä‘oÃ¡n má»™t giÃ¡ trá»‹ liÃªn tá»¥c y , vÃ  sau Ä‘Ã³ giÃ¡ trá»‹ nÃ y Ä‘Æ°á»£c Ã¡nh xáº¡ vÃ o lá»›p thá»© tá»± báº±ng cÃ¡ch sá»­ dá»¥ng cÃ¡c ngÆ°á»¡ng 
	
			\text{Class}(y) = \begin{cases}
			1 & \text{náº¿u } y \leq \theta_1 \\
			2 & \text{náº¿u } \theta_1 < y \leq \theta_2 \\ \vdots \\ L & \text{náº¿u } y > \theta_{L-1}
			\end{cases}
						
	
	
		trong Ä‘Ã³ 
				\theta_1, \theta_2, \ldots, \theta_{L-1}	 lÃ  cÃ¡c ngÆ°á»¡ng phÃ¢n chia cÃ¡c lá»›p.
	
	
	## á»¨ng dá»¥ng vá»›i Pytorch 
			OrdinalClassificationPlotByPyTorch.py
	


# PhÃ¢n loáº¡i chuá»—i thá»i gian (Time Series Classification)

	## KhÃ¡i niá»‡m
		PhÃ¢n loáº¡i chuá»—i thá»i gian (Time Series Classification) lÃ  má»™t phÆ°Æ¡ng phÃ¡p phÃ¢n loáº¡i trong lÄ©nh vá»±c há»c mÃ¡y, nÆ¡i dá»¯ liá»‡u Ä‘áº§u vÃ o lÃ  cÃ¡c chuá»—i thá»i gian, tá»©c lÃ  má»™t táº­p há»£p cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u Ä‘Æ°á»£c sáº¯p xáº¿p theo thá»© tá»± thá»i gian. CÃ¡c chuá»—i thá»i gian cÃ³ thá»ƒ Ä‘áº¡i diá»‡n cho báº¥t ká»³ hiá»‡n tÆ°á»£ng nÃ o thay Ä‘á»•i theo thá»i gian, cháº³ng háº¡n nhÆ° giÃ¡ cá»• phiáº¿u, tin hiá»‡u tÃ¢m Ä‘á»“(ECG), hoáº·c dá»¯ liá»‡u cáº£m biáº¿n  
		
	## PhÆ°Æ¡ng phÃ¡p phÃ¢n loáº¡i 
		PhÃ¢n loáº¡i chuá»—i thá»i gian cÃ³ thá»ƒ Ä‘Æ°á»£c thá»±c hiá»‡n báº±ng nhiá»u phÆ°Æ¡ng phÃ¡p khÃ¡c nhau bao gá»“m  
			
			
		### PhÆ°Æ¡ng phÃ¡p dá»±a trÃªn Ä‘áº·c trÆ°ng 
			TrÃ­ch xuáº¥t cÃ¡c Ä‘áº·c trÆ°ng tá»« chuá»—i thá»i gian, cháº³ng háº¡n nhÆ° giÃ¡ trá»‹ trung bÃ¬nh , Ä‘á»™ lá»‡ch chuáº©n, hoáº·c cÃ¡c chá»‰ sá»‘ phá»©c táº¡p hÆ¡n nhÆ° cÃ¡c Ä‘áº·c trÆ°ng táº§n sá»‘, sau Ä‘Ã³ sá»­ dá»¥ng cÃ¡c mÃ´ hÃ¬nh phÃ¢n loáº¡i truyá»n thá»‘ng nhÆ° SVM hoáº·c RandomForest 

		### PhÆ°Æ¡ng phÃ¡p dá»±a trÃªn mÃ´ hÃ¬nh 
			Sá»­ dá»¥ng cÃ¡c mÃ´ hÃ¬nh nhÆ° HMM(Hidden Markov Model) hoáº·c ARIMA(Autoregressive Integrated Moving Average) Ä‘á»ƒ phÃ¢n tÃ­ch vÃ  phÃ¢n loáº¡i chuá»—i thá»i gian 
		
		
		### PhÆ°Æ¡ng phÃ¡p Máº¡ng NÆ¡-ron há»“i quy(Recurrent Neural Networks - RNNs)
			RNNs, Ä‘áº·c biá»‡t lÃ  LSTM(Long Short-Term Memory), mÃ  má»™t trong nhá»¯ng phÆ°Æ¡ng phÃ¡p hiá»‡n Ä‘áº¡i vÃ  nhiá»‡u quáº£ nháº¥t Ä‘á»ƒ phÃ¢n loáº¡i chuá»—i thá»i gian. Nhá»¯ng mÃ´ hÃ¬nh nÃ y cÃ³ kháº£ nÄƒng há»c tá»« cÃ¡c chuá»—i dá»¯ liá»‡u cÃ³ Ä‘á»™ dÃ i biáº¿n Ä‘á»•i vÃ  ghi nhá»› cÃ¡c thÃ´ng tin quan trá»ng qua thá»i gian dÃ i. 
			
			
	## á»¨ng dá»¥ng vá»›i Pytorch 
		Má»™t vÃ­ dá»¥ thá»±c táº¿ vá» phÃ¢n loáº¡i chuá»—i thá»i gian lÃ  phÃ¢n loáº¡i cÃ¡c tÃ­n hiá»‡u Ä‘iá»‡n tÃ¢m Ä‘á»“(ECG) Ä‘á»ƒ chuáº©n Ä‘oÃ¡n cÃ¡c bá»‡nh tim máº¡ch. Trong vÃ­ dá»¥ dÆ°á»›i Ä‘Ã¢y, chÃºng ta sáº½ sá»­ dá»¥ng PyTorch Ä‘á»ƒ xÃ¢y dá»±ng má»™t mÃ´ hÃ¬nh RNN Ä‘Æ¡n giáº£n nháº±m phÃ¢n loáº¡i cÃ¡c chuá»—i thá»i gian thÃ nh cÃ¡c lá»›p khÃ¡c nhau dá»±a trÃªn máº«u tÃ­n hiá»‡u  
		
			
			TimeSeriesClassificationByPyTorch.py




# PhÃ¢n loáº¡i Ä‘á»“ thá»‹ (Graph Classification)

	# KhÃ¡i niá»‡m 
		PhÃ¢n loáº¡i Ä‘á»“ thá»‹(Graph Classification) lÃ  má»™t phÆ°Æ¡ng phÃ¡p trong há»c mÃ¡y liÃªn quan Ä‘áº¿n viá»‡c gÃ¡n nhÃ£n cho toÃ n bá»™ Ä‘á»“ thá»‹. ÄÃ¢y lÃ  má»™t bÃ i toÃ¡n quan trá»ng trong nhiá»u lÄ©nh vá»±c, tá»« hÃ³a há»c, sinh há»c Ä‘áº¿n khoa há»c xÃ£ há»™i, nÆ¡i cÃ¡c Ä‘á»‘i tÆ°á»£ng nghiÃªn cá»©u cÃ³ thá»ƒ Ä‘Æ°á»£c mÃ´ hÃ¬nh hÃ³a dÆ°á»›i dáº¡ng Ä‘á»“ thá»‹. VÃ­ dá»¥, má»™t pháº§n tá»­ cÃ³ thá»ƒ Ä‘Æ°á»£c biá»ƒu diá»…n dÆ°á»›i dáº¡ng Ä‘á»“ thá»‹ vá»›i cÃ¡c nguyÃªn tá»­ lÃ  cÃ¡c Ä‘á»‰nh(nodes) vÃ  cÃ¡c liÃªn káº¿t hÃ³a há»c lÃ  cÃ¡c cáº¡nh(edges)
		
		
	# PhÆ°Æ¡ng phÃ¡p phÃ¢n loáº¡i 
		CÃ³ nhiá»u phÆ°Æ¡ng phÃ¡p Ä‘á»ƒ phÃ¢n loáº¡i Ä‘á»“ thá»‹, bao gá»“m: 
			
			
		## PhÆ°Æ¡ng phÃ¡p dá»±a trÃªn Ä‘áº·c trÆ°ng 
			TrÃ­ch xuáº¥t cÃ¡c Ä‘áº·c trÆ°ng cá»§a Ä‘á»“ thá»‹ nhÆ° sá»‘ Ä‘á»‰nh, sá»‘ cáº¡nh, hoáº·c cÃ¡c chá»‰ sá»‘ khÃ¡c nhÆ° Ä‘á»™ táº­p trung(clustering coefficient), sau Ä‘Ã³ sá»­ dá»¥ng cÃ¡c thuáº­t toÃ¡n há»c mÃ¡y truyá»n thá»‘ng nhÆ° SVM hoáº·c Random Forest Ä‘á»ƒ phÃ¢n loáº¡i. 
			
		## Máº¡ng nÆ¡-ron Ä‘á»“ thá»‹ (Graph Neural Networks - GNNs)
		
			GNNs lÃ  phÆ°Æ¡ng phÃ¡p hiá»‡n Ä‘áº¡i vÃ  máº¡nh máº½ Ä‘á»ƒ xá»­ lÃ½ vÃ  phÃ¢n loáº¡i Ä‘á»“ thá»‹. ChÃºng há»c cÃ¡ch biá»ƒu diá»…n cÃ¡c Ä‘á»“ thá»‹ dÆ°á»›i dáº¡ng vector sá»‘(embedding) vÃ  sá»­ dá»¥ng cÃ¡c vector nÃ y Ä‘á»ƒ thá»±c hiá»‡n phÃ¢n loáº¡i. 
			
	## á»¨ng dá»¥ng vá»›i Pytorch 
		Má»™t vÃ­ dá»¥ thá»±c táº¿ vá» phÃ¢n loáº¡i Ä‘á»“ thá»‹ lÃ  dá»± Ä‘oÃ¡n tÃ­nh cháº¥t hÃ³a há»c cá»§a cÃ¡c phÃ¢n tá»­ trong hÃ³a há»c. Má»—i phÃ¢n tá»­ cÃ³ thá»ƒ Ä‘Æ°á»£c biá»ƒu diá»…n dÆ°á»›i dáº¡ng má»™t Ä‘á»“ thá»‹, vá»›i cÃ¡c nguyÃªn tá»­ lÃ  cÃ¡c Ä‘á»‰nh vÃ  cÃ¡c liÃªn káº¿t hÃ³a há»c lÃ  cÃ¡c cáº¡nh. Má»¥c tiÃªu lÃ  xÃ¢y dá»±ng má»™t mÃ´ hÃ¬nh há»c mÃ¡y cÃ³ thá»ƒ dá»± Ä‘oÃ¡n cÃ¡c tÃ­nh cháº¥t cá»§a phÃ¢n tá»­ dá»±a trÃªn cáº¥u trÃºc Ä‘á»“ thá»‹ cá»§a nÃ³. 
		
		DÆ°á»›i Ä‘Ã¢y lÃ  má»™t vÃ­ dá»¥ sá»­ dá»¥ng Pytorch Geometric, má»™t thÆ° viá»‡n má»Ÿ rá»™ng cá»§a PyTorch dÃ nh riÃªng cho viá»‡c xá»­ lÃ½ Ä‘á»“ thá»‹. 
			GraphClassificationByPytorch.py
		
			
		Trong vÃ­ dá»¥ nÃ y chÃºng ta sá»­ dá»¥ng PyTorch Geometric Ä‘á»ƒ xÃ¢y dá»±ng má»™t mÃ´ hÃ¬nh GCN(Graph Convolutional Network)		 nháº±m phÃ¢n loáº¡i cÃ¡c Ä‘á»“ thá»‹ trong bá»™ dá»¯ liá»‡u MUTAG, má»™t bá»™ dá»¯ liá»‡u phá»• biáº¿n trong phÃ¢n loáº¡i Ä‘á»“ thá»‹. MÃ´ hÃ¬nh GCN há»c cÃ¡ch biá»ƒu diá»…n cÃ¡c Ä‘á»“ thá»‹ dÆ°á»›i dáº¡ng cÃ¡c vector sá»‘ vÃ  sá»­ dá»¥ng cÃ¡c vector nÃ y Ä‘á»ƒ dá»± Ä‘oÃ¡n lá»›p cá»§a Ä‘á»“ thá»‹. 
		

# Káº¿t luáº­n 
	PhÃ¢n loáº¡i dá»¯ liá»‡u lÃ  má»™t trong nhá»¯ng nhiá»‡m vá»¥ cá»‘t lÃµi trong há»c mÃ¡y, vá»›i á»©ng dá»¥ng rá»™ng rÃ£i trong nhiá»u lá»‹ch vá»±c khÃ¡c nhau. Dá»±a trÃªn tÃ­nh cháº¥t vÃ  cáº¥u trÃºc dá»¯ liá»‡u, cÃ³ nhiá»u loáº¡i phÃ¢n loáº¡i khÃ¡c nhau, má»—i loáº¡i Ä‘á» cÃ³ nhá»¯ng Ä‘áº·c Ä‘iá»ƒm vÃ  á»©ng dá»¥ng riÃªng  
	
	## PhÃ¢n loáº¡i nhá»‹ phÃ¢n 
		ÄÃ¢y lÃ  loáº¡i phÃ¢n loáº¡i Ä‘Æ¡n giáº£n nháº¥t , nÆ¡i dá»¯ liá»‡u Ä‘Æ°á»£c chia thÃ nh 2 lá»›p khÃ¡c nhau, NÃ³ thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng trong cÃ¡c bÃ i toÃ¡n nhÆ° phÃ¡t hiá»‡n spam, phÃ¢n loáº¡i email hoáº·c dá»± Ä‘oÃ¡n bá»‡nh táº­t 
		
	## PhÃ¢n loáº¡i Ä‘a lá»›p 
		Khi cÃ³ nhiá»u hÆ¡n 2 lá»›p, bÃ i toÃ¡n trá»Ÿ thÃ nh phÃ¢n loáº¡i Ä‘a lá»›p. NÃ³ Ä‘Æ°á»£c á»©ng dá»¥ng rá»™ng rÃ£i trong nháº­n diá»‡n hÃ¬nh áº£nh, phÃ¢n loáº¡i vÄƒn báº£n, vÃ  nhiá»u bÃ i toÃ¡n khÃ¡c trong thá»±c táº¿ 
		
	## PhÃ¢n loáº¡i nhiá»u nhÃ£n  
		ÄÃ¢y lÃ  khi má»™t máº«u dá»¯ liá»‡u cÃ³ thá»ƒ thuá»™c vá» nhiá»u lá»›p cÃ¹ng lÃºc. VÃ­ dá»¥ Ä‘iá»ƒn hÃ¬nh trong phÃ¢n loáº¡i áº£nh, má»™t bá»©c áº£nh cÃ³ thá»ƒ chá»©a nhiá»u Ä‘á»‘i tÆ°á»£ng khÃ¡c nhua nhÆ° ngÆ°á»i, xe , Ä‘á»™ng váº­t, vÃ  má»—i Ä‘á»‘i tÆ°á»£ng Ä‘á»u lÃ  má»™t nhÃ£n 
		
	## PhÃ¢n loáº¡i thá»© tá»±  
		PhÃ¢n loáº¡i thá»© tá»±(Ordinal Classification) lÃ  khi cÃ¡c lá»›p cÃ³ má»™t thá»© tá»± nháº¥t Ä‘á»‹nh, vÃ­ dá»¥ nhÆ° má»©c Ä‘á»™ hÃ i lÃ²ng tá»« ráº¥t khÃ´ng hÃ i lÃ²ng Ä‘áº¿n ráº¥t hÃ i lÃ²ng. Äiá»u nÃ y thÆ°á»ng xuáº¥t hiá»‡n trong cÃ¡c bÃ i toÃ¡n kháº£o sÃ¡t vÃ  phÃ¢n tÃ­ch dá»¯ liá»‡u khÃ¡ch hÃ ng . 
		
	## PhÃ¢n loáº¡i chuá»—i thá»i gian  
		Äá»‘i vá»›i dá»¯ liá»‡u mÃ  cÃ¡c máº«u Ä‘Æ°á»£c sáº¯p xáº¿p theo thá»i gian, phÃ¢n loáº¡i chuá»—i thá»i gian giÃºp phÃ¡t hiá»‡n cÃ¡c máº«u vÃ  xu hÆ°á»›ng trong dá»¯ liá»‡u thay Ä‘á»•i theo thá»i gian, vá»›i á»©ng dá»¥ng trong y táº¿, tÃ i chÃ­nh vÃ  IoT 
		
	## PhÃ¢n loáº¡i Ä‘á»“ thá»‹  
		Khi dá»¯ liá»‡u Ä‘Æ°á»£c biá»ƒu diá»…n dÆ°á»›i dáº¡ng Ä‘á»“ thá»‹, phÃ¢n loáº¡i Ä‘á»“ thá»‹ cho phÃ©p chÃºng ta phÃ¢n tÃ­ch cÃ¡c Ä‘á»‘i tÆ°á»£ng cÃ³ cáº¥u trÃºc phá»©c táº¡p nhÆ° phÃ¢n tá»­ hÃ³a há»c, máº¡ng xÃ£ há»™i, hoáº·c há»‡ thá»‘ng káº¿t ná»‘i máº¡ng 
		
		
	Má»—i phÃ¢n loáº¡i Ä‘á»u cÃ³ nhá»¯ng Æ°u vÃ  nhÆ°á»£c Ä‘iá»ƒm riÃªng, cÅ©ng nhÆ° cÃ¡c phÆ°Æ¡ng phÃ¡p vÃ  ká»¹ thuáº­t phÃ¹ há»£p. Viá»‡c chá»n lá»±a loáº¡i phÃ¢n loáº¡i vÃ  phÆ°Æ¡ng phÃ¡p phÃ¹ há»£p phá»¥ thuá»™c vÃ o báº£n cháº¥t cá»§a dá»¯ liá»‡u vÃ  má»¥c tiÃªu cá»§a bÃ i toÃ¡n. Sá»± phÃ¡t triá»ƒn cá»§a cÃ¡c phÆ°Æ¡ng phÃ¡p há»c mÃ¡y, Ä‘áº·c biá»‡t lÃ  cÃ¡c máº¡ng nÆ¡-ron sÃ¢u, Ä‘Ã£ má»Ÿ ra nhiá»u cÆ¡ há»™i má»›i trong viá»‡c xá»­ lÃ½ vÃ  phÃ¢n loáº¡i dá»¯ liá»‡u phá»©c táº¡p, giÃºp cáº£i thiá»‡n hiá»‡u suáº¥t vÃ  kháº£ nÄƒng dá»± Ä‘oÃ¡n trong thá»±c táº¿. 
	
//============================ Thuáº­t toÃ¡n Random Forest ============================// 

# Giá»›i thiá»‡u 
	Random Forest lÃ  má»™t thuáº­t toÃ¡n há»c mÃ¡y thuá»™c nhÃ³m há»c cÃ³ giÃ¡m sÃ¡t(supervised learning ) vÃ  Ä‘Æ°á»£c sá»­ dá»¥ng phá»• biáº¿n trong cÃ¡c bÃ i toÃ¡n phÃ¢n loáº¡i (classification) vÃ  há»“i quy(regression). Thuáº­t toÃ¡n nÃ y lÃ  má»™t dáº¡ng cá»§a táº­p há»£p há»c(enmsemble learning), nÆ¡i mÃ  nhiá»u mÃ´ hÃ¬nh yáº¿u (weak learners), cá»¥ thá»ƒ lÃ  cÃ¡c cÃ¢y quyáº¿t Ä‘á»‹nh(decision trees), Ä‘Æ°á»£c káº¿t há»£p láº¡i Ä‘á»ƒ táº¡o thÃ nh má»™t mÃ´ hÃ¬nh máº¡nh máº½ hÆ¡n. 
		https://aicandy.vn/wp-content/uploads/2024/09/aicandy_randomforest_1.png
		
	## Táº¡i sao láº¡i lÃ  Random ?
		Thuáº­t ngá»¯ Random trong Random Forest xuáº¥t phÃ¡t tá»« hai yáº¿u tá»‘ chÃ­nh : 
		
		### 1. Ngáº«u nhiÃªn trong chá»n máº«u 
			Thay vÃ¬ sá»­ dá»¥ng toÃ n bá»™ dá»¯ liá»‡u huáº¥n luyá»‡n Ä‘á»ƒ xÃ¢y dá»±ng tá»«ng cÃ¢y quyáº¿t Ä‘á»‹nh, thuáº­t toÃ¡n Random Forest chá»n má»™t máº«u ngáº«u nhiÃªn tá»« táº­p dá»¯ liá»‡u(vá»›i hoÃ n láº¡i) Ä‘á»ƒ xÃ¢y dá»±ng má»—i cÃ¢y. Ká»¹ thuáº­t nÃ y Ä‘Æ°á»£c gá»i lÃ  Bagging (Bootstrap Aggregating). Bagging giÃºp giáº£m thiá»ƒu phÆ°Æ¡ng sai cá»§a mÃ´ hÃ¬nh, cáº£i thiá»‡n Ä‘á»™ chÃ­nh xÃ¡c tá»•ng thá»ƒ.
			
		### 2. Ngáº«u nhiÃªn trong chá»n Ä‘áº·c trÆ°ng 
			Khi táº¡o cÃ¡c nÃºt trong má»—i cÃ¢y, chá»‰ má»™t táº­p con ngáº«u nhiÃªn cá»§a táº¥t cáº£ cÃ¡c Ä‘áº·c trÆ°ng Ä‘Æ°á»£c xem xÃ©t Ä‘á»ƒ chá»n Ä‘áº·c trÆ°ng tá»‘t nháº¥t táº¡i má»—i bÆ°á»›c. Äiá»u nÃ y giÃºp cÃ¢y quyáº¿t Ä‘á»‹nh Ä‘a dáº¡ng hÆ¡n, giáº£m thiá»ƒu hiá»‡n tÆ°á»£ng overfitting vÃ  Ä‘áº£m báº£o ráº±ng cÃ¡c cÃ¢y khÃ´ng phá»¥ thuá»™c quÃ¡ má»©c vÃ o má»™t Ä‘áº·c trÆ°ng cá»¥ thá»ƒ nÃ o Ä‘Ã³. 
			
			
# CÆ¡ cháº¿ hoáº¡t Ä‘á»™ng 

	## Giá»›i thiá»‡u 
		Random Forest bao gá»“m nhiá»u cÃ¢y quyáº¿t Ä‘á»‹nh (Decision Trees). Má»—i cÃ¢y quyáº¿t Ä‘á»‹nh lÃ  má»™t mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n Ä‘á»™c láº­p vÃ  Ä‘Æ°a ra má»™t dá»± Ä‘oÃ¡n. Äá»‘i vá»›i bÃ i toÃ¡n phÃ¢n loáº¡i, Random Forest sáº½ láº¥y káº¿t quáº£ dá»± Ä‘oÃ¡n cá»§a tá»«ng cÃ¢y vÃ  chá»n káº¿t quáº£ nÃ o xuáº¥t hiá»‡n nhiá»u nháº¥t(majority vote). Äá»‘i vá»›i bÃ i toÃ¡n há»“i quy, káº¿t quáº£ cuá»‘i cÃ¹ng lÃ  giÃ¡ trá»‹ trung bÃ¬nh cá»§a cÃ¡c dá»± Ä‘oÃ¡n tá»« táº¥t cáº£ cÃ¡c cÃ¢y. 
		
		VÃ­ dá»¥, giáº£ sá»­ chÃºng ta cÃ³ 100 cÃ¢y quyáº¿t Ä‘á»‹nh. Äá»‘i vá»›i má»™t máº«u má»›i, náº¿u 60 cÃ¢y dá»± Ä‘oÃ¡n ráº±ng máº«u Ä‘Ã³ thuá»™c lá»›p A< vÃ  40 cÃ¢y dá»± Ä‘oÃ¡n ráº±ng máº«u Ä‘Ã³ thuá»™c lá»›p B, thÃ¬ Random Forest sáº½ dá»± Ä‘oÃ¡n ráº±ng máº«u Ä‘Ã³ thuá»™c lá»›p A(vÃ¬ nÃ³ nháº­n Ä‘Æ°á»£c sá»‘ phiáº¿u cao hÆ¡n)
		
		
	## CÃ´ng thá»©c tá»•ng quÃ¡t 
		Má»™t cÃ¢y quyáº¿t Ä‘á»‹nh trong Random Forest thá»±c hiá»‡n phÃ¢n loáº¡i hoáº·c há»“i quy báº±ng cÃ¡ch chia nhá» khÃ´ng gian Ä‘áº·c trÆ°ng thÃ nh cÃ¡c vÃ¹ng con. CÃ¡c phÃ¢n vÃ¹ng nÃ y Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh dá»±a trÃªn cÃ¡c Ä‘iá»u kiá»‡n phÃ¢n tÃ¡ch táº¡i má»—i nÃºt trong cÃ¢y. Giáº£ sá»­ cÃ³ má»™t Ä‘áº·c trÆ°ng X vÃ  má»™t ngÆ°á»¡ng phÃ¢n tÃ¡ch t trong viá»‡c phÃ¢n tÃ¡ch táº¡i má»™t nÃºt cÃ³ thá»ƒ Ä‘Æ°á»£c biá»ƒu diá»…n báº±ng cÃ¡ch chá»n má»™t hÃ m chá»‰ thá»‹ I, trong Ä‘Ã³ 
					I(X \leq t) \text{ vÃ  } I(X > t)
					
		Náº¿u Ä‘áº·c trÆ°ng X táº¡i máº«u Ä‘Ã³ nhá» hÆ¡n hoáº·c báº±ng t, máº«u sáº½ Ä‘Æ°á»£c chuyá»ƒn Ä‘áº¿n nhÃ¡nh trÃ¡i. ngÆ°á»£c láº¡i, nÃ³ sáº½ Ä‘Æ°á»£c chuyá»ƒn Ä‘áº¿n nhÃ¡nh pháº£i. QuÃ¡ trÃ¬nh nÃ y tiáº¿p tá»¥c cho Ä‘áº¿n khi Ä‘áº¡t Ä‘áº¿n má»™t nÃºt lÃ¡, nÆ¡i giÃ¡ trá»‹ Ä‘áº§u ra cá»§a nÃºt Ä‘Ã³ Ä‘Æ°á»£c sá»­ dá»¥ng lÃ m dá»± Ä‘oÃ¡n. 
		
		
					
# HÃ m máº¥t mÃ¡t vÃ  Ä‘á»™ khÃ´ng thuáº§n khiáº¿t 

	QuÃ¡ trÃ¬nh xÃ¢y dá»±ng cÃ¢y quyáº¿t Ä‘á»‹nh trong Random Forest liÃªn quan Ä‘áº¿n viá»‡c tá»‘i Æ°u hÃ³a má»™t hÃ m máº¥t mÃ¡t, thÆ°á»ng lÃ  giáº£m thiá»ƒu Ä‘á»™ khÃ´ng thuáº§n khiáº¿t(impurity) cá»§a cÃ¡c nÃºt. Äá»‘i vá»›i bÃ i toÃ¡n phÃ¢n loáº¡i, Ä‘á»™ khÃ´ng thuáº§n khiáº¿t thÆ°á»ng Ä‘Æ°á»£c Ä‘o báº±ng chá»‰ sá»‘ Gini hoáº·c entropy 
	
	## Chá»‰ sá»‘ Gini  
			Chá»‰ sá»‘ Gini lÃ  má»™t cÃ¡ch Ä‘o lÆ°á»ng Ä‘á»™ khÃ´ng thuáº§n khiáº¿t cá»§a má»™t nÃºt. CÃ´ng thá»©c tÃ­nh chá»‰ sá»‘ Gini cho má»™t nÃºt t lÃ  : 
							Gini(t) = 1 â€“ \sum_{i=1}^{C} p_i^2
							
			Trong Ä‘Ã³, p_i lÃ  xÃ¡c suáº¥t cá»§a viá»‡c má»™t máº«u thuá»™c lá»›p i táº¡i nÃºt t, vÃ  C lÃ  tá»•ng sá»‘ lá»›p. Má»™t nÃºt thuáº§n khiáº¿t(tá»©c lÃ  táº¥t cáº£ cÃ¡c máº«u Ä‘á»u thuá»™c má»™t lá»›p) sáº½ cÃ³ chá»‰ sá»‘ Gini báº±ng 0 . 
			
	## Entropy 
		Entropy lÃ  má»™t thÆ°á»›c Ä‘o khÃ¡c vá» Ä‘á»™ khÃ´ng thuáº§n khiáº¿t, vÃ  nÃ³ Ä‘Æ°á»£c sá»­ dá»¥ng trong viá»‡c xÃ¢y dá»±ng cÃ¢y quyáº¿t Ä‘á»‹nh theo phÆ°Æ¡ng phÃ¡p ID3 hoáº·c C4.5 . CÃ´ng thá»©c tÃ­nh entropy táº¡i má»™t nÃºt t lÃ  : 
		
					Entropy(t) = â€“ \sum_{i=1}^{C} p_i \log_2(p_i)
		
		TÆ°Æ¡ng tá»± nhÆ° chá»‰ sá»‘ Gini , entropy Ä‘áº¡t giÃ¡ trá»‹ nhá» nháº¥t khi nÃºt hoÃ n toÃ n thuáº§n khiáº¿t. 
		
		
# Giáº£m Ä‘á»™ khÃ´ng thuáº§n khiáº¿t (Information Gain)

	Khi má»™t Ä‘áº·c trÆ°ng Ä‘Æ°á»£c chá»n Ä‘á»ƒ phÃ¢n tÃ¡ch táº¡i má»™t nÃºt, má»¥c tiÃªu lÃ  lÃ m giáº£m Ä‘á»™ khÃ´ng thuáº§n khiáº¿t cá»§a cÃ¡c nÃºt con so vá»›i nÃºt cha. Sá»± giáº£m nÃ y, Ä‘Æ°á»£c gá»i lÃ  Information Gain(Äá»‘i vá»›i entropy) hoáº·c Gini Gain(Ä‘á»‘i vá»›i chá»‰ sá»‘ Gini ), Ä‘Æ°á»£c tÃ­nh nhÆ° sau : 
				\text{Information Gain} = \text{Impurity}(t) â€“ \sum_{k \in \{left, right\}} \frac{N_k}{N} \text{Impurity}(t_k)
				
	Trong Ä‘Ã³ : 
		- \text{Impurity}(t)   :  lÃ  Ä‘á»™ khÃ´ng thuáº§n khiáº¿t táº¡i nÃºt cha 
		- \text{Impurity}(t_k) :  LÃ  Ä‘á»™ khÃ´ng thuáº§n khiáº¿t táº¡i cÃ¡c nÃºt con sau khi phÃ¢n tÃ¡ch.
		- N lÃ  sá»‘ lÆ°á»£ng ngáº«u nhiÃªn táº¡i nÃºt cha, vÃ  Nk lÃ  sá»‘ lÆ°á»£ng máº«u táº¡i nÃºt con k . 
		
# Bagging vÃ  ngáº«u nhiÃªn hÃ³a 
	Trong Random Forest, hai khÃ­a cáº¡nh quan trá»ng cá»§a tÃ­nh ngáº«u nhiÃªn giÃºp tÄƒng cÆ°á»ng kháº£ nÄƒng tá»•ng quÃ¡t hÃ³a cá»§a mÃ´ hÃ¬nh : 
	
	
	## Bagging 
		Thuáº­t toÃ¡n Random Forest sá»­ dá»¥ng phÆ°Æ¡ng phÃ¡p Bagging( Bootstrap Aggregating) Ä‘á»ƒ xÃ¢y dá»±ng má»—i cÃ¢y quyáº¿t Ä‘á»‹nh. Thay vÃ¬ sá»­ dá»¥ng toÃ n bá»™ táº­p dá»¯ liá»‡u huáº¥n luyá»‡n, má»—i cÃ¢y Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn má»™t máº«u ngáº«u nhiÃªn tá»« táº­p dá»¯ liá»‡u, vá»›i viá»‡c láº¥y máº«u cÃ³ hoÃ n láº¡i(tá»©c lÃ  má»™t máº«u cÃ³ thá»ƒ Ä‘Æ°á»£c chá»n nhiá»u láº§n )
			https://aicandy.vn/wp-content/uploads/2024/09/aicandy_random_forest.png
			
			
	## Ngáº«u nhiÃªn hÃ³a Ä‘áº·c trÆ°ng 
		Táº¡i má»—i bÆ°á»›c chia tÃ¡ch trong cÃ¢y, má»™t táº­p con ngáº«u nhiÃªn cá»§a cÃ¡c Ä‘áº·c trÆ°ng Ä‘Æ°á»£c xem xÃ©t Ä‘á»ƒ tÃ¬m Ä‘áº·c trÆ°ng tá»‘t nháº¥t. Äiá»u nÃ y lÃ m cho má»—i cÃ¢y khÃ¡c biá»‡t hÆ¡n vÃ  giáº£m thiá»ƒu sá»± phá»¥ thuá»™c vÃ o má»™t sá»‘ Ä‘áº·c trÆ°ng cá»¥ thá»ƒ, tá»« Ä‘Ã³ giáº£m nguy cÆ¡ overfitting. 
		
	## CÃ´ng thá»©c Bagging  
		Giáº£ sá»­ cÃ³ B cÃ¢y quyáº¿t Ä‘á»‹nh vÃ  Ä‘Æ°á»£c xÃ¢y dá»±ng tá»« cÃ¡c bá»™ dá»¯ liá»‡u bootstrap khÃ¡c nhau. Dá»± Ä‘oÃ¡n cá»§a Random Forest Ä‘á»‘i vá»›i má»™t máº«u má»›i x lÃ  : 
		
			### Äá»‘i vá»›i bÃ i toÃ¡n phÃ¢n loáº¡i  
					\hat{y} = \text{majority_vote}\left(\hat{y}^{(1)}, \hat{y}^{(2)}, \dots, \hat{y}^{(B)}\right)
					
			### Äá»‘i vá»›i bÃ i toÃ¡n há»“i quy  
					\hat{y} = \frac{1}{B} \sum_{b=1}^{B} \hat{y}^{(b)}
		

			Trong Ä‘Ã³  : 
				\hat{y}^{(b)} : lÃ  dá»± Ä‘oÃ¡n cá»§a cÃ¢y quyáº¿t Ä‘á»‹nh thá»© b cho máº«u x 
				
				B lÃ  tá»•ng sá»‘ cÃ¢y 
				


# ÄÃ¡nh giÃ¡ má»©c Ä‘á»™ quan trá»ng cá»§a Ä‘áº·c trÆ°ng 
	Trong Random Forest, má»©c Ä‘á»™ quan trá»ng cá»§a cÃ¡c Ä‘áº·c trÆ°ng Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ dá»±a trÃªn má»©c giáº£m Ä‘á»™ khÃ´ng thuáº§n khiáº¿t (impurity) mÃ  Ä‘áº·c trÆ°ng Ä‘Ã³ Ä‘Ã³ng gÃ³p khi Ä‘Æ°á»£c chá»n lÃ m Ä‘áº·c trÆ°ng phÃ¢n tÃ¡ch. Äá»‘i vá»›i má»—i cÃ¢y, tá»•ng má»©c giáº£m impurity trÃªn toÃ n bá»™ cÃ¢y Ä‘Æ°á»£c tÃ­nh cho má»—i Ä‘áº·c trÆ°ng, vÃ  sau Ä‘Ã³ Ä‘Æ°á»£c trung bÃ¬nh hÃ³a qua táº¥t cáº£ cÃ¡c cÃ¢y trong rá»«ng.

	Má»™t cÃ¡ch khÃ¡c Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ táº§m quan trá»ng cá»§a Ä‘áº·c trÆ°ng lÃ  sá»­ dá»¥ng phÆ°Æ¡ng phÃ¡p Permuted Feature Importance, trong Ä‘Ã³ cÃ¡c giÃ¡ trá»‹ cá»§a má»™t Ä‘áº·c trÆ°ng cá»¥ thá»ƒ Ä‘Æ°á»£c xÃ¡o trá»™n ngáº«u nhiÃªn vÃ  má»©c giáº£m trong Ä‘á»™ chÃ­nh xÃ¡c cá»§a mÃ´ hÃ¬nh Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ má»©c Ä‘á»™ quan trá»ng cá»§a Ä‘áº·c trÆ°ng Ä‘Ã³.

	TÃ³m láº¡i, má»©c Ä‘á»™ quan trá»ng cá»§a má»™t Ä‘áº·c trÆ°ng 
	cÃ³ thá»ƒ Ä‘Æ°á»£c tÃ­nh báº±ng cÃ´ng thá»©c:
		\text{Feature Importance}(X_j) = \frac{1}{B} \sum_{b=1}^{B} \sum_{t \in \text{nodes}} \Delta I(t)
		
		Trong Ä‘Ã³:
		\Delta I(t) : lÃ  má»©c giáº£m impurity táº¡i nÃºt t khi sá»­ dá»¥ng Ä‘áº·c trÆ°ng X_j
		B0			: lÃ  tá»•ng sá»‘ cÃ¢y trong rá»«ng.
		\text{nodes}:  lÃ  táº­p há»£p cÃ¡c nÃºt trong cÃ¢y b nÆ¡i X_j Ä‘Æ°á»£c sá»­ dá»¥ng.
		
# 	Out-of-Bag Error
	Má»™t Ä‘áº·c Ä‘iá»ƒm ná»•i báº­t cá»§a Random Forest lÃ  kháº£ nÄƒng Æ°á»›c tÃ­nh lá»—i dá»± Ä‘oÃ¡n mÃ  khÃ´ng cáº§n tÃ¡ch riÃªng má»™t táº­p dá»¯ liá»‡u kiá»ƒm tra, thÃ´ng qua khÃ¡i niá»‡m lá»—i Out-of-Bag (OOB). Trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n, khoáº£ng má»™t pháº§n ba máº«u trong má»—i bootstrap khÃ´ng Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ huáº¥n luyá»‡n cÃ¢y vÃ  do Ä‘Ã³, cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng nhÆ° má»™t táº­p kiá»ƒm tra tá»± nhiÃªn.
	
		\text{OOB Error} = \frac{1}{N} \sum_{i=1}^{N} I(\hat{y}_{OOB}(i) \neq y_i)

	Trong Ä‘Ã³: 
		
	\hat{y}_{OOB}(i)	: lÃ  dá»± Ä‘oÃ¡n cá»§a mÃ´ hÃ¬nh cho máº«u i khi máº«u nÃ y lÃ  out-of-bag cho cÃ¢y mÃ  dá»± Ä‘oÃ¡n nÃ y Ä‘Æ°á»£c thá»±c hiá»‡n.
	 y_i				:  lÃ  giÃ¡ trá»‹ thá»±c táº¿ cá»§a máº«u i 
	 N 					: lÃ  tá»•ng sá»‘ máº«u trong táº­p dá»¯ liá»‡u.
	 I(.)				: lÃ  hÃ m chá»‰ thá»‹, nháº­n giÃ¡ trá»‹ 1 náº¿u Ä‘iá»u kiá»‡n trong ngoáº·c Ä‘Æ¡n lÃ  Ä‘Ãºng vÃ  0 náº¿u Ä‘iá»u kiá»‡n lÃ  sai. 
		
		
	VÃ­ dá»¥ sá»­ dá»¥ng python Ä‘á»ƒ tÃ­nh toÃ¡n OOB:
				from sklearn.ensemble import RandomForestClassifier
				from sklearn.datasets import make_classification
				from sklearn.model_selection import train_test_split
				from sklearn.metrics import accuracy_score
				
				# Táº¡o dá»¯ liá»‡u giáº£ láº­p
				X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_classes=2, random_state=42)
				X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
				
				# XÃ¢y dá»±ng mÃ´ hÃ¬nh Random Forest vá»›i lá»—i OOB
				model = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42)
				model.fit(X_train, y_train)
				
				# TÃ­nh toÃ¡n Ä‘á»™ chÃ­nh xÃ¡c OOB
				oob_error = 1 - model.oob_score_
				print(f'OOB Error: {oob_error:.4f}')
				
				
# 	 á»¨ng dá»¥ng 		

	Random Forest Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i trong nhiá»u lÄ©nh vá»±c khÃ¡c nhau nhá» vÃ o tÃ­nh chÃ­nh xÃ¡c vÃ  kháº£ nÄƒng tá»•ng quÃ¡t hÃ³a tá»‘t. DÆ°á»›i Ä‘Ã¢y lÃ  má»™t sá»‘ á»©ng dá»¥ng tiÃªu biá»ƒu:

	##  PhÃ¢n loáº¡i vÃ  phÃ¡t hiá»‡n gian láº­n (Fraud Detection) 
		Trong lÄ©nh vá»±c tÃ i chÃ­nh, Random Forest Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ phÃ¡t hiá»‡n cÃ¡c giao dá»‹ch gian láº­n. VÃ­ dá»¥, cÃ¡c giao dá»‹ch tháº» tÃ­n dá»¥ng cÃ³ thá»ƒ Ä‘Æ°á»£c phÃ¢n loáº¡i dá»±a trÃªn cÃ¡c Ä‘áº·c trÆ°ng nhÆ° sá»‘ tiá»n giao dá»‹ch, táº§n suáº¥t giao dá»‹ch, Ä‘á»‹a Ä‘iá»ƒm giao dá»‹ch, vÃ  lá»‹ch sá»­ giao dá»‹ch trÆ°á»›c Ä‘Ã³. Random Forest cÃ³ kháº£ nÄƒng phÃ¡t hiá»‡n nhá»¯ng giao dá»‹ch báº¥t thÆ°á»ng vÃ  Ä‘Æ°a ra cáº£nh bÃ¡o vá» kháº£ nÄƒng gian láº­n.
				
	## 	Y táº¿ vÃ  chuáº©n Ä‘oÃ¡n bá»‡nh
		Trong y há»c, Random Forest há»— trá»£ chuáº©n Ä‘oÃ¡n bá»‡nh báº±ng cÃ¡ch phÃ¢n loáº¡i cÃ¡c bá»‡nh nhÃ¢n dá»±a trÃªn cÃ¡c triá»‡u chá»©ng vÃ  káº¿t quáº£ xÃ©t nghiá»‡m. VÃ­ dá»¥, Ä‘á»‘i vá»›i bá»‡nh ung thÆ°, Random Forest cÃ³ thá»ƒ sá»­ dá»¥ng cÃ¡c Ä‘áº·c trÆ°ng nhÆ° kÃ­ch thÆ°á»›c khá»‘i u, máº­t Ä‘á»™ táº¿ bÃ o, vÃ  cÃ¡c yáº¿u tá»‘ di truyá»n Ä‘á»ƒ dá»± Ä‘oÃ¡n liá»‡u má»™t khá»‘i u cÃ³ Ã¡c tÃ­nh hay khÃ´ng.

		VÃ­ dá»¥ khÃ¡c, trong dá»± Ä‘oÃ¡n bá»‡nh tiá»ƒu Ä‘Æ°á»ng, Random Forest cÃ³ thá»ƒ sá»­ dá»¥ng cÃ¡c chá»‰ sá»‘ nhÆ° má»©c Ä‘Æ°á»ng huyáº¿t lÃºc Ä‘Ã³i, chá»‰ sá»‘ khá»‘i cÆ¡ thá»ƒ (BMI), vÃ  tiá»n sá»­ gia Ä‘Ã¬nh Ä‘á»ƒ dá»± Ä‘oÃ¡n kháº£ nÄƒng máº¯c bá»‡nh.		
				
				
	## 	Dá»± Ä‘oÃ¡n thá»‹ trÆ°á»ng chá»©ng khoÃ¡n 
			
		Random Forest Ä‘Æ°á»£c á»©ng dá»¥ng trong phÃ¢n tÃ­ch vÃ  dá»± Ä‘oÃ¡n xu hÆ°á»›ng thá»‹ trÆ°á»ng chá»©ng khoÃ¡n. Báº±ng cÃ¡ch xá»­ lÃ½ cÃ¡c dá»¯ liá»‡u lá»‹ch sá»­ giÃ¡ cá»• phiáº¿u, thÃ´ng tin kinh táº¿, vÃ  cÃ¡c chá»‰ sá»‘ ká»¹ thuáº­t, Random Forest cÃ³ thá»ƒ dá»± Ä‘oÃ¡n xu hÆ°á»›ng tÄƒng giáº£m cá»§a cá»• phiáº¿u hoáº·c chá»‰ sá»‘ thá»‹ trÆ°á»ng. Äiá»u nÃ y cÃ³ thá»ƒ giÃºp cÃ¡c nhÃ  Ä‘áº§u tÆ° Ä‘Æ°a ra cÃ¡c quyáº¿t Ä‘á»‹nh giao dá»‹ch dá»±a trÃªn cÃ¡c dá»± bÃ¡o Ä‘Ã¡ng tin cáº­y.


	## PhÃ¢n tÃ­ch hÃ¬nh áº£nh vÃ  nháº­n dáº¡ng khuÃ´n máº·t 
		Trong thá»‹ giÃ¡c mÃ¡y tÃ­nh, Random Forest Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ phÃ¢n loáº¡i cÃ¡c Ä‘á»‘i tÆ°á»£ng trong hÃ¬nh áº£nh vÃ  nháº­n dáº¡ng khuÃ´n máº·t. Báº±ng cÃ¡ch phÃ¢n tÃ­ch cÃ¡c Ä‘áº·c trÆ°ng nhÆ° mÃ u sáº¯c, káº¿t cáº¥u, vÃ  cÃ¡c Ä‘iá»ƒm ná»•i báº­t, Random Forest cÃ³ thá»ƒ phÃ¢n loáº¡i cÃ¡c Ä‘á»‘i tÆ°á»£ng trong hÃ¬nh áº£nh vá»›i Ä‘á»™ chÃ­nh xÃ¡c cao. Trong nháº­n dáº¡ng khuÃ´n máº·t, thuáº­t toÃ¡n cÃ³ thá»ƒ xÃ¡c Ä‘á»‹nh danh tÃ­nh cá»§a má»™t ngÆ°á»i dá»±a trÃªn cÃ¡c Ä‘áº·c Ä‘iá»ƒm khuÃ´n máº·t, ngay cáº£ trong cÃ¡c Ä‘iá»u kiá»‡n Ã¡nh sÃ¡ng vÃ  gÃ³c Ä‘á»™ khÃ¡c nhau.
		
		
	## Dá»± bÃ¡o thá»i tiáº¿t vÃ  khÃ­ háº­u
		Random Forest cÅ©ng Ä‘Æ°á»£c sá»­ dá»¥ng trong cÃ¡c mÃ´ hÃ¬nh dá»± bÃ¡o thá»i tiáº¿t vÃ  khÃ­ háº­u. Thuáº­t toÃ¡n nÃ y giÃºp dá»± Ä‘oÃ¡n nhiá»‡t Ä‘á»™, lÆ°á»£ng mÆ°a, vÃ  cÃ¡c hiá»‡n tÆ°á»£ng thá»i tiáº¿t khÃ¡c dá»±a trÃªn dá»¯ liá»‡u thu tháº­p tá»« nhiá»u nguá»“n khÃ¡c nhau. Random Forest cÃ³ kháº£ nÄƒng xá»­ lÃ½ tá»‘t dá»¯ liá»‡u khÃ´ng Ä‘á»“ng nháº¥t vÃ  phá»©c táº¡p, cháº³ng háº¡n nhÆ° dá»¯ liá»‡u vá»‡ tinh vÃ  dá»¯ liá»‡u cáº£m biáº¿n tá»« cÃ¡c tráº¡m khÃ­ tÆ°á»£ng.
		
	
# Æ¯u Ä‘iá»ƒm vÃ  nhÆ°á»£c Ä‘iá»ƒm cá»§a Random Forest 
	
	## Æ¯u Ä‘iá»ƒm
		### Kháº£ nÄƒng xá»­ lÃ½ dá»¯ liá»‡u lá»›n
			Random Forest cÃ³ thá»ƒ xá»­ lÃ½ má»™t lÆ°á»£ng lá»›n dá»¯ liá»‡u vá»›i Ä‘á»™ chÃ­nh xÃ¡c cao, ngay cáº£ khi dá»¯ liá»‡u chá»©a nhiá»u nhiá»…u hoáº·c cÃ³ sá»± phÃ¢n bá»‘ khÃ´ng Ä‘á»“ng Ä‘á»u.
			
		### Giáº£m thiá»ƒu overfitting
			Báº±ng cÃ¡ch káº¿t há»£p nhiá»u cÃ¢y quyáº¿t Ä‘á»‹nh, Random Forest giÃºp giáº£m thiá»ƒu hiá»‡n tÆ°á»£ng overfitting, Ä‘áº·c biá»‡t lÃ  khi cÃ¡c cÃ¢y quyáº¿t Ä‘á»‹nh Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn cÃ¡c máº«u ngáº«u nhiÃªn khÃ¡c nhau.
			
		### Kháº£ nÄƒng xá»­ lÃ½ dá»¯ liá»‡u máº¥t mÃ¡t
			Thuáº­t toÃ¡n cÃ³ thá»ƒ hoáº¡t Ä‘á»™ng tá»‘t ngay cáº£ khi má»™t pháº§n dá»¯ liá»‡u bá»‹ thiáº¿u, vÃ¬ má»—i cÃ¢y chá»‰ sá»­ dá»¥ng má»™t pháº§n dá»¯ liá»‡u vÃ  cÃ¡c Ä‘áº·c trÆ°ng khÃ¡c nhau.
			
		### Dá»… dÃ ng Ä‘iá»u chá»‰nh vÃ  má»Ÿ rá»™ng
			Sá»‘ lÆ°á»£ng cÃ¢y trong Random Forest cÃ³ thá»ƒ dá»… dÃ ng Ä‘iá»u chá»‰nh Ä‘á»ƒ cÃ¢n báº±ng giá»¯a Ä‘á»™ chÃ­nh xÃ¡c vÃ  hiá»‡u suáº¥t tÃ­nh toÃ¡n. NgoÃ i ra, thuáº­t toÃ¡n nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c má»Ÿ rá»™ng Ä‘á»ƒ xá»­ lÃ½ cÃ¡c bÃ i toÃ¡n phÃ¢n loáº¡i nhiá»u lá»›p hoáº·c há»“i quy Ä‘a Ä‘áº§u ra.
			
	
	## NhÆ°á»£c Ä‘iá»ƒm 
		### TÃ­nh toÃ¡n phá»©c táº¡p 
			Random Forest yÃªu cáº§u nhiá»u tÃ i nguyÃªn tÃ­nh toÃ¡n hÆ¡n so vá»›i má»™t sá»‘ thuáº­t toÃ¡n khÃ¡c do sá»‘ lÆ°á»£ng cÃ¢y lá»›n vÃ  quÃ¡ trÃ¬nh huáº¥n luyá»‡n phá»©c táº¡p.

		### KhÃ³ tá»‘i Æ°u hÃ³a cÃ¡c siÃªu tham sá»‘ 
			Viá»‡c tÃ¬m kiáº¿m vÃ  tá»‘i Æ°u hÃ³a cÃ¡c siÃªu tham sá»‘ cá»§a Random Forest, cháº³ng háº¡n nhÆ° sá»‘ lÆ°á»£ng cÃ¢y, Ä‘á»™ sÃ¢u tá»‘i Ä‘a cá»§a cÃ¢y, vÃ  sá»‘ lÆ°á»£ng Ä‘áº·c trÆ°ng Ä‘á»ƒ phÃ¢n tÃ¡ch táº¡i má»—i nÃºt, cÃ³ thá»ƒ lÃ  má»™t quÃ¡ trÃ¬nh tá»‘n thá»i gian vÃ  phá»©c táº¡p. Viá»‡c lá»±a chá»n cÃ¡c siÃªu tham sá»‘ tá»‘t nháº¥t thÆ°á»ng yÃªu cáº§u thá»­ nghiá»‡m vÃ  Ä‘iá»u chá»‰nh nhiá»u láº§n.
			
		### KÃ­ch thÆ°á»›c mÃ´ hÃ¬nh lá»›n 
			Khi sá»‘ lÆ°á»£ng cÃ¢y lá»›n, kÃ­ch thÆ°á»›c cá»§a mÃ´ hÃ¬nh Random Forest cÃ³ thá»ƒ trá»Ÿ nÃªn ráº¥t lá»›n, Ä‘Ã²i há»i nhiá»u bá»™ nhá»› Ä‘á»ƒ lÆ°u trá»¯ vÃ  quáº£n lÃ½.
			
			
# á»¨ng dá»¥ng vá»›i PyTorch 
	DÆ°á»›i Ä‘Ã¢y lÃ  vÃ­ dá»¥ vá» cÃ¡ch xÃ¢y dá»±ng má»™t mÃ´ hÃ¬nh Random Forest Ä‘Æ¡n giáº£n Ä‘á»ƒ phÃ¢n loáº¡i dá»¯ liá»‡u sá»­ dá»¥ng PyTorch vÃ  scikit-learn. Máº·c dÃ¹ PyTorch khÃ´ng cÃ³ triá»ƒn khai Random Forest, chÃºng ta cÃ³ thá»ƒ sá»­ dá»¥ng scikit-learn Ä‘á»ƒ xÃ¢y dá»±ng mÃ´ hÃ¬nh vÃ  sau Ä‘Ã³ chuyá»ƒn dá»¯ liá»‡u vÃ o PyTorch
		

	## Táº¡o dá»¯ liá»‡u giáº£ láº­p 
			import torch
			from sklearn.datasets import make_classification
			from sklearn.model_selection import train_test_split
			
			# Táº¡o dá»¯ liá»‡u giáº£ láº­p
			X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_classes=2, random_state=42)
			X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
			
			# Chuyá»ƒn Ä‘á»•i sang tensor
			X_train = torch.tensor(X_train, dtype=torch.float32)
			y_train = torch.tensor(y_train, dtype=torch.long)
			X_test = torch.tensor(X_test, dtype=torch.float32)
			y_test = torch.tensor(y_test, dtype=torch.long)

			
		á» bÆ°á»›c nÃ y chÃºng ta táº¡o ra má»™t bá»™ dá»¯ liá»‡u giáº£ láº­p vá»›i 1000 máº«u, má»—i máº«u cÃ³ 20 Ä‘áº·c trÆ°ng, trong Ä‘Ã³ cÃ³ 10 Ä‘áº·c trÆ°ng mang thÃ´ng tin quan trá»ng chon viá»‡c phÃ¢n loáº¡i. Dá»¯ liá»‡u sau Ä‘Ã³ Ä‘Æ°á»£c chia thÃ nh táº­p huáº¥n luyá»‡n vÃ  táº­p kiá»ƒm tra vá»›i tá»· lá»‡ 80-20
		

	## XÃ¢y dá»±ng mÃ´ hÃ¬nh 
		Random Forest khÃ´ng Ä‘Æ°á»£c tÃ­ch há»£p sáºµn trong PyTorch, nhÆ°ng ta cÃ³ thá»ƒ sá»­ dá»¥ng thÆ° viá»‡n scikit-lean Ä‘á»ƒ xÃ¢y dá»±ng mÃ´ hÃ¬nh vÃ  sau Ä‘Ã³ chuyá»ƒn dá»¯ liá»‡u vÃ o PyTorch Ä‘á»ƒ huáº¥n luyá»‡n  
		
		from sklearn.ensemble import RandomForestClassifier
		from sklearn.metrics import accuracy_score
		
		# XÃ¢y dá»±ng mÃ´ hÃ¬nh Random Forest
		model = RandomForestClassifier(n_estimators=100, random_state=42)
		model.fit(X_train.numpy(), y_train.numpy())
		
		# Dá»± Ä‘oÃ¡n trÃªn táº­p kiá»ƒm tra
		y_pred = model.predict(X_test.numpy())
		
		# ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh
		accuracy = accuracy_score(y_test.numpy(), y_pred)
		print(f'Accuracy: {accuracy:.4f}')
						
		
		Trong vÃ­ dá»¥ nÃ y, chÃºng ta Ä‘Ã£ xÃ¢y dá»±ng má»™t mÃ´ hÃ¬nh Random Forest vá»›i 100 cÃ¢y quyáº¿t Ä‘á»‹nh(n_estimators=100). MÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn táº­p dá»¯ liá»‡u huáº¥n luyá»‡n vÃ  sau Ä‘Ã³ Ä‘Æ°á»£c kiá»ƒm tra trÃªn táº­p dá»¯ liá»‡u kiá»ƒm tra. Cuá»‘i cÃ¹ng , Ä‘á»™ chÃ­nh xÃ¡c cá»§a mÃ´ hÃ¬nh Ä‘Æ°á»£c tÃ­nh toÃ¡n vÃ  in ra káº¿t quáº£. 
		
		
	## Tinh chá»‰nh mÃ´ hÃ¬nh 
		ChÃºng ta cÃ³ thá»ƒ cáº£i thiá»‡n mÃ´ hÃ¬nh Random Forest báº±ng cÃ¡ch Ä‘iá»u chá»‰nh cÃ¡c tham sá»‘ nhÆ° sá»‘ lÆ°á»£ng cÃ¢y(n_estimators) , Ä‘á»™ sÃ¢u tá»‘i Ä‘a cá»§a má»—i cÃ¢y(max_depth ), vÃ  sá»‘ lÆ°á»£ng Ä‘áº·c trÆ°ng Ä‘Æ°á»£c xem xÃ©t khi chia tÃ¡ch(max_features). VÃ­ dá»¥, ta cÃ³ thá»ƒ thá»­ nghiá»‡m vá»›i cÃ¡c giÃ¡ trá»‹ khÃ¡c nhau cá»§a max_depth Ä‘á»ƒ tÃ¬m ra Ä‘á»™ sÃ¢u tá»‘i Æ°u cho mÃ´ hÃ¬nh . 
		
			# Tinh chá»‰nh mÃ´ hÃ¬nh vá»›i Ä‘á»™ sÃ¢u tá»‘i Ä‘a lÃ  10
			model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)
			model.fit(X_train.numpy(), y_train.numpy())
			
			# ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh
			y_pred = model.predict(X_test.numpy())
			accuracy = accuracy_score(y_test.numpy(), y_pred)
			print(f'Accuracy with max_depth=10: {accuracy:.4f}')
		
		
		á» Ä‘Ã¢y chÃºng ta Ä‘Ã£ giá»›i háº¡n Ä‘á»™ sÃ¢u cá»§a má»—i cÃ¢y trong rá»«ng á»Ÿ má»©c 10. Äiá»u nÃ y cÃ³ thá»ƒ giÃºp giáº£m thiá»ƒu hiá»‡n tÆ°á»£ng overfitting, Ä‘áº·c biá»‡t lÃ  khi dá»¯ liá»‡u cÃ³ nhiá»…u hoáº·c chá»©a cÃ¡c máº«u khÃ´ng Ä‘áº¡i diá»‡n. 
		
		
# Káº¿t luáº­n 
			
	Random Forest lÃ  má»™t trong nhá»¯ng thuáº­t toÃ¡n há»c mÃ¡y máº¡nh máº½ vÃ  linh hoáº¡t, phÃ¹ há»£p vá»›i nhiá»u loáº¡i bÃ i toÃ¡n khÃ¡c nhau tá»« phÃ¢n loáº¡i, há»“i quy Ä‘áº¿n cÃ¡c bÃ i toÃ¡n phá»©c táº¡p hÆ¡n nhÆ° phÃ¡t hiá»‡n gian láº­n hay phÃ¢n tÃ­ch hÃ¬nh áº£nh. Máº·c dÃ¹ cÃ³ má»™t sá»‘ háº¡n cháº¿ vá» tÃ­nh phá»©c táº¡p vÃ  kháº£ nÄƒng giáº£i thÃ­ch, Random Forest váº«n Ä‘Æ°á»£c Æ°a chuá»™ng nhá» vÃ o kháº£ nÄƒng xá»­ lÃ½ dá»¯ liá»‡u lá»›n, giáº£m thiá»ƒu overfitting, vÃ  tÃ­nh dá»… dÃ ng trong viá»‡c tinh chá»‰nh mÃ´ hÃ¬nh 
	
//===================================== SVM trong xá»­ lÃ½ dá»¯ liá»‡u phi tuyáº¿n tÃ­nh: Ká»¹ thuáº­t Kernel vÃ  á»©ng dá»¥ng ===================// 

# KhÃ¡i niá»‡m 
	Trong há»c mÃ¡y, viá»‡c phÃ¢n tÃ­ch vÃ  xá»­ lÃ½ dá»¯ liá»‡u lÃ  má»™t yáº¿u tá»‘ quan trá»ng Ä‘á»ƒ xÃ¢y dá»±ng cÃ¡c mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n. Má»™t trong nhá»¯ng khÃ­a cáº¡nh phá»©c táº¡p nháº¥t cá»§a dá»¯ liá»‡u lÃ  Ä‘áº·c tÃ­nh phi tuyáº¿n tÃ­nh. Dá»¯ liá»‡u phi tuyáº¿n tÃ­nh Ä‘áº·t ra nhiá»u thÃ¡ch thá»©c cho cÃ¡c mÃ´ hÃ¬nh há»c mÃ¡y tuyáº¿n tÃ­nh truyá»n thá»‘ng. 
	
	## Dá»¯ liá»‡u tuyáº¿n tÃ­nh 
		Dá»¯ liá»‡u tuyáº¿n tÃ­nh lÃ  dá»¯ liá»‡u cÃ³ thá»ƒ phÃ¢n tÃ¡ch báº±ng má»™t Ä‘Æ°á»ng tháº³ng trong khÃ´ng gian 2D hoáº·c má»™t siÃªu pháº³ng trong khÃ´ng gian nhiá»u chiá»u. NÃ³i cÃ¡ch khÃ¡c, cÃ¡c lá»›p dá»¯ liá»‡u cÃ³ thá»ƒ Ä‘Æ°á»£c chia tÃ¡ch má»™t cÃ¡ch rÃµ ráº±ng báº±ng má»™t hÃ m tuyáº¿n tÃ­nh. 
		
	## Dá»¯ liá»‡u phi tuyáº¿n tÃ­nh 
		Dá»¯ liá»‡u phi tuyáº¿n tÃ­nh lÃ  dá»¯ liá»‡u mÃ  cÃ¡c lá»›p khÃ´ng thá»ƒ phÃ¢n tÃ¡ch báº±ng má»™t Ä‘Æ°á»ng tháº³ng hoáº·c siÃªu pháº³ng. Trong trÆ°á»ng há»£p nÃ y, viá»‡c sá»­ dá»¥ng cÃ¡c mÃ´ hÃ¬nh tuyáº¿n tÃ­nh sáº½ khÃ´ng hiá»‡u quáº£. CÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u cÃ³ thá»ƒ táº¡o thÃ nh cÃ¡c hÃ¬nh dáº¡ng phá»©c táº¡p nhÆ° vÃ²ng trÃ²n, xoáº¯n á»‘c, hoáº·c cÃ¡c bá» máº·t phi tuyáº¿n. 
		
	## Support Vector Machine(SVM)
		Support Vector Machine(SVM) lÃ  má»™t thuáº­t toÃ¡n há»c mÃ¡y thuá»™c loáº¡i supervied learning, Ä‘Æ°á»£c sá»­ dá»¥ng chá»§ yáº¿u cho cÃ¡c bÃ i toÃ¡n phÃ¢n loáº¡i vÃ  há»“i quy. Ã tÆ°á»Ÿng chÃ­nh cá»§a SVM lÃ  tÃ¬m ra má»™t siÃªu pháº³ng(hyperplane) tá»‘i Æ°u Ä‘á»ƒ phÃ¢n tÃ¡ch cÃ¡c lá»›p dá»¯ liá»‡u. Trong khÃ´ng gian hai chiá»u, siÃªu pháº³ng lÃ  má»™t Ä‘Æ°á»ng tháº³ng, cÃ²n trong khÃ´ng gian nhiá»u chiá»u, Ä‘Ã³ lÃ  má»™t máº·t pháº³ng hoáº·c siÃªu pháº³ng. 
		https://aicandy.vn/wp-content/uploads/2024/09/aicandy_SVM_1.jpg
		Trong trÆ°á»ng há»£p dá»¯ liá»‡u khÃ´ng thá»ƒ phÃ¢n tÃ¡ch tuyáº¿n tÃ­nh(dá»¯ liá»‡u phi tuyáº¿n tÃ­nh), SVM sá»­ dá»¥ng ká»¹ thuáº­t kernel Ä‘á»ƒ chuyá»ƒn dá»¯ liá»‡u tá»« khÃ´ng gian gá»‘c sang má»™t khÃ´ng gian Ä‘áº·c trÆ°ng cao hÆ¡n, nÆ¡i dá»¯ liá»‡u cÃ³ thá»ƒ trá»Ÿ thÃ nh tuyáº¿n tÃ­nh. Thay vÃ¬ tÃ­nh toÃ¡n trá»±c tiáº¿p cÃ¡c tá»a Ä‘á»™ má»›i, SVM sá»­ dá»¥ng má»™t hÃ m kernel Ä‘á»ƒ tÃ­nh toÃ¡n sáº£n pháº©m vÃ´ hÆ°á»›ng trong khÃ´ng gian Ä‘áº·c trÆ°ng Ä‘Ã³  
		
				https://aicandy.vn/wp-content/uploads/2024/09/aicandy_SVM_2.jpg

2. NguyÃªn lÃ½ hoáº¡t Ä‘á»™ng  

	## SVM cho bÃ i toÃ¡n tuyáº¿n tÃ­nh 
		Khi dá»¯ liá»‡u tuyáº¿n tÃ­nh, nghÄ©a lÃ  cÃ³ thá»ƒ phÃ¢n tÃ¡ch cÃ¡c lá»›p dá»¯ liá»‡u báº±ng má»™t Ä‘Æ°á»ng tháº³ng(trong khÃ´ng gian 2d ) hoáº·c má»™t siÃªu pháº³ng(trong khÃ´ng gian nhiá»u chiá»u), SVM sáº½ tÃ¬m cÃ¡ch xÃ¡c Ä‘á»‹nh siÃªu pháº³ng nÃ y sao cho margin(khoáº£ng cÃ¡ch) lá»›n nháº¥t giá»¯a cÃ¡c lá»›p dá»¯ liá»‡u. Äiá»u nÃ y giÃºp tá»‘i Æ°u hÃ³a kháº£ nÄƒng phÃ¢n loáº¡i vÃ  giáº£m thiá»ƒu lá»—i phÃ¢n loáº¡i. 
				https://aicandy.vn/wp-content/uploads/2024/09/aicandy_SVM_tuyentinh.jpg
				
		Giáº£ sá»­ chÃºng ta cÃ³ má»™t táº­p dá»¯ liá»‡u vá»›i hai lá»›p, trong Ä‘Ã³ má»—i Ä‘iá»ƒm dá»¯ liá»‡u x_i thuá»™c vá» má»™t trong hai lá»›p  	y_i \in \{-1, 1\}
			
		Má»¥c tiÃªu cá»§a SVM lÃ  tÃ¬m ra siÃªu pháº³ng dÆ°á»›i dáº¡ng : 
			\mathbf{w}^T \mathbf{x} + b = 0 
			
		trong Ä‘Ã³, w lÃ  vector trá»ng sá»‘ vÃ  b lÃ  bias. SiÃªu pháº³ng nÃ y pháº£i Ä‘áº£m báº£o phÃ¢n tÃ¡ch hai lá»›p dá»¯ liá»‡u má»™t cÃ¡ch chÃ­nh xÃ¡c, nghÄ©a lÃ  
						y_i (\mathbf{w}^T \mathbf{x}_i + b) \geq 1, \forall i 
						
		Khoáº£ng cÃ¡ch tá»« má»™t Ä‘iá»ƒm dá»¯ liá»‡u Ä‘áº¿n siÃªu pháº³ng Ä‘Æ°á»£c tÃ­nh báº±ng cÃ´ng thá»©c : 
				\frac{| \mathbf{w}^T \mathbf{x} + b |}{\|\mathbf{w}\|} 
				
		SVM sáº½ tÃ¬m cÃ¡ch tá»‘i Æ°u hÃ³a trá»ng sá»‘ w vÃ  bias b sao cho margin (khoáº£ng cÃ¡ch giá»¯a hai lá»›p) lÃ  lá»›n nháº¥t. CÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u gáº§n siÃªu pháº³ng nháº¥t Ä‘Æ°á»£c gá»i lÃ  support vectors, vÃ  chÃ­nh cÃ¡c Ä‘iá»ƒm nÃ y Ä‘Ã³ng vai trÃ² quan trá»ng trong viá»‡c xÃ¡c Ä‘á»‹nh siÃªu pháº³ng phÃ¢n loáº¡i  
		
		BÃ i toÃ¡n SVM tuyáº¿n tÃ­nh Ä‘Æ°á»£c biá»ƒu diá»…n nhÆ° má»™t bÃ i toÃ¡n tá»‘i Æ°u hÃ³a, vá»›i má»¥c tiÃªu lÃ  tá»‘i Æ°u hÃ³a margin giá»¯a cÃ¡c lá»›p. Ta cáº§n cá»±c tiá»ƒu hÃ³a hÃ m má»¥c tiÃªu sau 
						\min_{\mathbf{w}, b} \frac{1}{2} \|\mathbf{w}\|^2  
						
				vá»›i cÃ¡c rÃ ng buá»™c 
						y_i (\mathbf{w}^T \mathbf{x}_i + b) \geq 1, \forall i   
						
		HÃ m má»¥c tiÃªu \frac{1}{2} \|\mathbf{w}\|^2 nháº±m tá»‘i thiá»ƒu hÃ³a Ä‘á»™ lá»›n cá»§a vector trong sá»‘ w, tá»©c lÃ  tá»‘i Ä‘a hÃ³a khoáº£ng cÃ¡ch margin. BÃ i toÃ¡n nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c giáº£i thÃ­ch báº±ng cÃ¡ch sá»­ dá»¥ng cÃ¡c ká»¹ thuáº­t tá»‘i Æ°u hÃ³a báº­c hai(Quadratic Programing)
		
	
	## 2.2 SVM vá»›i SoftMargin 
		Trong thá»±c táº¿, dá»¯ liá»‡u cÃ³ thá»ƒ khÃ´ng hoÃ n toÃ¡n tuyáº¿n tÃ­nh vÃ  Ä‘Ã´i khi cÃ³ nhiá»…u, dáº«n Ä‘áº¿n viá»‡c khÃ´ng thá»ƒ phÃ¢n tÃ¡ch chÃ­nh xÃ¡c cÃ¡c lá»›p dá»¯ liá»‡u báº±ng má»™t siÃªu pháº³ng. Äá»ƒ giáº£i quyáº¿t váº¥n Ä‘á» nÃ y, SVM tuyáº¿n tÃ­nh sá»­ dá»¥ng má»™t biáº¿n sá»‘ slack \xi_i cho phÃ©p má»™t sá»‘ Ä‘iá»ƒm dá»¯ liá»‡u náº±m trong margin hoáº·c bá»‹ phÃ¢n loáº¡i sai . MÃ´ hÃ¬nh nÃ y Ä‘Æ°á»£c gá»i lÃ  Soft Margin SVM 
		
			BÃ i toÃ¡n tá»‘i Æ°u hÃ³a cá»§a Soft Margin SVM  Ä‘Æ°á»£c viáº¿t láº¡i nhÆ° sau : 
								\min_{\mathbf{w}, b, \xi} \frac{1}{2} \|\mathbf{w}\|^2 + C \sum_{i=1}^{n} \xi_i   
								
			Vá»›i cÃ¡c rÃ ng buá»™c : 
				y_i (\mathbf{w}^T \mathbf{x}_i + b) \geq 1 â€“ \xi_i, \forall i\quad \text{vÃ } \quad \xi_i \geq 0   
				
		Trong Ä‘Ã³, C lÃ  má»™t tham sá»‘ Ä‘iá»u chá»‰nh, giÃºp cÃ¢n báº±ng giá»¯a viá»‡c tá»‘i thiá»ƒu hÃ³a lá»—i phÃ¢n loáº¡i vÃ  tá»‘i Ä‘a hÃ³a margin. Khi C lá»›n, mÃ´ hÃ¬nh sáº½ cá»‘ gáº¯ng giáº£m thiá»ƒu lá»—i phÃ¢n loáº¡i, nhÆ°ng cÃ³ thá»ƒ dáº«n Ä‘áº¿n overfitting. NgÆ°á»£c láº¡i, khi C nhá», mÃ´ hÃ¬nh sáº½ táº­p trung vÃ o viá»‡c tá»‘i Ä‘a hÃ³a margin nhá»¯ng cÃ³ thá»ƒ cháº¥p nháº­n má»™t sá»‘ lá»—i phÃ¢n loáº¡i. 
				
		
		
	## 2.3 Ká»¹ thuáº­t Kernel : Giáº£i quyáº¿t váº¥n Ä‘á» phi tuyáº¿n tÃ­nh  
		Ká»¹ thuáº­t kernel lÃ  phÆ°Æ¡ng phÃ¡p chÃ­nh Ä‘á»ƒ má»Ÿ rá»™ng SVM tá»« cÃ¡c váº¥n Ä‘á» tuyáº¿n tÃ­nh sang phi tuyáº¿n tÃ­nh. Ã tÆ°á»Ÿng chÃ­nh lÃ  biáº¿n Ä‘á»•i dá»¯ liá»‡u tá»« khÃ´ng gian ban Ä‘áº§u thÃ nh má»™t khÃ´ng gian Ä‘áº·c trÆ°ng cÃ³ chiá»u cao hÆ¡n , nÆ¡i dá»¯ liá»‡u cÃ³ thá»ƒ phÃ¢n tÃ¡ch thÃ nh tuyáº¿n tÃ­nh  
			https://aicandy.vn/wp-content/uploads/2024/09/aicandy_SVM_kernel.png
	
		HÃ£y xem xÃ©t má»™t hÃ m biáº¿n Ä‘á»•i phi tuyáº¿n  \phi(\mathbf{x}), chuyá»ƒn dá»¯ liá»‡u tá»« khÃ´ng gian gá»‘c \mathbb{R}^n sang khÃ´ng gian Ä‘áº·c trÆ°ng cao hÆ¡n \mathbb{R}^m . Thay vÃ¬ tÃ­nh trá»±c tiáº¿p sáº£n pháº©n vÃ´ hÆ°á»›ng trong khÃ´ng gian Ä‘áº·c trÆ°ng, chÃºng ta sá»­ dá»¥ng má»™t hÃ m kernel K(\mathbf{x}_i, \mathbf{x}_j) thá»a mÃ£n : 
					K(\mathbf{x}_i, \mathbf{x}_j) = \langle \phi(\mathbf{x}_i), \phi(\mathbf{x}_j) \rangle 
					
				PhÆ°Æ¡ng trÃ¬nh tá»‘i Æ°u hÃ³a SVM vá»›i kernel sáº½ trá»Ÿ thÃ nh 
					\min_{\alpha} \frac{1}{2} \sum_{i,j} \alpha_i \alpha_j y_i y_j K(\mathbf{x}_i, \mathbf{x}_j) â€“ \sum_i \alpha_i
					
				vá»›i rÃ ng buá»™c 
					\sum_i \alpha_i y_i = 0 \quad \text{vÃ } \quad \alpha_i \geq 0  
					
		Má»™t sá»‘ hÃ m kernel phá»• biáº¿n bao gá»“m 

			### Kernel Ä‘a thá»©c(Polinomial Kernel ): 
				K(\mathbf{x}_i, \mathbf{x}_j) = (\mathbf{x}_i^T \mathbf{x}_j + c)^d
				
			### Kernel Gaussian hay RBG (Radial Basis Function)
				K(\mathbf{x}_i, \mathbf{x}_j) = \exp\left(-\frac{\|\mathbf{x}_i â€“ \mathbf{x}_j\|^2}{2\sigma^2}\right)
			
			### Kernel sigmoid : 
					K(\mathbf{x}_i, \mathbf{x}_j) = \tanh(\kappa \mathbf{x}_i^T \mathbf{x}_j + c)
					

# Æ¯u Ä‘iá»ƒm vÃ  nhÆ°á»£c Ä‘iá»ƒm 
	## Æ¯u Ä‘iá»ƒm cá»§a SVM 
		
		### Hiá»‡u quáº£ cao vá»›i dá»¯ liá»‡u cÃ³ sá»‘ lÆ°á»£ng Ä‘áº·c trÆ°ng lá»›n. 
			SVM hoáº¡t Ä‘á»™ng tá»‘t khi sá»‘ lÆ°á»£ng Ä‘áº·c trÆ°ng(features) cá»§a dá»¯ liá»‡u lá»›n hÆ¡n nhiá»u so vá»›i sá»‘ lÆ°á»£ng máº«u dá»¯ liá»‡u. Äiá»u nÃ y lÃ m cho SVM trá»Ÿ thÃ nh lá»±a chá»n lÃ½ tÆ°á»Ÿng cho cÃ¡c bÃ i toÃ¡n nhÆ° phÃ¢n loáº¡i vÄƒn báº£n vÃ  nháº­n dáº¡ng hÃ¬nh áº£nh, nÆ¡i sá»‘ lÆ°á»£ng Ä‘áº·c trÆ°ng thÆ°á»ng ráº¥t lá»›n. 
			
		### Hiá»‡u suáº¥t cao vá»›i dá»¯ liá»‡u phi tuyáº¿n tÃ­nh 
			Khi dá»¯ liá»‡u khÃ´ng thá»ƒ phÃ¢n tÃ¡ch tuyáº¿n tÃ­nh, SVM cÃ³ thá»ƒ sá»­ dá»¥ng cÃ¡c ká»¹ thuáº­t kernel Ä‘á»ƒ chuyá»ƒn Ä‘á»•i dá»¯ liá»‡u sang khÃ´ng gian cÃ³ thá»ƒ phÃ¢n tÃ¡ch. CÃ¡c Kernel phá»• biáº¿n bao gá»“m kernel Gaussion(RBF) , kernel Polynomial, vÃ  Sigmoid .
			
		### Tá»‘i Æ°u hÃ³a margin 
			SVM tá»‘i Ä‘a hÃ³a margin giá»¯a cÃ¡c lá»›p dá»¯ liá»‡u, giÃºp cáº£i thiá»‡n kháº£ nÄƒng tá»•ng quÃ¡t hÃ³a (generalization) cá»§a mÃ´ hÃ¬nh. Margin cÃ ng lá»›n thÃ¬ mÃ´ hÃ¬nh cÃ ng Ã­t cÃ³ nguy cÆ¡ bá»‹ overfitting vÃ  cÃ³ thá»ƒ phÃ¢n loáº¡i chÃ­nh xÃ¡c hÆ¡n trÃªn dá»¯ liá»‡u chÆ°a tá»«ng gáº·p 
			
		### Lá»i giáº£i duy nháº¥t vÃ  tá»‘i Æ°u 
			BÃ i toÃ¡n tá»‘i Æ°u hÃ³a cá»§a SVM cÃ³ lá»i giáº£i duy nháº¥t vÃ  tá»‘i Æ°u nhá» vÃ o viá»‡c sá»­ dá»¥ng ká»¹ thuáº­t tá»‘i Æ°u hÃ³a báº­c hai(Quadratic Programming). Äiá»u nÃ y giÃºp mÃ´ hÃ¬nh trá»Ÿ nÃªn á»•n Ä‘á»‹nh vÃ  dá»… giáº£i thÃ­ch. 
			
		### Xá»­ lÃ½ hiá»‡u quáº£ vá»›i outliers 
			SVM cÃ³ kháº£ nÄƒng xá»­ lÃ½ tá»‘t cÃ¡c dá»¯ liá»‡u ngoáº¡i lai (outliers) báº±ng cÃ¡ch sá»­ dá»¥ng Soft Margin. Äiá»u nÃ y cho phÃ©p mÃ´ hÃ¬nh phÃ¢n loáº¡i chÃ­nh xÃ¡c ngay cáº£ khi cÃ³ dá»¯ liá»‡u bá»‹ nhiá»…u. 
			
	## NhÆ°á»£c Ä‘iá»ƒm cá»§a SVM 
		
		### KhÃ³ khÄƒn trong viá»‡c chá»n kernel phÃ¹ há»£p 
			Má»™t trong nhá»¯ng thÃ¡ch thá»©c lá»›n nháº¥t khi sá»­ dá»¥ng SVM lÃ  lá»±a chá»n kernel phÃ¹ há»£p cho dá»¯ liá»‡u. Náº¿u chá»n kernel khÃ´ng Ä‘Ãºng, mÃ´ hÃ¬nh cÃ³ thá»ƒ khÃ´ng hoáº¡t Ä‘á»™ng tá»‘t vÃ  gÃ¢y ra hiá»‡n tÆ°á»£ng overfitting hoáº·c underfitting 
			
		### Äá»™ phá»©c táº¡p tÃ­nh toÃ¡n cao  
			SVM thÆ°á»ng yÃªu cáº§u tÃ­nh toÃ¡n phá»©c táº¡p, Ä‘áº·c biá»‡t lÃ  vá»›i cÃ¡c bá»™ dá»¯ liá»‡u lá»›n. Khi sá»‘ lÆ°á»£ng máº«u tÄƒng lÃªn, chi phÃ­ tÃ­nh toÃ¡n cá»§a SVM tÄƒng theo cáº¥p sá»‘ nhÃ¢n, Ä‘iá»u nÃ y lÃ m giáº£m tÃ­nh kháº£ thi cá»§a nÃ³ Ä‘á»‘i vá»›i cÃ¡c bÃ i toÃ¡n dá»¯ liá»‡u lá»›n.
		
		 ### Nháº¡y cáº£m vá»›i dá»¯ liá»‡u nhiá»…u
			Máº·c dÃ¹ SVM cÃ³ kháº£ nÄƒng xá»­ lÃ½ outliers tá»‘t, nhÆ°ng nÃ³ váº«n cÃ³ thá»ƒ bá»‹ áº£nh hÆ°á»Ÿng bá»Ÿi cÃ¡c dá»¯ liá»‡u nhiá»…u. Náº¿u dá»¯ liá»‡u bá»‹ nhiá»…u quÃ¡ má»©c, hiá»‡u suáº¥t cá»§a SVM cÃ³ thá»ƒ giáº£m máº¡nh.
			
		### Thá»i gian huáº¥n luyá»‡n lÃ¢u 
			Vá»›i cÃ¡c bá»™ dá»¯ liá»‡u lá»›n hoáº·c cÃ³ nhiá»u Ä‘áº·c trÆ°ng, thá»i gian huáº¥n luyá»‡n cá»§a SVM cÃ³ thá»ƒ ráº¥t lÃ¢u, Ä‘áº·c biá»‡t khi cáº§n sá»­ dá»¥ng kernel phá»©c táº¡p. Äiá»u nÃ y cÃ³ thá»ƒ lÃ  má»™t háº¡n cháº¿ lá»›n khi xá»­ lÃ½ dá»¯ liá»‡u thá»i gian thá»±c hoáº·c yÃªu cáº§u huáº¥n luyá»‡n nhanh.
			
		### KhÃ³ má»Ÿ rá»™ng vá»›i nhiá»u lá»›p 
			Máº·c dÃ¹ SVM hoáº¡t Ä‘á»™ng tá»‘t vá»›i bÃ i toÃ¡n phÃ¢n loáº¡i hai lá»›p, viá»‡c má»Ÿ rá»™ng SVM cho bÃ i toÃ¡n phÃ¢n loáº¡i Ä‘a lá»›p (multi-class classification) cÃ³ thá»ƒ trá»Ÿ nÃªn phá»©c táº¡p vÃ  kÃ©m hiá»‡u quáº£ hÆ¡n so vá»›i cÃ¡c thuáº­t toÃ¡n khÃ¡c nhÆ° Random Forest hay Gradient Boosting.
		
		
# 	á»¨ng dá»¥ng cá»§a SVM trong thá»±c táº¿	
	Suport Vector Machine(SVM) lÃ  má»™t trong nhá»¯ng thuáº­t toÃ¡n há»c mÃ¡y máº¡nh máº½, Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i trong nhiá»u lÄ©nh vá»±c khÃ¡c nhau. SVM khÃ´ng chá»‰ giá»›i háº¡n trong viá»‡c phÃ¢n loáº¡i dá»¯ liá»‡u tuyáº¿n tÃ­nh mÃ  cÃ²n má»Ÿ rá»™ng Ä‘á»ƒ xá»­ lÃ½ cÃ¡c dá»¯ liá»‡u phi tuyáº¿n tÃ­nh phá»©c táº¡p thÃ´ng qua cÃ¡c ká»¹ thuáº­t kernel. DÆ°á»›i Ä‘Ã¢y lÃ  má»™t sá»‘ á»©ng dá»¥ng ná»•i báº­t cá»§a SVM trong thá»±c táº¿. 
	
	## Nháº­n dáº¡ng hÃ¬nh áº£nh  
		SVM lÃ  má»™t cÃ´ng cá»¥ máº¡nh máº½ trong cÃ¡c bÃ i toÃ¡n nháº­n dáº¡ng hÃ¬nh áº£nh. CÃ¡c á»©ng dá»¥ng phá»• biáº¿n bao gá»“m nháº­n diá»‡n khuÃ´n máº·t, nháº­n dáº¡ng chá»¯ viáº¿t tay, vÃ  phÃ¢n loáº¡i Ä‘á»‘i tÆ°á»£ng trong áº£nh. VÃ­ dá»¥, trong há»‡ thá»‘ng nháº­n diá»‡n khuÃ´n máº·t, SVM cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ phÃ¢n loáº¡i cÃ¡c Ä‘áº·c trÆ°ng khuÃ´n máº·t thÃ nh cÃ¡c lá»›p khÃ¡c nhau, giÃºp xÃ¡c Ä‘á»‹nh danh tÃ­nh cá»§a ngÆ°á»i dÃ¹ng. 
		Má»™t vÃ­ dá»¥ khÃ¡c lÃ  nháº­n dáº¡ng chá»¯ viáº¿t tay(Handwritten Character Recognition). Trong á»©ng dá»¥ng nÃ y, má»—i kÃ½ tá»± viáº¿t tay Ä‘Æ°á»£c chuyá»ƒn thÃ nh má»™t vector Ä‘áº·c trÆ°ng, vÃ  SVM sáº½ phÃ¢n loáº¡i cÃ¡c vector nÃ y vÃ o cÃ¡c lá»›p tÆ°Æ¡ng á»©ng vá»›i tá»«ng kÃ½ tá»± 
		
	## PhÃ¢n loáº¡i vÄƒn báº£n vÃ  lá»c thÆ° rÃ¡c. 
		SVM cÅ©ng Ä‘Æ°á»£c á»©ng dá»¥ng trong xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn(NLP), Ä‘áº·c biá»‡t lÃ  phÃ¢n loáº¡i vÄƒn báº£n. Má»™t trong nhá»¯ng á»©ng dá»¥ng phá»• biáº¿n nháº¥t lÃ  lá»c thÆ° rÃ¡c(spam filtering). Trong bÃ i toÃ¡n nÃ y, má»—i email Ä‘Æ°á»£c biá»ƒu diá»…n dÆ°á»›i dáº¡ng má»™t vector Ä‘áº·c trÆ°ng(cháº³ng háº¡n nhÆ° táº§n suáº¥t xuáº¥t hiá»‡n cá»§a tá»« ngá»¯), vÃ  SVM sáº½ phÃ¢n loáº¡i email vÃ o nhÃ³m thÆ° rÃ¡c hoáº·c khÃ´ng pháº£i thÆ° rÃ¡c.
		
		á»¨ng dá»¥ng nÃ y giÃºp cÃ¡c há»‡ thá»‘ng email lá»c bá» nhá»¯ng email khÃ´ng mong muá»‘n, tÄƒng cÆ°á»ng hiá»‡u suáº¥t lÃ m viá»‡c vÃ  báº£o vá»‡ ngÆ°á»i dÃ¹ng khá»i cÃ¡c má»—i Ä‘e dá»a an ninh máº¡ng. 
		
	## PhÃ¡t hiá»‡n gian láº­n 
		Trong lÄ©nh vá»±c tÃ i chÃ­nh, SVM Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ phÃ¡t hiá»‡n cÃ¡c hÃ nh vi gian láº­n trong giao dá»‹ch. Vá»›i kháº£ nÄƒng phÃ¢n loáº¡i chÃ­nh xÃ¡c, SVM cÃ³ thá»ƒ phÃ¡t hiá»‡n cÃ¡c giao dá»‹ch báº¥t thÆ°á»ng dá»±a trÃªn cÃ¡c máº«u dá»¯ liá»‡u lá»‹ch sá»­. Nhá»¯ng á»©ng dá»¥ng nhÆ° váº­y Ä‘áº·c biá»‡t quan trá»ng trong viá»‡c báº£o vá»‡ cÃ¡c tá»• chá»©c tÃ i chÃ­nh khá»i cÃ¡c hoáº¡t Ä‘á»™ng lá»«a Ä‘áº£o vÃ  giáº£m thiá»ƒu rá»§i ro. 
		
		CÃ¡c cÃ´ng ty tháº» tÃ­n dá»¥ng, ngÃ¢n hÃ ng vÃ  dá»‹ch vá»¥ tÃ i chÃ­nh thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng SVM Ä‘á»ƒ phÃ¡t hiá»‡n cÃ¡c giao dá»‹ch gian láº­n trong thá»i gian thá»±c, giÃºp há» Ä‘Æ°a ra cÃ¡c biá»‡n phÃ¡p ngÄƒn cháº·n ká»‹p thá»i. 
		
	## á»¨ng dá»¥ng trong y táº¿ vÃ  sinh há»c. 
		Trong lÄ©nh vá»±c y táº¿, SVM Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ phÃ¢n tÃ­ch dá»¯ liá»‡u y khoa, chuáº©n Ä‘oÃ¡n bá»‡nh, vÃ  dá»± Ä‘oÃ¡n káº¿t quáº£ Ä‘iá»u trá»‹. Má»™t vÃ­ dá»¥ Ä‘iá»ƒn hÃ¬nh lÃ  viá»‡c sá»­ dá»¥ng SVM Ä‘á»ƒ phÃ¢n loáº¡i cÃ¡c táº¿ bÃ o ung thÆ° dá»±a trÃªn cÃ¡c máº«u sinh há»c. Báº±ng cÃ¡ch phÃ¢n tÃ­ch dá»¯ liá»‡u táº¿ bÃ o há»c hoáº·c dá»¯ liá»‡u hÃ¬nh áº£nh y táº¿, SVM cÃ³ thá»ƒ há»— trá»£ cÃ¡c bÃ¡c sÄ© trong viá»‡c chuáº©n Ä‘oÃ¡n bá»‡nh nhanh chÃ³ng vÃ  chÃ­nh xÃ¡c hÆ¡n.
		Trong nghiÃªn cá»©u sinh há»c, SVM Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ phÃ¢n loáº¡i dá»¯ liá»‡u gene vÃ  protein, giÃºp cÃ¡c nhÃ  kha há»c xÃ¡c Ä‘á»‹nh cÃ¡c máº«u sinh há»c vÃ  má»‘i liÃªn há»‡ giá»¯a cÃ¡c bá»‡nh lÃ½ vÃ  gene 
		
# XÃ¢y dá»±ng chÆ°Æ¡ng trÃ¬nh SVM 

	## VÃ­ dá»¥ thá»±c hiá»‡n 
		Táº¡o dá»¯ liá»‡u máº«u: sá»­ dá»¥ng make_classification tá»« sklearn Ä‘á»ƒ táº¡o má»™t táº­p dá»¯ liá»‡u máº«u vá»›i 2 lá»›p. 
		
		Chuáº©n hÃ³a dá»¯ liá»‡u : Sá»­ dá»¥ng StandardScaler Ä‘á»ƒ chuáº©n hÃ³a dá»¯ liá»‡u vá» dáº¡ng phÃ¢n phá»‘i chuáº©n. 
		
		Äá»‹nh nghÄ©a mÃ´ hÃ¬nh SVM : SVM Ä‘Æ°á»£c xÃ¢y dá»±ng báº±ng cÃ¡ch sá»­ dá»¥ng má»™t lá»›p tuyáº¿n tÃ­nh (nn.Linear). ChÃºng ta sá»­ dá»¥ng hÃ m máº¥t mÃ¡t Hinge (nn.HingeEmbeddingLoss)  Ä‘á»ƒ thá»±c hiá»‡n huáº¥n luyá»‡n mÃ´ hÃ¬nh . 
		
		Huáº¥n luyá»‡n mÃ´ hÃ¬nh : ChÃºng ta sá»­ dá»¥ng Optimizer Adam Ä‘á»ƒ cáº­p nháº­t sá»‘ lÆ°á»£ng vÃ  trá»ng sá»‘, vÃ  huáº¥n luyá»‡n mÃ´ hÃ¬nh trong má»™t sá»‘ epoch nháº¥t Ä‘á»‹nh. 
		
		ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh : Sau khi huáº¥n luyá»‡n, chÃºng ta Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh trÃªn táº­p test vÃ  tÃ­nh toÃ¡n Ä‘á»™ chÃ­nh xÃ¡c. 
		
			SVMByPytorch.py
	
		
# Káº¿t luáº­n 
	SVM lÃ  má»™t thuáº­t toÃ¡n há»c mÃ¡y cÃ³ tÃ­nh á»©ng dá»¥ng cao vá»›i nhiá»u Æ°u Ä‘iá»ƒm vÆ°á»£t trá»™i nhÆ° hiá»‡u suáº¥t cao vá»›i dá»¯ liá»‡u cÃ³ sá»‘ lÆ°á»£ng Ä‘áº·c trÆ°ng lá»›n, kháº£ nÄƒng xá»­ lÃ½ dá»¯ liá»‡u phi tuyáº¿n tÃ­nh vá»›i kernel, vÃ  tá»‘i Æ°u hÃ³a margin. Tuy nhiÃªn, SVM cÅ©ng cÃ³ nhá»¯ng nhÆ°á»£c Ä‘iá»ƒm nhÆ° Ä‘á»™ phá»©c táº¡p tÃ­nh toÃ¡n cao, khÃ³ khÄƒn trong viá»‡c chá»n kernel phÃ¹ há»£p, vÃ  nháº¡y cáº£m vá»›i nhiá»…u. 
	
	Viá»‡c lá»±a chá»n SVM lÃ m thuáº­t toÃ¡n há»c mÃ¡y phá»¥ thuá»™c vÃ o Ä‘áº·c Ä‘iá»ƒm cá»§a bÃ i toÃ¡n cá»¥ thá»ƒ. Náº¿u bÃ i toÃ¡n cÃ³ sá»‘ lÆ°á»£ng Ä‘áº·c trÆ°ng lá»›n, dá»¯ liá»‡u phá»©c táº¡p vÃ  cáº§n mÃ´ hÃ¬nh cÃ³ kháº£ nÄƒng tá»•ng quÃ¡t hÃ³a tá»‘t, SVM cÃ³ thá»ƒ lÃ  má»™t lá»±a chá»n lÃ½ tÆ°á»Ÿng. Tuy nhiÃªn, Ä‘á»‘i vá»›i cÃ¡c bÃ i toÃ¡n vá»›i dá»¯ liá»‡u lá»›n yÃªu cáº§u thá»i gian xá»­ lÃ½ nhanh, cÃ¡c thuáº­t toÃ¡n khÃ¡c cÃ³ thá»ƒ phÃ¹ há»£p hÆ¡n. 
	
//========================================= Máº¡ng Neural nhÃ¢n táº¡o : CÃ´ng nghá»‡ Ä‘á»™t phÃ¡ trong trÃ­ tuá»‡ nhÃ¢n táº¡o ==================== 
	
	# Giá»›i thiá»‡u 
		Máº¡ng nÆ¡-ron nhÃ¢n táº¡o (Artificial Neural Networks - ANN) lÃ  má»™t mÃ´ hÃ¬nh tÃ­nh toÃ¡n Ä‘Æ°á»£c láº¥y cáº£m há»©ng tá»« cáº¥u trÃºc vÃ  chá»©c nÄƒng cá»§a bá»™ nÃ£o con ngÆ°á»i. ANN Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ nháº­n diá»‡n cÃ¡c máº«u phá»©c táº¡p vÃ  thá»±c hiá»‡n cÃ¡c nhiá»‡m vá»¥ nhÆ° phÃ¢n loáº¡i, dá»± Ä‘oÃ¡n, vÃ  xá»­ lÃ½ dá»¯ liá»‡u . 
		
			https://aicandy.vn/wp-content/uploads/2024/09/aicandy_ANN.jpg
		
		Lá»‹ch sá»­ phÃ¡t triá»ƒn cá»§a ANN báº¯t Ä‘áº§u tá»« nhá»¯ng nÄƒm 1940 vá»›i cÃ¡c nghiÃªn cá»©u vá» mÃ´ hÃ¬nh neuron Ä‘Æ¡n giáº£n. Tá»« Ä‘Ã³, ANN Ä‘Ã£ tráº£i qua nhiá»u giai Ä‘oáº¡n phÃ¡t triá»ƒn vá»›i sá»± cáº£i tiáº¿n vá» kiáº¿n trá»©c vÃ  thuáº­t toÃ¡n há»c, Ä‘Ã³ng vai trÃ² quan trá»ng trong sá»± phÃ¡t triá»ƒn cá»§a trÃ­ tuá»‡ nhÃ¢n táº¡o hiá»‡n Ä‘áº¡i 

		Máº¡ng nÆ¡-ron nhÃ¢n táº¡o Ä‘Ã£ áº£nh hÆ°á»Ÿng máº¡nh máº½ Ä‘áº¿n nhiá»u lÄ©nh vá»±c nhÆ° thá»‹ giÃ¡c mÃ¡y tÃ­nh, xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn, y há»c vÃ  nhiá»u á»©ng dá»¥ng cÃ´ng nghá»‡ khÃ¡c. 
		
	# Cáº¥u trÃºc máº¡ng nÆ¡-ron nhÃ¢n táº¡o 
		Máº¡ng nÆ¡-ron nhÃ¢n táº¡o Ä‘Æ°á»£c táº¡o thÃ nh tá»« cÃ¡c Ä‘Æ¡n vá»‹ nhá» gá»i lÃ  nÆ¡-ron. Má»—i nÆ¡-ron cÃ³ thá»ƒ Ä‘Æ°á»£c xem nhÆ° má»™t hÃ m tÃ­nh toÃ¡n nháº­n Ä‘áº§u vÃ o vÃ  táº¡o Ä‘áº§u ra. Má»™t máº¡ng nÆ¡-ron thÆ°á»ng bao gá»“m ba lá»›p chÃ­nh : 
			
			
			## Lá»›p Ä‘áº§u vÃ o(input Layer )
			
				Lá»›p nÃ y nháº­n thÃ´ng tin tá»« bÃªn ngoÃ i vÃ o máº¡ng. Sá»‘ lÆ°á»£ng nÆ¡-ron trong lá»›p nÃ y phá»¥ thuá»™c vÃ o sá»‘ lÆ°á»£ng Ä‘áº·c trÆ°ng (features ) cá»§a dá»¯ liá»‡u 
				
			## Lá»›p áº©n (Hidden Layer )
				Lá»›p nÃ y thá»±c hiá»‡n cÃ¡c tÃ­nh toÃ¡n phá»©c táº¡p Ä‘á»ƒ tÃ¬m ra cÃ¡c Ä‘áº·c trÆ°ng áº©n trong dá»¯ liá»‡u. Máº¡ng cÃ³ thá»ƒ cÃ³ nhiá»u lá»›p áº©n khÃ¡c nhau 
				
			## Lá»›p Ä‘áº§u ra (Output Layer)
				Lá»›p nÃ y Ä‘Æ°a ra káº¿t quáº£ cá»§a máº¡ng. Sá»‘ lÆ°á»£ng nÆ¡-ron trong lá»›p nÃ y phá»¥ thuá»™c vÃ o bÃ i toÃ¡n cá»¥ thá»ƒ, vÃ­ dá»¥: PhÃ¢n loáº¡i nhá»‹ phÃ¢n sáº½ cÃ³ má»™t nÆ¡-ron, cÃ²n phÃ¢n loáº¡i Ä‘a lá»›p sáº½ cÃ³ sá»‘ nÆ¡-ron tÆ°Æ¡ng á»©ng. 
				
		
		

	# NguyÃªn lÃ½ hoáº¡t Ä‘á»™ng 
			https://aicandy.vn/wp-content/uploads/2024/09/aicandy_ANN_cal.jpg
			
		
		## CÃ´ng thá»©c tÃ­nh toÃ¡n trong nÆ¡-ron 
			Má»—i nÆ¡-ron nháº­n Ä‘áº§u vÃ o tá»« cÃ¡c nÆ¡-ron trÆ°á»›c Ä‘Ã³, tÃ­nh toÃ¡n giÃ¡ trá»‹ Ä‘áº§u ra dá»±a trÃªn trá»ng sá»‘ vÃ  giÃ¡ trá»‹ thiÃªn lá»‡ch : 
					y = f\left(\sum_{i=1}^{n} w_i x_i + b\right) 
					
			Trong Ä‘Ã³ :
				x_i: GiÃ¡ trá»‹ Ä‘áº§u vÃ o 
				w_i: Trá»ng sá»‘ cá»§a káº¿t ná»‘i 
				b: GiÃ¡ trá»‹ thiÃªn lá»‡ch 
				f: HÃ m kÃ­ch hoáº¡t
				y: Äáº§u ra cá»§a nÆ¡-ron 
				
			Sau khi tÃ­nh toÃ¡n tá»•ng Ä‘áº§u vÃ o z , nÆ¡-ron Ã¡p dá»¥ng má»™t hÃ m kÃ­ch hoáº¡t(activation function) Ä‘á»ƒ táº¡o ra Ä‘áº§u ra. hÃ m kÃ­ch hoáº¡t nÃ y giÃºp máº¡ng nÆ¡-ron cÃ³ kháº£ nÄƒng mÃ´ hÃ¬nh hÃ³a cÃ¡c quan há»‡ phi tuyáº¿n tÃ­nh . 
				
				
		## HÃ m kÃ­ch hoáº¡t 

			### HÃ m Sigmoid 
				HÃ m Sigmoid chuyá»ƒn Ä‘á»•i giÃ¡ trá»‹ Ä‘áº§u vÃ o thÃ nh má»™t giÃ¡ trá»‹ trong khoáº£ng tá»« 0 Ä‘áº¿n 1, phÃ¹ há»£p cho cÃ¡c bÃ i toÃ¡n phÃ¢n loáº¡i. CÃ´ng thá»©c toÃ¡n há»c Ä‘Æ°á»£c biá»ƒu diá»…n nhÆ° sau : 
				
						f(x) = \frac{1}{1 + e^{-x}}
						
			
			## HÃ m ReLU 
				HÃ m ReLU giÃºp máº¡ng há»c nhanh hÆ¡n, Ä‘áº·c biá»‡t lÃ  trong cÃ¡c mÃ´ hÃ¬nh lá»›n, do tÃ­nh cháº¥t Ä‘Æ¡n giáº£n cá»§a nÃ³. CÃ´ng thá»©c toÃ¡n há»c Ä‘Æ°á»£c biá»ƒu diá»…n nhÆ° sau : 
					f(x) = \max(0, x)
					
			## HÃ m Tanh 
				HÃ m Tanh tÆ°Æ¡ng tá»± nhÆ° Sigmoid nhÆ°ng Ä‘áº§u ra náº±m trong khoáº£ng tá»« -1 Ä‘áº¿n 1, há»¯u Ã­ch trong cÃ¡c bÃ i toÃ¡n liÃªn quan Ä‘áº¿n giÃ¡ trá»‹ Ã¢m . 
							f(x) = \frac{2}{1 + e^{-2x}} â€“ 1
							
							
			
		## QuÃ¡ trÃ¬nh lan truyá»n tiáº¿n (Feedforward)
			
			Trong quÃ¡ trÃ¬nh lan truyá»n tiáº¿n, dá»¯ liá»‡u Ä‘áº§u vÃ o Ä‘Æ°á»£c truyá»n tá»« lá»›p Ä‘áº§u vÃ o qua cÃ¡c lá»›p áº©n, vÃ  cuá»‘i cÃ¹ng Ä‘áº¿n lá»›p Ä‘áº§u ra. Má»—i nÆ¡-ron trong má»™t lá»›p nháº­n Ä‘áº§u vÃ o tá»« cÃ¡c nÆ¡-ron cá»§a lá»›p trÆ°á»›c Ä‘Ã³, tÃ­nh toÃ¡n Ä‘áº§u ra vÃ  truyá»n Ä‘áº¿n lá»›p tiáº¿p theo. QuÃ¡ trÃ¬nh nÃ y Ä‘Æ°á»£c gá»i lÃ  lan truyá»n tiáº¿n(feedforward)
			
			VÃ­ dá»¥, xÃ©t má»™t máº¡ng nÆ¡-ron Ä‘Æ¡n giáº£n vá»›i má»™t lá»›p Ä‘áº§u vÃ o, má»™t lá»›p áº©n vÃ  má»™t lá»›p Ä‘áº©u ra. Náº¿u Ä‘áº§u vÃ o lÃ  x1 vÃ  x2, cÃ¡c trá»ng sá»‘ káº¿t ná»‘i giá»¯a Ä‘áº§u vÃ o vÃ  lá»›p áº©n lÃ  w11 , w12 , w21 , w22 thÃ¬ Ä‘áº§u ra cá»§a cÃ¡c nÆ¡-ron trong lá»›p áº©n cÃ³ thá»ƒ Ä‘Æ°á»£c tÃ­nh nhÆ° sau: 
				z_1 = w_{11}x_1 + w_{12}x_2 + b_1
				
				z_2 = w_{21}x_1 + w_{22}x_2 + b_2
					
				
			Sau khi Ã¡p dá»¥ng hÃ m kÃ­ch hoáº¡t, Ä‘áº§u ra cá»§a cÃ¡c nÆ¡-ron lá»›p áº©n sáº½ lÃ  a_1 = \sigma(z_1) vÃ  a_2 = \sigma(z_2) . Äáº§u ra nÃ y sau Ä‘Ã³ Ä‘Æ°á»£c lan truyá»n Ä‘áº¿n lá»›p tiáº¿p theo(lá»›p Ä‘áº§u ra )
			
		## QuÃ¡ trÃ¬nh lan truyá»n ngÆ°á»£c 
			Sau khi máº¡ng nÆ¡-ron tÃ­nh toÃ¡n Ä‘áº§u ra, bÆ°á»›c tiáº¿p theo lÃ  cáº­p nháº­t cÃ¡c trá»ng sá»‘ dá»±a trÃªn lá»—i(error) giá»¯a Ä‘áº§u ra dá»± Ä‘oÃ¡n vÃ  Ä‘áº§u ra thá»±c táº¿. QuÃ¡ trÃ¬nh nÃ y Ä‘Æ°á»£c gá»i lÃ  lan truyá»n ngÆ°á»£c(backpropagation)
				https://aicandy.vn/wp-content/uploads/2024/09/aicandy_ANN_back.jpg 
				
			Lá»—i cá»§a máº¡ng Ä‘Æ°á»£c tÃ­nh toÃ¡n báº±ng má»™t hÃ m máº¥t mÃ¡t(Loss Function ) vÃ­ dá»¥ nhÆ° hÃ m bÃ¬nh phÆ°Æ¡ng lá»—i trung bÃ¬nh(Mean Squared Error - MSE) Ä‘á»‘i vá»›i bÃ i toÃ¡n há»“i quy 
				L = \frac{1}{2} \sum_{i=1}^{n} (y_i â€“ \hat{y}_i)^2 
				
			Trong Ä‘Ã³ 
				y_i : lÃ  giÃ¡ trá»‹ thá»±c táº¿ cá»§a Ä‘áº§u ra 
				\hat{y}_i : lÃ  giÃ¡ trá»‹ dá»± Ä‘oÃ¡n cá»§a máº¡ng 
				
				
			Sau khi tÃ­nh toÃ¡n lá»—i, máº¡ng sá»­ dá»¥ng thuáº­t toÃ¡n gradient descent Ä‘á»ƒ Ä‘iá»u chá»‰nh cÃ¡c trá»ng sá»‘ sao cho lá»—i Ä‘Æ°á»£c giáº£m thiá»ƒu. Gradient descent cáº­p nháº­t trá»ng sá»‘ theo phÆ°Æ¡ng trÃ¬nh : 
					w_i = w_i â€“ \eta \frac{\partial L}{\partial w_i} 
					
			Trong Ä‘Ã³ \eta lÃ  tá»‘c Ä‘á»™ há»c(learning rate) vÃ  \frac{\partial L}{\partial w_i} lÃ  Ä‘áº¡o hÃ m máº¥t mÃ¡t Ä‘á»‘i vá»›i trá»ng sá»‘ w_i 
			
		
			### VÃ­ dá»¥ cá»¥ thá»ƒ 
					Giáº£ sá»­ chÃºng ta cÃ³ má»™t bÃ i toÃ¡n Ä‘Æ¡n giáº£n vá»›i má»™t máº¡ng nÆ¡-ron cÃ³ má»™t lá»›p Ä‘áº§u vÃ o vá»›i hai nÆ¡-ron, má»™t lá»›p áº©n vá»›i hai nÆ¡-ron vÃ  má»™t lá»›p Ä‘áº§u ra vá»›i má»™t nÆ¡-ron. Máº¡ng nÃ y Ä‘Æ°á»£c huáº¥n luyá»‡n Ä‘á»ƒ dá»± Ä‘oÃ¡n má»™t giÃ¡ trá»‹ Ä‘áº§u ra tá»« hai Ä‘áº§u vÃ o.
					
				sau khi lan truyá»n tiáº¿n qua cÃ¡c lá»›p, máº¡ng sáº½ tÃ­nh toÃ¡n Ä‘áº§u ra dá»± Ä‘oÃ¡n. Náº¿u Ä‘áº§u ra thá»±c táº¿ lÃ  y = 0.5 vÃ  Ä‘áº§u ra dá»± Ä‘oÃ¡n lÃ  \hat{y} = 0.6, thÃ¬ lá»—i sáº½ lÃ : L = \frac{1}{2} (0.5 â€“ 0.6)^2 = 0.005
				
				Máº¡ng sau Ä‘Ã³ sáº½ sá»­ dá»¥ng quÃ¡ trÃ¬nh lan truyá»n ngÆ°á»£c Ä‘á»ƒ Ä‘iá»u chá»‰nh cÃ¡c trá»ng sá»‘ nháº±m giáº£m lá»—i nÃ y trong cÃ¡c láº§n láº·p tiáº¿p theo . 
				
				

 
# CÃ¡c loáº¡i máº¡ng nÆ¡-ron nháº­n táº¡o phá»• biáº¿n 
	## Feedforward Neural Network - FNN 
		
		Máº¡ng nÆ¡-ron truyá»n tháº³ng (FNN) lÃ  dáº¡ng Ä‘Æ¡n giáº£n nháº¥t, nÆ¡i cÃ¡c tÃ­n hiá»‡u di chuyá»ƒn má»™t chiá»u tá»« Ä‘áº§u vÃ o Ä‘áº¿n Ä‘áº§u ra mÃ  khÃ´ng cÃ³ vÃ²ng láº·p. Cáº¥u trÃºc tá»•ng quÃ¡t cá»§a máº¡ng bao gá»“m cÃ¡c lá»›p Ä‘áº§u vÃ o, lá»›p áº©n vÃ  lá»›p Ä‘áº§u ra. 
		
		CÃ´ng thá»©c tá»•ng quÃ¡t cho Ä‘áº§u ra cá»§a má»™t lá»›p nÆ¡-ron lÃ  : 
			y = f(Wx + b) 
			
		Trong Ä‘Ã³ : 
			x : vector Ä‘áº§u vÃ o 
			W : ma tráº­n trá»ng sá»‘ 
			b : há»‡ sá»‘ dá»‹ch chuyá»ƒn 
			f : hÃ m kÃ­ch hoáº¡t 
			
	## Convolutional Neural Network - CNN 
		Máº¡ng nÆ¡-ron tÃ­ch cháº­p (CNN) thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng trong cÃ¡c bÃ i toÃ n nháº­n diá»‡n hÃ¬nh áº£nh. Máº¡ng nÃ y dá»±a trÃªn cÃ¡c phÃ©p tÃ­nh tÃ­ch cháº­p Ä‘á»ƒ trÃ­ch xuáº¥t Ä‘áº·c trÆ°ng tá»« dá»¯ liá»‡u Ä‘áº§u vÃ o, giÃºp giáº£m sá»‘ lÆ°á»£ng tham sá»‘ so vá»›i FNN 
		
		phÃ©p tÃ­ch cháº­p Ä‘Æ°á»£c tÃ­nh theo cÃ´ng thá»©c 
			(f * g)(i, j) = \sum_m \sum_n f(m, n) \cdot g(i â€“ m, j â€“ n) 
			
		Trong Ä‘Ã³ : 
			f(i, j) : GiÃ¡ trá»‹ Ä‘iá»ƒm áº£nh táº¡i vá»‹ trÃ­(i , j) cá»§a áº£nh Ä‘áº§u vÃ o 
			g(m, n)	: GiÃ¡ trá»‹ táº¡i vá»‹ trÃ­(m,n) cá»§a kernel 
			(i, j)	: tá»a Ä‘á»™ cá»§a Ä‘iá»ƒm áº£nh trong káº¿t quáº£ tÃ­ch cháº­p 
			
		Trong thá»±c táº¿, phÃ©p tÃ­ch cháº­p trong máº¡ng CNN Ä‘Æ°á»£c thá»±c hiá»‡n thÃ´ng qua cÃ¡c kernel vá»›i ma tráº­n nhá» trÃªn dá»¯ liá»‡u hÃ¬nh áº£nh 
		
		
	## 4.3 Recurrent Neural Network - RNN 
	
		Máº¡ng nÆ¡-ron há»“i quy RNN lÃ  máº¡ng nÆ¡-ron cÃ³ káº¿t ná»‘i vÃ²ng láº·p, nÆ¡i cÃ¡c thÃ´ng tin cÃ³ thá»ƒ quay trá»Ÿ láº¡i nÃºt trÆ°á»›c Ä‘Ã³, táº¡o ra kháº£ nÄƒng ghi nhá»› ngá»¯ cáº£nh trong chuá»—i dá»¯ liá»‡u. Äiá»u nÃ y lÃ m RNN Ä‘áº·c biá»‡t phÃ¹ há»£p vá»›i cÃ¡c bÃ i toÃ¡n liÃªn quan Ä‘áº¿n chuá»—i thá»i gian, xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn. 
		
		CÃ´ng thá»©c tÃ­nh tráº¡ng thÃ¡i áº©n ht táº¡i thá»Ÿi Ä‘iá»ƒm t cá»§a RNN lÃ  : 
			h_t = f(W_h h_{t-1} + W_x x_t + b) 
			
		Trong Ä‘Ã³ : 
			h_t : tráº¡ng thÃ¡i áº©n táº¡i thá»i Ä‘iá»ƒm t 
			x_t : Ä‘áº§u vÃ o táº¡i thá»i Ä‘iá»ƒm t 
			W_h , W_x : trá»ng sá»‘ 
			b : há»‡ sá»‘ dá»‹ch chuyá»ƒn 
			
			
	
	## 4.4 	Long Short-Term Memory - LSTM 
		Máº¡ng nÆ¡-ron truy há»“i dÃ i háº¡n(LSTM) lÃ  má»™t biáº¿n thá»ƒ Ä‘áº·c biá»‡t cá»§a RNN, Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ giáº£i quyáº¿t váº¥n Ä‘á» quÃªn ngáº¯n háº¡n trong RNN báº±ng cÃ¡ch giá»¯ láº¡i thÃ´ng tin trong khoáº£ng thá»i gian dÃ i hÆ¡n. Máº¡ng nÃ y sá»­ dá»¥ng cÃ¡c cá»•ng Ä‘á»ƒ kiá»ƒm soÃ¡t dÃ²ng cháº£y cá»§a thÃ´ng tin 
		
		CÃ´ng thá»©c cá»§a cá»•ng Ä‘áº§u vÃ o i_t trong LSTM lÃ  : 
			i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i) 
			
		Trong Ä‘Ã³ : 
		\sigma  : lÃ  hÃ m sigmoid 
		W_i     : trá»ng sá»‘ cá»§a cá»•ng Ä‘áº§u vÃ o 
  		h_{t-1} : tráº¡ng thÃ¡i áº©n táº¡i thá»i Ä‘iá»ƒm t-1 
		x_t 	: Ä‘áº§u vÃ o táº¡i thá»i Ä‘iá»ƒm t 
		b_i 	: Há»‡ sá»‘ dá»‹ch chuyá»ƒn 
		
		
	## 4.5 Generative Adversarial Network - GAN 
		Máº¡ng nÆ¡-ron Ä‘á»‘i khÃ¡ng(GAN) bao gá»“m hai máº¡ng : Má»™t máº¡ng sinh (Generator) vÃ  má»™t máº¡ng phÃ¢n biá»‡t (Discriminator). Má»¥c tiÃªu cá»§a máº¡ng sinh lÃ  táº¡o ra dá»¯ liá»‡u giá»‘ng tháº­t nháº¥t cÃ³ thá»ƒ, trong khi máº¡ng phÃ¢n biá»‡t cá»‘ gáº¯ng phÃ¢n biá»‡t giá»¯a dá»¯ liá»‡u giáº£ vÃ  tháº­t. 
		
		HÃ m máº¥t mÃ¡t cá»§a GAN thÆ°á»ng Ä‘Æ°á»£c biá»ƒu diá»…n nhÆ° sau 
			\min_G \max_D V(D, G) = \mathbb{E}_{x \sim p_{data}(x)}[\log D(x)] + \mathbb{E}_{z \sim p_z(z)}[\log(1 â€“ D(G(z)))] 
			
		Trong Ä‘Ã³ : 
			G 			: máº¡ng sinh 
			D 			: máº¡ng phÃ¢n biá»‡t 
			p_{data}(x) : PhÃ¢n phá»‘i dá»¯ liá»‡u tháº­t 
			p_z(z)		: PhÃ¢n phá»‘i ngáº«u nhiÃªn dá»¯ liá»‡u Ä‘áº§u vÃ o 

# 5. á»¨ng dá»¥ng cá»§a máº¡ng nÆ¡-ron nhÃ¢n táº¡o 
		
	Máº¡ng nÆ¡-ron nhÃ¢n táº¡i (Artificial Neural Networks - ANN )	 Ä‘Ã£ vÃ  Ä‘ang Ä‘Ã³ng vai trÃ² quan trá»ng trong nhiá»u lÄ©nh vá»±c cá»§a Ä‘á»i sá»‘ng. Vá»›i kháº£ nÄƒng há»c há»i tá»« dá»¯ liá»‡u vÃ  mÃ´ hÃ¬nh hÃ³a cÃ¡c má»‘i quan há»‡ phá»©c táº¡p, ANN Ä‘Æ°á»£c á»©ng dá»¥ng rá»™ng rÃ£i trong cÃ¡c lÄ©nh vá»±c nhÆ° Ã½ táº¿, váº­n táº£i, vÃ  giáº£i trÃ­. DÆ°á»›i Ä‘Ã¢y lÃ  má»™t sá»‘ á»©ng dá»¥ng ná»•i báº­t cá»§a máº¡ng nÆ¡-ron nhÃ¢n táº¡o trong Ä‘á»i sá»‘ng thá»±c táº¿. 
	
	
	## Y táº¿ 
		Trong lÄ©nh vá»±c y táº¿, máº¡ng nÆ¡-ron nhÃ¢n táº¡o Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ chuáº©n Ä‘oÃ¡n bá»‡nh, phÃ¡t hiá»‡n cÃ¡c máº«u báº¥t thÆ°á»ng trong hÃ¬nh áº£nh y táº¿, vÃ  dá»± Ä‘oÃ¡n káº¿t quáº£ Ä‘iá»u trá»‹ , VÃ­ dá»¥ : 
		
			### Chuáº©n Ä‘oÃ¡n hÃ¬nh áº£nh y táº¿ : ANN cÃ³ kháº£ nÄƒng phÃ¢n tÃ­ch hÃ¬nh áº£nh y táº¿ nhÆ° X-quang, MRI hoáº·c CT scan Ä‘á»ƒ phÃ¡t hiá»‡n cÃ¡c dáº¥u hiá»‡u cá»§a ung thÆ°, tá»•n thÆ°Æ¡ng nÃ£o, hoáº·c cÃ¡c bá»‡nh khÃ¡c vá»›i Ä‘á»™ chÃ­nh xÃ¡c cao. 
			
			### Dá»± Ä‘oÃ¡n káº¿t quáº£ Ä‘iá»u trá»‹ : ANN cÃ³ thá»ƒ phÃ¢n tÃ­ch dá»¯ liá»‡u bá»‡nh Ã¡n vÃ  cÃ¡c yáº¿u tá»‘ khÃ¡c Ä‘á»ƒ dá»± Ä‘oÃ¡n hiá»‡u quáº£ cá»§a cÃ¡c phÆ°Æ¡ng phÃ¡p Ä‘iá»u trá»‹, giÃºp cÃ¡c bÃ¡c sÄ© Ä‘Æ°a ra quyáº¿t Ä‘á»‹nh tá»‘t hÆ¡n. 
			
	## TÃ i chÃ­nh 
		Trong lÄ©nh vá»±c tÃ i chÃ­nh, máº¡ng nÆ¡-ron nhÃ¢n táº¡o Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ dá»± Ä‘oÃ¡n thá»‹ trÆ°á»ng, phÃ¡t hiá»‡n gian láº­n, vÃ  tá»± Ä‘á»™ng hÃ³a cÃ¡c quy trÃ¬nh tÃ i chÃ­nh. Má»™t sá»‘ á»©ng dá»¥ng cá»¥ thá»ƒ bao gá»“m : 
			
			### Dá»± Ä‘oÃ¡n giÃ¡ cá»• phiáº¿u : ANN cÃ³ thá»ƒ phÃ¢n tÃ­ch dá»¯ liá»‡u lá»‹ch sá»­ cá»§a cá»• phiáº¿u, biáº¿n Ä‘á»™ng thá»‹ trÆ°á»ng, vÃ  cÃ¡c yáº¿u tá»‘ kinh táº¿ Ä‘á»ƒ dá»± Ä‘oÃ¡n xu hÆ°á»›ng giÃ¡ cá»• phiáº¿u trong tÆ°Æ¡ng lai. 
			
			### PhÃ¡t hiá»‡n gian láº­n : ANN Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ phÃ¢n tÃ­ch cÃ¡c giao dá»‹ch tÃ i chÃ­nh vÃ  phÃ¡t hiá»‡n nhá»¯ng giao dá»‹ch báº¥t thÆ°á»ng cÃ³ thá»ƒ liÃªn quan Ä‘áº¿n gian láº­n. 
			
			### Quáº£n lÃ½ rá»§i ro : CÃ¡c tá»• chá»©c tÃ i chÃ­nh sá»­ dá»¥ng ANN Ä‘á»ƒ dá»± Ä‘oÃ¡n rá»§i ro tÃ­n dá»¥ng vÃ  quáº£n lÃ½ danh má»¥c Ä‘áº§u tÆ° má»™t cÃ¡ch hiá»‡u quáº£ hÆ¡n. 
			
		
	## Váº­n táº£i 
		Máº¡ng nÆ¡-ron nhÃ¢n táº¡o Ä‘Ã³ng vai trÃ² quan trá»ng trong viá»‡c phÃ¡t triá»ƒn cÃ¡c há»‡ thá»‘ng giao thÃ´ng thÃ´ng minh vÃ  tá»± Ä‘á»™ng. Má»™t sá»‘ á»©ng dá»¥ng Ä‘iá»ƒn hÃ¬nh trong lÄ©nh vá»±c váº­n táº£i bao gá»“m. 
		
		### Xe tá»± lÃ¡i : ANN lÃ  ná»n táº£ng cho cÃ¡c há»‡ thá»‘ng lÃ¡i xe tá»± Ä‘á»™ng, giÃºp xe nháº­n diá»‡n vÃ  phÃ¢n tÃ­ch cÃ¡c tÃ¬nh huá»‘ng giao thÃ´ng, nhÆ° nháº­n diá»‡n ngÆ°á»i Ä‘i bá»™, biá»ƒn bÃ¡o, vÃ  cÃ¡c phÆ°Æ¡ng tiá»‡n khÃ¡c. 
		
		### Tá»‘i Æ°u hÃ³a lá»™ trÃ¬nh : ANN giÃºp cÃ¡c há»‡ thá»‘ng giao thÃ´ng thÃ´ng minh tá»‘i Æ°u hÃ³a lá»™ trÃ¬nh di chuyá»ƒn, dá»±a trÃªn dá»¯ liá»‡u giao thÃ´ng thá»i gian thá»±c vÃ  cÃ¡c yÃªu tá»‘ nhÆ° thá»i tiáº¿t, tai náº¡n hoáº·c cÃ´ng trÃ¬nh. 
		
		### Quáº£n lÃ½ Ä‘á»™i xe :  CÃ¡c cÃ´ng ty váº­n táº£i sá»­ dá»¥ng ANN Ä‘á»ƒ quáº£n lÃ½ Ä‘á»™i xe cá»§a há». dá»± Ä‘oÃ¡n nhu cáº§u váº­n táº£i vÃ  tá»‘i Æ°u hÃ³a viá»‡c sá»­ dá»¥ng tÃ i nguyÃªn. 
		
		
	## GiÃ¡o dá»¥c 
		Trong lÄ©nh vá»±c giÃ¡o dá»¥c, ANN Ä‘Æ°á»£c á»©ng dá»¥ng Ä‘á»ƒ phÃ¡t triá»ƒn cÃ¡c há»‡ thá»‘ng há»c táº­p cÃ¡ nhÃ¢n hÃ³a vÃ  há»— trá»£ giáº£ng dáº¡y thÃ´ng minh. Má»˜t sá»‘ á»©ng dá»¥ng tiÃªu biá»ƒu bao gá»“m : 
		
			### Há»c táº­p cÃ¡ nhÃ¢n hÃ³a: ANN cÃ³ thá»ƒ phÃ¢n tÃ­ch hÃ nh vi vÃ  hiá»‡u suáº¥t há»c táº­p cá»§a tá»«ng há»c sinh Ä‘á»ƒ Ä‘Æ°a ra cÃ¡c Ä‘á» xuáº¥t há»c táº­p cÃ¡ nhÃ¢n hÃ³a, giÃºp cáº£i thiá»‡n káº¿t quáº£ há»c táº­p . 
			
			### Há»‡ thá»‘ng giáº£ng dáº¡y thÃ´ng minh : ANN cÃ³ thá»ƒ Ä‘Æ°á»£c tÃ­ch há»£p vÃ o cÃ¡c há»‡ thá»‘ng giáº£ng dáº¡y tá»± Ä‘á»™ng Ä‘Ã¡nh giÃ¡ bÃ i táº­p, cung cáº¥p pháº£n há»“i tá»©c thá»i, vÃ  há»— trá»£ giáº£ng viÃªn trong viá»‡c quáº£n lÃ½ lá»›p há»c. 
			
			### PhÃ¡t hiá»‡n gian láº­n trong thi cá»­ : ANN cÃ³ thá»ƒ phÃ¢n tÃ­ch cÃ¡c máº«u hÃ nh vi trong thi cá»­ Ä‘á»ƒ phÃ¡t hiá»‡n nhá»¯ng hÃ nh vi gian láº­n báº¥t thÆ°á»ng. 
			
			
	## Giáº£i trÃ­ 
		
		Trong ngÃ nh cÃ´ng nghiá»‡p giáº£i trÃ­, ANN Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ táº¡o ra cÃ¡c há»‡ thá»‘ng Ä‘á» xuáº¥t thÃ´ng minh, phÃ¡t triá»ƒn trÃ² chÆ¡i Ä‘iá»‡n tá»­, vÃ  xá»­ lÃ½ Ã¢m thanh, hÃ¬nh áº£nh. Má»™t sá»‘ á»©ng dá»¥ng bao gá»“m: 
		
		### Há»‡ thá»‘ng Ä‘á» xuáº¥t : ANN Ä‘Æ°á»£c sá»­ dá»¥ng bá»Ÿi cÃ¡c ná»n táº£ng giáº£i trÃ­ trá»±c tuyáº¿n nhÆ° Netflix, Youtube , Spotify Ä‘á»ƒ phÃ¢n tÃ­ch sá»Ÿ thÃ­ch cá»§a ngÆ°á»i dÃ¹ng vÃ  Ä‘á» xuáº¥t cÃ¡c ná»™i dung phÃ¹ há»£p. 
		
		### PhÃ¡t triá»ƒn trÃ² chÆ¡i Ä‘iá»‡n tá»­ : ANN cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ phÃ¡t triá»ƒn cÃ¡c nhÃ¢n váº­t trÃ­ tuá»‡ nhÃ¢n táº¡o trong trÃ² chÆ¡i Ä‘iá»‡n tá»­, giÃºp chÃºng tÆ°Æ¡ng tÃ¡c thÃ´ng minh vÃ  chÃ¢n thá»±c hÆ¡n vá»›i ngÆ°á»i chÆ¡i. 
		
		### Xá»­ lÃ½ Ã¢m thanh vÃ  hÃ¬nh áº£nh : ANN Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ xá»­ lÃ½ vÃ  cáº£i thiá»‡n cháº¥t lÆ°á»£ng Ã¢m thanh, hÃ¬nh áº£nh trong cÃ¡c sáº£n pháº©m giáº£i trÃ­ nhÆ° phim áº£nh, Ã¢m nháº¡c hoáº·c video. 
		
		
		
# ThÃ¡ch thá»©c cá»§a máº¡ng nÆ¡-ron nhÃ¢n táº¡o 
	Máº·c dÃ¹ máº¡ng nÆ¡-ron nhÃ¢n táº¡o (Artificial Neural Networkds - ANN ) Ä‘Ã£ Ä‘áº¡t Ä‘Æ°á»£c nhiá»u thÃ nh tá»±u vÆ°á»£t báº­c, váº«n cÃ²n nhiá»u thÃ¡ch thá»©c lá»›n Ä‘á»‘i vá»›i cÃ´ng nghá»‡ nÃ y 
	
	## ÄÃ²i há»i tÃ i nguyÃªn tÃ­nh toÃ¡n khá»•ng lá»“ 
		CÃ¡c mÃ´ hÃ¬nh ANN Ä‘áº·c biá»‡t lÃ  cÃ¡c mÃ´ hÃ¬nh lá»›n nhÆ° máº¡ng nÆ¡-ron sÃ¢u(Deep Neural Networks - DNN) yÃªu cáº§u khá»‘i lÆ°á»£ng tÃ­nh toÃ¡n lá»›n. Viá»‡c huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh nÃ y thÆ°á»ng Ä‘Ã²i há»i pháº§n cá»©ng máº¡nh máº½, nhÆ° cÃ¡c GPU hoáº·c TPU, Ä‘á»“ng thá»i tiÃªu tá»‘n nhiá»u nÄƒng lÆ°á»£ng vÃ  thá»i gian 
		
	## Váº¥n Ä‘á» quÃ¡ khá»›p (Overfitting)
		Khi máº¡ng nÆ¡-ron quÃ¡ phá»©c táº¡p hoáº·c dá»¯ liá»‡u khÃ´ng Ä‘á»§ Ä‘a dáº¡ng, mÃ´ hÃ¬nh cÃ³ thá»ƒ há»c quÃ¡ ká»¹ cÃ¡c chi tiáº¿t tá»« dá»¯ liá»‡u huáº¥n luyá»‡n, dáº«n Ä‘áº¿n hiá»‡n tÆ°á»£ng quÃ¡ khá»›p. Äiá»u nÃ y lÃ m cho mÃ´ hÃ¬nh hoáº¡t Ä‘á»™ng kÃ©m khi gáº·p pháº£i dá»¯ liá»‡u má»›i. 
		
	## TÃ­nh minh báº¡ch vÃ  kháº£ nÄƒng giáº£i thÃ­ch 
		Máº¡ng nÆ¡-ron nhÃ¢n táº¡o thÆ°á»ng Ä‘Æ°á»£c gá»i lÃ  há»™p Ä‘en vÃ¬ ráº¥t khÃ³ Ä‘á»ƒ giáº£i thÃ­ch cÃ¡ch mÃ  mÃ´ hÃ¬nh Ä‘Æ°a ra quyáº¿t Ä‘á»‹nh. Äiá»u nÃ y Ä‘áº·c biá»‡t lÃ  váº¥n Ä‘á» trong cÃ¡c lÄ©nh vá»±c yÃªu cáº§u tÃ­nh minh báº¡ch cao nhÆ° y táº¿ hoáº·c tÃ i chÃ­nh, khi cÃ¡c quyáº¿t Ä‘á»‹nh cÃ³ áº£nh hÆ°á»Ÿng lá»›n Ä‘áº¿n con ngÆ°á»i 
		
	## YÃªu cáº§u lÆ°á»£ng Ä‘iá»‡n lá»›n 
		ANN cáº§n xá»­ lÃ½ dá»¯ liá»‡u khá»•ng lá»“ Ä‘á»ƒ cÃ³ thá»ƒ há»c hiá»‡u quáº£. Äá»‘i vá»›i nhiá»u lÄ©nh vá»±c, viá»‡c thu tháº­p, xá»­ lÃ½ vÃ  gÃ¡n nhÃ£n cho dá»¯ liá»‡u lá»›n ra ráº¥t tá»‘n kÃ©m vÃ  phá»©c táº¡p. 
		
		
# Káº¿t luáº­n 
	Máº¡ng nÆ¡-ron nhÃ¢n táº¡o(Artificial Neural Networks - ANN) Ä‘Ã£ vÃ  Ä‘ang Ä‘Ã³ng vai trÃ² cá»‘t lÃµi trong sá»± phÃ¡t triá»ƒn vÆ°á»£t báº­c cá»§a trÃ­ tuá»‡ nhÃ¢n táº¡o. Vá»›i kháº£ nÄƒng mÃ´ phá»ng cÃ¡ch hoáº¡t Ä‘á»™ng cá»§a nÃ£o ngÆ°á»i. ANN Ä‘Ã£ chá»©ng minh tiá»m nÄƒng to lá»›n trong viá»‡c giáº£i quyáº¿t cÃ¡c bÃ i toÃ¡n phá»©c táº¡p tá»« nháº­n diá»‡n hÃ¬nh áº£nh, phÃ¢n loáº¡i ngÃ´n ngá»¯ tá»± nhiÃªn, Ä‘áº¿n dá»± Ä‘oÃ¡n xu hÆ°á»›ng kinh táº¿ vÃ  y há»c. Tuy nhiÃªn, ANN cÅ©ng Ä‘i kÃ¨m vá»›i nhá»¯ng thÃ¡ch thá»©c lá»›n, bao gá»“m nhu cáº§u tÃ i nguyÃªn tÃ­nh toÃ¡n khá»•ng lá»“, váº¥n Ä‘á» quÃ¡ khá»›p vÃ  tÃ­nh minh báº¡ch trong mÃ´ hÃ¬nh. 
	Trong tÆ°Æ¡ng lai, cÃ¹ng vá»›i sá»± phÃ¡t triá»ƒn cá»§a cÃ´ng nghá»‡ vÃ  nghiÃªn cá»©u, ANN há»©a háº¹n sáº½ trá»Ÿ nÃªn ngÃ y cÃ ng máº¡nh máº½ vÃ  tá»‘i Æ°u hÆ¡n, má»Ÿ rá»™ng á»©ng dá»¥ng trong nhiá»u lÄ©nh vá»±c vÃ  gÃ³p pháº§n xÃ¢y dá»±ng nhá»¯ng há»‡ thá»‘ng trÃ­ tuá»‡ nhÃ¢n táº¡o tiÃªn tiáº¿n hÆ¡n. Tuy nhiÃªn, sá»± thÃ nh cÃ´ng cá»§a ANN khÃ´ng chá»‰ phá»¥ thuá»™c vÃ o viá»‡c cáº£i tiáº¿n ká»¹ thuáº­t mÃ  cÃ²n á»Ÿ viá»‡c xá»­ lÃ½ cÃ¡c váº¥n Ä‘á» vá» Ä‘áº¡o Ä‘á»©c, báº£o máº­t, vÃ  tÃ­nh kháº£ dá»¥ng trong Ä‘á»i sá»‘ng thá»±c táº¿. 
	
	
	
//================================ Convolutional Neural Networks (CNN) trong Deep Learning
# Giá»›i thiá»‡u vá» CNN 
		Convolutional Neural Networks (CNN) lÃ  má»™t trong nhá»¯ng kiáº¿n trÃºc máº¡ng nÆ¡-ron nhÃ¢n táº¡o Ä‘áº·c biá»‡t, chá»§ yáº¿u Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ xá»­ lÃ½ dá»¯ liá»‡u hÃ¬nh áº£nh, video, vÃ  cÃ¡c dáº¡ng dá»¯ liá»‡u cÃ³ cáº¥u trÃºc khÃ´ng gian nhÆ° tÃ­n hiá»‡u Ã¢m thanh vÃ  chuá»—i dá»¯ liá»‡u. CNN xuáº¥t phÃ¡t tá»« viá»‡c mÃ´ phá»ng láº¡i cÆ¡ cháº¿ háº¡t Ä‘á»™ng cá»§a vá» nÃ£o thá»‹ giÃ¡c cá»§a con ngÆ°á»i, nÆ¡i cÃ¡c táº¿ bÃ o tháº§n kinh cÃ³ kháº£ nÄƒng pháº£n á»©ng vá»›i cÃ¡c kÃ­ch thÃ­ch thá»‹ giÃ¡c, tá»« cÃ¡c Ä‘Æ°á»ng biÃªn Ä‘áº¿n cÃ¡c hÃ¬nh dáº¡ng phá»©c táº¡p. 
		
		CNN Ä‘Ã£ cÃ³ sá»± phÃ¡t triá»ƒn Ä‘ang ká»ƒ trong vÃ i tháº­p ká»· qua, bÆ°á»›c ngoáº·t lÃ  sá»± thÃ nh cÃ´ng cá»§a mÃ´ hÃ¬nh LeNet-5 do Yann LeCun giá»›i thiá»‡u vÃ o nÄƒm 1998, Ä‘Æ°á»£c sá»­ dá»¥ng cho viá»‡c nháº­n dáº¡ng chá»¯ sá»‘ viáº¿t tay. Tuy nhiÃªn, CNN thá»±c sá»± ná»•i tiáº¿ng nhá» mÃ´ hÃ¬nh AlexNext, chiáº¿n tháº¯ng trong cuá»™c thi ImageNet Large Scale Visual Recognition Challenge (ILSVRC) vÃ o nÄƒm 2012. Sá»± thÃ nh cÃ´ng cá»§a AlexNet má»Ÿ ra má»™t ká»· nguyÃªn má»›i cho nghiÃªn cá»©u trong thá»‹ giÃ¡c mÃ¡y tÃ­nh vÃ  há»c sÃ¢u 
		
	
	
	# Cáº¥u trÃºc cá»§a CNN 
		CNN Ä‘Æ°á»£c xÃ¢y dá»±ng tá»« nhiá»u lá»›p khÃ¡c nhau, má»—i lá»›p thá»±c hiá»‡n má»™t nhiá»‡m vá»¥ cá»¥ thá»ƒ trong quÃ¡ trÃ¬nh trÃ­ch xuáº¥t vÃ  xá»­ lÃ½ Ä‘áº·c trÆ°ng tá»« dá»¯ liá»‡u Ä‘áº§u vÃ o. Cáº¥u trÃºc Ä‘iá»ƒn hÃ¬nh cá»§a CNN bao gá»“m bá»‘n loáº¡i lá»›p chÃ­nh : Convolutional Layer , Activation Layer , Pooling Layer , Fully Connected Layer . Má»—i lá»›p cÃ³ vai trÃ² cá»¥ thá»ƒ vÃ  cÃ¹ng nhau táº¡o thÃ nh má»™t máº¡ng CNN máº¡nh máº½. 
		
			https://aicandy.vn/wp-content/uploads/2024/09/aicandy_CNN_arch.jpg


		## 2.1 Convolutional Layer 
			Lá»›p tÃ­ch cháº­p lÃ  ná»n táº£ng cá»‘t lÃµi cá»§a CNN, chá»‹u trÃ¡ch nhiá»‡m trÃ­ch xuáº¥t cÃ¡c Ä‘áº·c trÆ°ng tá»« hÃ¬nh áº£nh Ä‘áº§u vÃ o. Bá»™ lá»c(Filter )	 sáº½ trÆ°á»£t qua áº£nh vÃ  tÃ­nh toÃ¡n phÃ©p tÃ­ch cháº­p giá»¯a áº£nh vÃ  bá»™ lá»c Ä‘Ã³ 
			
			CÃ´ng thá»©c cá»§a phÃ©p tÃ­ch cháº­p hai chiá»u 
				Y[i,j] = \sum_{m} \sum_{n} X[i+m, j+n] \cdot K[m,n] 
				
			Trong Ä‘Ã³ 
				X : Ma tráº­n Ä‘áº§u vÃ o biá»ƒu diá»…n hÃ¬nh áº£nh. 
				K : Ma tráº­n kernel (bá»™ lá»c)
				Y[i,j] : GiÃ¡ trá»‹ Ä‘áº§u ra táº¡i vá»‹ trÃ­(i,j) sau khi thá»±c hiá»‡n phÃ©p tÃ­ch cháº­p 
				
		
		## 2.2 Activation Layer 
			Sau khi thá»±c hiá»‡n phÃ©p tÃ­ch cháº­p, dá»¯ liá»‡u sáº½ Ä‘i qua lá»›p kÃ­ch hoáº¡t Ä‘á»ƒ thÃªm tÃ­nh phi tuyáº¿n vÃ o mÃ´ hÃ¬nh. HÃ m kÃ­ch hoáº¡t phá»• biáº¿n nháº¥t lÃ  ReLU (Rectified Linear Unit)
					\text{ReLU}(x) = \max(0, x)
					
			ReLU giÃºp loáº¡i bá» cÃ¡c giÃ¡ trá»‹ Ã¢m trong Ä‘áº§u ra cá»§a phÃ©p tÃ­ch cháº­p, giá»¯ láº¡i cÃ¡c giÃ¡ trá»‹ dÆ°Æ¡ng vÃ  giÃºp tÄƒng tá»‘c quÃ¡ trÃ¬nh huáº¥n luyá»‡n 
			


		## 2.3 Pooling Layer 
			Lá»›p gá»™p cÃ³ nhiá»‡m vá»¥ giáº£m kÃ­ch thÆ°á»›c khÃ´ng gian cá»§a báº£n Ä‘á»“ Ä‘áº·c trÆ°ng, giÃºp giáº£m sá»‘ lÆ°á»£ng tham sá»‘ vÃ  tÃ­nh toÃ¡n trong máº¡ng. Lá»›p gá»™p lÃ m cho mÃ´ hÃ¬nh bá»n vá»¯ng hÆ¡n vá»›i cÃ¡c phÃ©p biáº¿n Ä‘á»•i nhÆ° dá»‹ch chuyá»ƒn hoáº·c xoay áº£nh 
			
			Phá»• biáº¿n nháº¥t lÃ  Max Pooling vá»›i cÃ´ng thá»©c 
				Y[i,j] = \max(X[i:i+f, j:j+f]) 
				
			trong Ä‘Ã³ f lÃ  kÃ­ch thÆ°á»›c cá»§a cá»­a sá»• gá»™p 
			
		## 2.4 Fully Connected Layer 
			Sau khi Ä‘i qua nhiá»u lá»›p tÃ­ch cháº­p vÃ  gá»™p, cÃ¡c báº£n Ä‘á»“ Ä‘áº·c trÆ°ng sáº½ Ä‘Æ°á»£c lÃ m pháº³ng thÃ nh má»™t vector má»™t chiá»u vÃ  Ä‘Æ°a vÃ o cÃ¡c lá»›p hoÃ n toÃ¡n káº¿t ná»‘i.
			
			Lá»›p nÃ y sá»­ dá»¥ng hÃ m kÃ­ch hoáº¡t softmax cho cÃ¡c bÃ i toÃ¡n phÃ¢n loáº¡i Ä‘a lá»›p 
					\text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{n} e^{z_j}} 
					
			Trong Ä‘Ã³ : 
				z_i : Äáº§u ra cá»§a nÆ¡-ron táº¡i lá»›p fullyconnected 
				n : Sá»‘ lá»›p phÃ¢n loáº¡i Ä‘áº§u ra 
				
		## 2.5 Back propagation 
			QuÃ¡ trÃ¬nh huáº¥n luyá»‡n CNN sá»­ dá»¥ng thuáº­t toÃ¡n lan truyá»n ngÆ°á»£c Ä‘á»ƒ tá»‘i Æ°u hÃ³a cÃ¡c tham sá»‘ báº±ng cÃ¡ch giáº£m thiá»ƒu hÃ m máº¥t mÃ¡t(loss function). HÃ m máº¥t mÃ¡t phá»• biáº¿n cho bÃ i toÃ¡n phÃ¢n loáº¡i lÃ  cross-entropy 
						L = â€“ \sum_{i} y_i \log(\hat{y_i})
						
			Trong Ä‘Ã³ : 
				y_i : GiÃ¡ trá»‹ thá»±c táº¿ (ground truth) cho lá»›p i 
				\hat{y_i} : XÃ¡c suáº¥t dá»± Ä‘oÃ¡n cá»§a mÃ´ hÃ¬nh cho lá»›p i 
				
		
		## 2.6 		 Triá»ƒn khai CNN báº±ng PyTorch
		
			ConvolutionalNeuralNetworksByPyTorch.py
			
			Giáº£i thÃ­ch : 
				conv1 , conv2 : CÃ¡c lá»›p tÃ­ch cháº­p Ä‘á»ƒ trÃ­ch xuáº¥t Ä‘áº·c trÆ°ng 
				pool : lá»›p Max Pooling giáº£m kÃ­ch thÆ°á»›c áº£nh 
				fc1 , fc2 : CÃ¡c lá»›p fully connected thá»±c hiá»‡n phÃ¢n loáº¡i cuá»‘i cÃ¹ng 
				relu : HÃ m kÃ­ch hoáº¡t ReLU 
				
			ÄÃ¢y lÃ  má»™t vÃ­ dá»¥ Ä‘Æ¡n giáº£n vá» CNN. CÃ¡c cáº¥u trÃºc CNN thá»±c táº¿ cÃ³ thá»ƒ phá»©c táº¡p hÆ¡n, bao gá»“m nhiá»u táº§ng tÃ­ch cháº­p vÃ  gá»™p Ä‘á»ƒ trÃ­ch xuáº¥t cÃ¡c Ä‘áº·c trÆ°ng phá»©c táº¡p tá»« áº£nh. 
			
			
			
	# CÃ¡c khÃ¡i niá»‡m chÃ­nh trong CNN 		
		
		## 3.1 Stride 
			Stride lÃ  bÆ°á»›c nháº£y cá»§a cá»­a sá»• tÃ­ch cháº­p khi nÃ³ di chuyá»ƒn qua áº£nh Ä‘áº§u vÃ o. GiÃ¡ trá»‹ stride quyáº¿t Ä‘á»‹nh tá»‘c Ä‘á»™ di chuyá»ƒn cá»§a cá»­a sá»•. Náº¿u stride báº±ng 1, cá»­a sá»• tÃ­ch cháº­p sáº½ di chuyá»ƒn tá»«ng bÆ°á»›c má»™t qua cÃ¡c pixel, cÃ²n náº¿u stride báº±ng 2, cá»­a sá»• sáº½ di chuyá»ƒn cÃ¡ch 2 pixel má»™t láº§n. 
				https://aicandy.vn/wp-content/uploads/2024/09/aicandy_CNN_stride.jpg
			
			GiÃ¡ trá»‹ stride cÃ ng lá»›n, kÃ­ch thÆ°á»›c cá»§a Ä‘áº§u ra sáº½ nhá» hÆ¡n vÃ¬ cá»­a sá»• tÃ­ch cháº­p sáº½ bá» qua nhiá»u pixel hÆ¡n. Äiá»u nÃ y cÃ³ thá»ƒ lÃ m giáº£m Ä‘á»™ phÃ¢n giáº£i cá»§a Ä‘áº§u ra, nhÆ°ng Ä‘á»“ng thá»i giáº£m thiá»ƒu khá»‘i lÆ°á»£ng tÃ­nh toÃ¡n. 
			
			
		## 3.2 Padding 
			Padding lÃ  ká»¹ thuáº­t thÃªm cÃ¡c pixel giáº£(thÆ°á»ng lÃ  giÃ¡ trá»‹ 0 , gá»i lÃ  Zero padding) xung quanh biÃªn cá»§a áº£nh Ä‘áº§u vÃ o. Äiá»u nÃ y giÃºp duy trÃ¬ kÃ­ch thÆ°á»›c Ä‘áº§u ra sau khi tÃ­ch cháº­p. 
					https://aicandy.vn/wp-content/uploads/2024/09/aicandy_CNN_padding.png
					
			Trong nhiá»u trÆ°á»ng há»£p, ngÆ°á»i ta thÃªm padding Ä‘á»ƒ kÃ­ch thÆ°á»›c Ä‘áº§u ra cá»§a lá»›p tÃ­ch cháº­p khÃ´ng bá»‹ giáº£m. VÃ­ dá»¥, náº¿u khÃ´ng cÃ³ padding, má»—i láº§n tÃ­ch cháº­p cÃ³ thá»ƒ lÃ m giáº£m kÃ­ch thÆ°á»›c khÃ´ng gian cá»§a áº£nh Ä‘áº§u ra. 
			
			
		## 3.3 Filter(Kernel)
			Filter hay cÃ²n gá»i lÃ  kernels lÃ  cÃ¡c ma tráº­n nhá» Ä‘Æ°á»£c Ã¡p dá»¥ng lÃªn áº£nh Ä‘áº§u vÃ o trong quÃ¡ trÃ¬nh tÃ­ch cháº­p. CÃ¡c bá»™ lá»c nÃ y thá»±c hiá»‡n viá»‡c quÃ©t qua toÃ n bá»™ áº£nh Ä‘áº§u vÃ o, tÃ­nh toÃ¡n cÃ¡c giÃ¡ trá»‹ má»›i dá»±a trÃªn phÃ©p nhÃ¢n tÃ­ch cháº­p giá»¯a bá»™ lá»c vÃ  cÃ¡c pháº§n tÆ°Æ¡ng á»©ng cá»§a áº£nh. 
			
			Má»—i bá»™ lá»c sáº½ phÃ¡t hiá»‡n cÃ¡c Ä‘áº·c trÆ°ng cá»¥ thá»ƒ, cháº³ng háº¡n nhÆ° cáº¡nh, Ä‘Æ°á»ng nÃ©t, hoáº·c chi tiáº¿t phá»©c táº¡p hÆ¡n á»Ÿ cÃ¡c lá»›p sÃ¢u. Má»—i lá»›p tÃ­ch cháº­p trong máº¡ng CNN cÃ³ thá»ƒ sá»­ dá»¥ng nhiá»u bá»™ lá»c Ä‘á»ƒ phÃ¡t hiá»‡n nhiá»u Ä‘áº·c trÆ°ng khÃ¡c nhau. 
			
			
		## 3.4 Feature Maps 
			Features Maps(Báº£n Ä‘á»“ Ä‘áº·c trÆ°ng) lÃ  káº¿t quáº£ Ä‘áº§u ra cá»§a má»™t lá»›p tÃ­ch cháº­p sau khi Ã¡p dá»¥ng cÃ¡c bá»™ lá»c lÃªn áº£nh Ä‘áº§u vÃ o. ÄÃ¢y lÃ  nÆ¡i lÆ°u trá»¯ cÃ¡c Ä‘áº·c trÆ°ng Ä‘Ã£ Ä‘Æ°á»£c phÃ¡t hiá»‡n bá»Ÿi bá»™ lá»c trong quÃ¡ trÃ¬nh tÃ­ch cháº­p. 
			
			Feature maps thá»ƒ hiá»‡n sá»± hiá»‡n diá»‡n cá»§a cÃ¡c Ä‘áº·c trÆ°ng(nhÆ° cáº¡nh, gÃ³c) trong má»™t bá»©c áº£nh táº¡i cÃ¡c vá»‹ trÃ­ khÃ´ng gian khÃ¡c nhau. CÃ¡c feature maps cÃ ng sÃ¢u trong máº¡ng CNN thÃ¬ chá»©a cÃ¡c Ä‘áº·c trÆ°ng cÃ ng phá»©c táº¡p, trá»«u tÆ°á»£ng hÆ¡n. 
			
			

	# Chuáº©n bá»‹ dá»¯ liá»‡u 
	
	TrÆ°á»›c khi bÃ¡t Ä‘áº§u quÃ¡ trÃ¬nh huáº¥n luyá»‡n, dá»¯ liá»‡u cáº§n Ä‘Æ°á»£c chuáº©n bá»‹ cáº©n tháº­n: 
		
		## Tiá»n xá»­ lÃ½ dá»¯ liá»‡u 
			CÃ¡c hÃ¬nh áº£nh Ä‘áº§u vÃ o thÆ°á»ng Ä‘Æ°á»£c Ä‘iá»u chá»‰nh kÃ­ch thÆ°á»›c, chuáº©n hÃ³a giÃ¡ trá»‹ pixel vá» má»™t khoáº£ng giÃ¡ trá»‹ nháº¥t Ä‘á»‹nh (vÃ­ dá»¥ tá»« 0 Ä‘áº¿n 1), vÃ  Ä‘Ã´i khi Ä‘Æ°á»£c Ã¡p dá»¥ng cÃ¡c ká»¹ thuáº­t tÄƒng cÆ°á»ng dá»¯ liá»‡u (data augmentation) nhÆ° xoay, láº­t áº£nh Ä‘á»ƒ tÄƒng tÃ­nh Ä‘a dáº¡ng cá»§a dá»¯ liá»‡u. 
			
		## vÃ­ dá»¥ code vá»›i pytorch 
			# 1. Äá»‹nh nghÄ©a cÃ¡c phÃ©p biáº¿n Ä‘á»•i (transform) cho dá»¯ liá»‡u
			transform = transforms.Compose([
				transforms.Resize((28, 28)),          # Thay Ä‘á»•i kÃ­ch thÆ°á»›c áº£nh vá» 28x28
				transforms.ToTensor(),                # Chuyá»ƒn Ä‘á»•i áº£nh thÃ nh tensor
				transforms.Normalize((0.1307,), (0.3081,))  # Chuáº©n hÃ³a dá»¯ liá»‡u vá»›i giÃ¡ trá»‹ trung bÃ¬nh vÃ  Ä‘á»™ lá»‡ch chuáº©n
			])


		## PhÃ¢n chia dá»¯ liá»‡u 
			Bá»™ dá»¯ liá»‡u thÆ°á»ng Ä‘Æ°á»£c chia thÃ nh 3 pháº§n : 
				- Dá»¯ liá»‡u huáº¥n luyá»‡n (training set)  : DÃ¹ng Ä‘á»ƒ huáº¥n luyá»‡n mÃ´ hÃ¬nh 
				- Dá»¯ liá»‡u kiá»ƒm Ä‘á»‹nh (validation set) : DÃ¹ng Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n nháº±m ngÄƒn cháº·n hiá»‡n tÆ°á»£ng overfitting 
				- Dá»¯ liá»‡u kiá»ƒm tra(test set) : DÃ¹ng Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh sau khi hoÃ n thÃ nh huáº¥n luyá»‡n. 
				
		## Truyá»n dá»¯ liá»‡u qua máº¡ng  
			QuÃ¡ trÃ¬nh huáº¥n luyá»‡n bÄƒt Ä‘áº§u báº±ng viá»‡c truyá»n dá»¯u liá»‡u qua máº¡ng CNN. CÃ¡c BÆ°á»›c chÃ­nh gá»“m  : 
				
				### Forward pass 
					Trong bÆ°á»›c nÃ y, dá»¯ liá»‡u Ä‘áº§u vÃ o Ä‘i qua cÃ¡c lá»›p cá»§a máº¡ng CNN, báº¯t Ä‘áº§u tá»« lá»›p tÃ­ch cháº­p, lá»›p kÃ­ch hoáº¡t(ReLU), Lá»›p pooling(giáº£m kÃ­ch thÆ°á»›c), vÃ  cuá»‘i cÃ¹ng lÃ  lá»›p fully connected (káº¿t ná»‘i Ä‘áº§y Ä‘á»§).
					
						- CÃ¡c lá»›p tÃ­ch cháº­p(convolutaional layers) thá»±c hiá»‡n tÃ­ch cháº­p vá»›i cÃ¡c bá»™ lá»c (filters) Ä‘á»ƒ phÃ¡t hiá»‡n cÃ¡c Ä‘áº·c trÆ°ng cá»§a áº£nh. 
						- CÃ¡c lá»›p pooling giáº£m kÃ­ch thÆ°á»›c cá»§a báº£n Ä‘á»“ Ä‘áº·c trÆ°ng(feature maps), giÃºp giáº£m khá»‘i lÆ°á»£ng tÃ­nh toÃ¡n vÃ  trÃ­ch xuáº¥t cÃ¡c Ä‘áº·c trÆ°ng chÃ­nh
						- Cuá»‘i cÃ¹ng, lá»›p fully connected táº¡o ra cÃ¡c dá»± Ä‘oÃ¡n (output) vá» lá»›p cá»§a áº£nh 
						
				### TÃ­nh toÃ¡n hÃ m máº¥t mÃ¡t (Loss Function)
					
					Sau khi nháº­n Ä‘Æ°á»£c Ä‘áº§u ra tá»« máº¡ng, má»™t hÃ m máº¥t mÃ¡t (loss function) Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ Ä‘o lÆ°á»ng sá»± khÃ¡c biá»‡t giá»¯a dá»± Ä‘oÃ¡n cá»§a máº¡ng vÃ  giÃ¡ trá»‹ nhÃ£n thá»±c táº¿ cá»§a dá»¯ liá»‡u. 
					
					HÃ m máº¥t mÃ¡t phá»• biáº¿n : Ä‘á»‘i vá»›i cÃ¡c bÃ i toÃ¡n phÃ¢n loáº¡i hÃ¬nh áº£nh, hÃ m máº¥t mÃ¡t phá»• biáº¿n nháº¥t lÃ  cross-entropy loss, giÃºp tÃ­nh toÃ¡n má»©c Ä‘á»™ sai lá»‡ch giá»¯a xÃ¡c suáº¥t dá»± Ä‘oÃ¡n cá»§a máº¡ng vÃ  nhÃ£n thá»±c táº¿. 
					
					VÃ­ dá»¥ : code pytorch sá»­ dá»¥ng cross-entropy loss : 
						model = SimpleCNN()
						criterion = nn.CrossEntropyLoss()  # HÃ m máº¥t mÃ¡t Cross-Entropy
						...
						...
						outputs = model(inputs)  # Forward pass
						loss = criterion(outputs, labels)  # TÃ­nh toÃ¡n hÃ m máº¥t mÃ¡t
					
			
			

				### Lan truyá»n ngÆ°á»£c (Back propagation)	
					Sau khi tÃ­nh toÃ¡n hÃ m máº¥t mÃ¡t, quÃ¡ trÃ¬nh lan truyá»n ngÆ°á»£c (backpropagation) báº¯t Ä‘áº§u Ä‘á»ƒ cáº­p nháº­t cÃ¡c tham sá»‘ (weights)  cá»§a máº¡ng: 
						
						#### TÃ­nh gradient : QuÃ¡ trÃ¬nh nÃ y sá»­ dá»¥ng Ä‘áº¡o hÃ m cá»§a hÃ m máº¥t mÃ¡t vá»›i tá»«ng trá»ng sá»‘ (weight) cá»§a cÃ¡c lá»›p thÃ´ng qua quy táº¯c chuá»—i (chain rule). Äiá»u nÃ y cho phÃ©p tÃ­nh toÃ¡n Ä‘Æ°á»£c gradient, cho biáº¿t trá»ng sá»‘ nÃ o cáº§n Ä‘iá»u chá»‰nh vÃ  Ä‘iá»u chá»‰nh bao nhiÃªu. 
						
						#### Cáº­p nháº­t weights : CÃ¡c tham sá»‘ weights cá»§a máº¡ng Ä‘Æ°á»£c cáº­p nháº­t báº±ng cÃ¡ch sá»­ dá»¥ng thuáº­t toÃ¡n tá»‘i Æ°u hÃ³a, phá»• biáº¿n nháº¥t lÃ  stochastic gradient descent(SGD) hoáº·c cÃ¡c biáº¿n thá»ƒ cá»§a nÃ³ nhÆ° Adam. CÃ¡c ham sá»‘ nÃ y Ä‘Æ°á»£c Ä‘iá»u chá»‰nh Ä‘á»ƒ lÃ m giáº£m máº¥t mÃ¡t trong cÃ¡c láº§n láº·p tiáº¿p theo. 
						
						
				### Early stopping 
					QuÃ¡ trÃ¬nh trÃªn (forward pass , tÃ­nh hÃ m máº¥t mÃ¡t, backpropagation) diá»…n ra trong nhiá»u láº§n láº·p gá»i lÃ  epoch 
					
					Trong má»—i epoch, máº¡ng sáº½ tráº£i qua toÃ n bá»™ dá»¯ liá»‡u huáº¥n luyá»‡n. Sau Ä‘Ã³, máº¡ng tiáº¿p tá»¥c Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ trÃªn bá»™ dá»¯ liá»‡u kiá»ƒm Ä‘á»‹nh(validation set) Ä‘á»ƒ theo dÃµi Ä‘á»™ chÃ­nh xÃ¡c vÃ  kiá»ƒm tra tÃ¬nh trjang overfitting.
					
					Early stopping : Trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n, náº¿u Ä‘á»™ chÃ­nh xÃ¡c trÃªn táº­p kiá»ƒm Ä‘á»‹nh báº¯t Ä‘áº§u giáº£m máº·c dÃ¹ Ä‘á»™ chÃ­nh xÃ¡c trÃªn táº­p huáº¥n luyá»‡n tÄƒng, quÃ¡ trÃ¬nh huáº¥n luyá»‡n cÃ³ thá»ƒ Ä‘Æ°á»£c dá»«ng sá»›m (early stopping) Ä‘á»ƒ trÃ¡nh hiá»‡n tÆ°á»£ng Overfitting.
					
			
		## á»¨ng dá»¥ng thá»±c táº¿ cá»§a CNN 
			Máº¡ng nÆ¡-ron tÃ­ch cháº­p (CNN) lÃ  má»™t trong nhá»¯ng cÃ´ng nghá»‡ quan trá»ng vÃ  Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i trong nhiá»u lÄ©nh vá»±c liÃªn quan Ä‘áº¿n xá»­ lÃ½ hÃ¬nh áº£nh vÃ  dá»¯ liá»‡u khÃ´ng gian. DÆ°á»›i Ä‘Ã¢y lÃ  cÃ¡c á»©ng dá»¥ng thá»±c táº¿ phá»• biáº¿n cá»§a CNN : 
			
			### 5.1 Image Recognition & Classification 
				CNN Ä‘Ã£ Ä‘áº¡t Ä‘Æ°á»£c nhiá»u thÃ nh cÃ´ng vÆ°á»£t trá»™i trong cÃ¡c bÃ i toÃ¡n nháº­n diá»‡n vÃ  phÃ¢n loáº¡i hÃ¬nh áº£nh. CÃ¡c mÃ´ hÃ¬nh CNN cÃ³ thá»ƒ phÃ¢n loáº¡i cÃ¡c Ä‘á»‘i tÆ°á»£ng trong áº£nh vá»›i Ä‘á»™ chÃ­nh xÃ¡c ráº¥t cao. 
				
				á»¨ng dá»¥ng : PhÃ¢n loáº¡i Ä‘á»™ng váº­t, nháº­n diá»‡n cÃ¡c loáº¡i phÆ°Æ¡ng tiá»‡n giao thÃ´ng, phÃ¢n loáº¡i hoa vÃ  thá»±c pháº©m. 
				
				VÃ­ dá»¥ thá»±c táº¿ : Trong Google Photos, CNN Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ phÃ¢n loáº¡i hÃ¬nh áº£nh thÃ nh cÃ¡c nhÃ³m dá»±a trÃªn ná»™i dung nhÆ° con ngÆ°á»i, phong cáº£nh, Ä‘á»“ váº­t. 
				
			### Optical Character Recognition - OCR 
				CNN KhÃ´ng chá»‰ á»©ng dá»¥ng cho hÃ¬nh áº£nh mÃ  cÃ²n Ä‘Æ°á»£c Ã¡p dá»¥ng trong xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn vÃ  nhiá»‡n diá»‡n kÃ½ tá»± quang há»c (OCR), giÃºp mÃ¡y tÃ­nh nháº­n diá»‡n vÃ  chuyá»ƒn Ä‘á»•i vÄƒn báº£n tá»« hÃ¬nh áº£nh sang dáº¡ng vÄƒn báº£n sá»‘ . 
				
				á»¨ng dá»¥ng : QuÃ©t vÃ  nháº­n diá»‡n vÄƒn báº£n trong cÃ¡c tÃ i liá»‡u giáº¥y, biá»ƒn bÃ¡o giao thÃ´ng, sÃ¡ch, hoáº·c hÃ³a Ä‘Æ¡n. 
				VÃ­ dá»¥ thá»±c táº¿ : Google Translate sá»­ dá»¥ng trong CNN trong tÃ­nh nÄƒng dá»‹ch trá»±c tiáº¿p tá»« hÃ¬nh áº£nh báº±ng cÃ¡ch nháº­n diá»‡n vÄƒn báº£n trong áº£nh vÃ  dá»‹ch sang ngÃ´n ngá»¯ khÃ¡c. 
				
			### Image Segmentation 
				PhÃ¢n Ä‘oáº¡n áº£nh lÃ  quÃ¡ trÃ¬nh chia má»™t bá»©c áº£nh thÃ nh cÃ¡c thÃ nh pháº§n hoáº·c Ä‘á»‘i tÆ°á»£ng khÃ¡c nhau dá»±a trÃªn cÃ¡c Ä‘áº·c Ä‘iá»ƒm. CNN giÃºp phÃ¢n Ä‘oáº¡n cÃ¡c vÃ¹ng khÃ¡c nhau trong áº£nh vÃ  xÃ¡c Ä‘á»‹nh cÃ¡c Ä‘áº·c trÆ°ng riÃªng biá»‡t cá»§a tá»«ng vÃ¹ng. 
				
				 á»¨ng dá»¥ng : 
					PhÃ¢n Ä‘oáº¡n trong y táº¿ Ä‘á»ƒ xÃ¡c Ä‘á»‹nh cÃ¡c táº¿ bÃ o ung thÆ° hoáº·c cÃ¡c mÃ´ báº¥t thÆ°á»ng trong áº£nh y khoa, phÃ¢n Ä‘oáº¡n Ä‘Æ°á»ng trong hÃ¬nh áº£nh vá»‡ tinh . 
					
				 VÃ­ dá»¥ thá»±c táº¿ : 
					Trong y há»c, CNN giÃºp phÃ¢n Ä‘oáº¡n cÃ¡c cÆ¡ quan ná»™i táº¡ng hoáº·c vÃ¹ng mÃ´ ung thÆ° tá»« cÃ¡c áº£nh chá»¥p CT hoáº·c MRI< há»— trá»£ chuáº©n Ä‘oÃ¡n vÃ  pháº«u thuáº­t. 
					
			### Autonomous Driving 
				CNN Ä‘Ã³ng vai trÃ² quan trá»ng trong cÃ¡c há»‡ thá»‘ng láº¡i xe tá»± Ä‘á»™ng, giÃºp xe nháº­n diá»‡n vÃ  phÃ¢n tÃ­ch mÃ´i trÆ°á»ng xung quanh, tá»« Ä‘Ã³ Ä‘Æ°a ra quyáº¿t Ä‘á»‹nh Ä‘iá»u hÆ°á»›ng vÃ  trÃ¡nh váº­t cáº£n. 
				
				á»¨ng dá»¥ng : 
					Nháº­n diá»‡n biá»ƒn bÃ¡o giao thÃ´ng, lÃ n Ä‘Æ°á»ng, ngÆ°á»i Ä‘i bá»™, phÆ°Æ¡ng tiá»‡n khÃ¡c vÃ  cÃ¡c váº­t thá»ƒ nguy hiá»ƒm. 
				VÃ­ dá»¥ thá»±c táº¿ : 
					Xe tá»± lÃ¡i cá»§a Tesla sá»­ dá»¥ng CNN Ä‘á»ƒ xá»­ lÃ½ thÃ´ng tin tá»« camera vÃ  cáº£m biáº¿n, giÃºp xe di chuyá»ƒn an toÃ n trÃªn Ä‘Æ°á»ng. 
					
				
# Káº¿t luáº­n 
				Convolutional Neural Network(CNN) lÃ  má»™t trong nhá»¯ng mÃ´ hÃ¬nh quan tá»ng vÃ  máº¡nh máº½ nháº¥t trong lÄ©nh vá»±c Deep Learning, Ä‘áº·c biá»‡t hiá»‡u quáº£ cho cÃ¡c bÃ i toÃ¡n xá»­ lÃ½ hÃ¬nh áº£nh vÃ  nháº­n diá»‡n thá»‹ giÃ¡c. CNN táº­n dá»¥ng cÃ¡c lá»›p tÃ­ch cháº­p Ä‘á»ƒ tá»± Ä‘á»™ng trÃ­ch xuáº¥t cÃ¡c Ä‘áº·c trÆ°ng tá»« dá»¯ liá»‡u mÃ  khÃ´ng cáº§n can thiá»‡p cá»§a con ngÆ°á»i trong viá»‡c lá»±a chá»n cÃ¡c Ä‘áº·c trÆ°ng phÃ¹ há»£p .
				
				Nhá» vÃ o cáº¥u trÃºc phÃ¢n cáº¥p, CNN cÃ³ thá»ƒ há»c tá»« cÃ¡c Ä‘áº·c trÆ°ng cÆ¡ báº£n (nhÆ° cáº¡nh, gÃ³c) Ä‘áº¿n cÃ¡c Ä‘áº·c trÆ°ng phá»©c táº¡p hÆ¡n (nhÆ° hÃ¬nh dáº¡ng, Ä‘á»‘i tÆ°á»£ng) khi Ä‘á»™ sÃ¢u cá»§a máº¡ng tÄƒng lÃªn. CÃ¡c lá»›p tÃ­ch cháº­p, lá»›p gá»™p, vÃ  lá»›p fully connected káº¿t há»£p vá»›i nhau giÃºp CNN cÃ³ kháº£ nÄƒng xá»­ lÃ½ dá»¯ liá»‡u lá»›n vÃ  Ä‘áº¡t Ä‘á»™ chÃ­nh xÃ¡c cao trong cÃ¡c bÃ i toÃ¡n phÃ¢n loáº¡i, nháº­n diá»‡n Ä‘á»‘i tÆ°á»£ng, vÃ  cÃ¡c nhiá»‡m vá»¥ khÃ¡c liÃªn quan Ä‘áº¿n thá»‹ giÃ¡c mÃ¡y tÃ­nh. 
				Máº·c dÃ¹ cÃ³ cáº¥u trÃºc phá»©c táº¡p vÃ  Ä‘Ã²i há»i nhiá»u tÃ i nguyÃªn tÃ­nh toÃ¡n, CNN váº«n Ä‘ang Ä‘Æ°á»£c nghiÃªn cá»©u vÃ  cáº£i thiá»‡n vá»›i cÃ¡c phiÃªn báº£n tiÃªn tiáº¿n hÆ¡n nhÆ° ResNet, Inception, YOLO, vÃ  nhiá»u hÆ¡n ná»¯a, giÃºp nÃ¢ng cao hiá»‡u quáº£ vÃ  tá»‘c Ä‘á»™ huáº¥n luyá»‡n mÃ´ hÃ¬nh. 
				
				
//====================== Recurrent Neural Network (RNN)	: á»¨ng dá»¥ng vÃ  cÃ¡ch hoáº¡t Ä‘á»™ng. ================================


# KhÃ¡i niá»‡m 
	Recurrent Neural Networks(RNN) lÃ  má»™t loáº¡i máº¡ng nÆ¡-ron Ä‘áº·c biá»‡t Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ xá»­ lÃ½ dá»¯ liá»‡u tuáº§n tá»±. KhÃ¡c vá»›i cÃ¡c máº¡ng nÆ¡-ron thÃ´ng thÆ°á»ng, RNN cÃ³ kháº£ nÄƒng ghi nhá»› thÃ´ng tin tá»« cÃ¡c bÆ°á»›c trÆ°á»›c Ä‘Ã³ nhá» cÆ¡ cháº¿ pháº£n há»“i (recurrent), cho phÃ©p mÃ´ hÃ¬nh cÃ³ thá»ƒ táº­n dá»¥ng ngá»¯ cáº£nh cá»§a dá»¯ liá»‡u trÆ°á»›c Ä‘á»ƒ dá»± Ä‘oÃ¡n hoáº·c suy luáº­n dá»¯ liá»‡u hiá»‡n táº¡i. Äiá»u nÃ y lÃ m cho RNN Ä‘áº·c biá»‡t hiá»‡u quáº£ trong viá»‡c phÃ¢n tÃ­ch dá»¯ liá»‡u chuá»—i thá»i gian, xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn, dá»‹ch mÃ¡y, vÃ  nháº­n diá»‡n giá»ng nÃ³i, nÆ¡i mÃ  thÃ´ng tin tá»« quÃ¡ khá»© lÃ  ráº¥t quan trá»ng Ä‘á»ƒ hiá»ƒu chÃ­nh xÃ¡c hiá»‡n táº¡i. 
	

# CÃ¡ch hoáº¡t Ä‘á»™ng cá»§a Recurrent Neural Network(RNN)	
	
https://aicandy.vn/wp-content/uploads/2024/09/aicandy_RNN.png

	
	## Hidden State 
		Äiá»ƒm khÃ¡c biá»‡t lá»›n nháº¥t cá»§a RNN so vá»›i cÃ¡c máº¡ng nÆ¡-ron truyá»n thá»‘ng lÃ  kháº£ nÄƒng lÆ°u trá»¯ vÃ  cáº­p nháº­t thÃ´ng tin qua cÃ¡c bÆ°á»›c cá»§a chuá»—i thá»i gian. Táº¡i má»—i bÆ°á»›c thá»i gian(timestep), RNN duy trÃ¬ má»™t biáº¿n gá»i lÃ  tráº¡ng thÃ¡i áº©n (h_t), biá»ƒu diá»…n bá»™ nhá»› cá»§a mÃ´ hÃ¬nh vá» cÃ¡c thÃ´ng tin Ä‘Ã£ nháº­n Ä‘Æ°á»£c tá»« nhá»¯ng bÆ°á»›c trÆ°á»›c Ä‘Ã³. Tráº¡ng thÃ¡i áº©n nÃ y Ä‘Æ°á»£c cáº­p nháº­t táº¡i má»—i bÆ°á»›c thá»i gian dá»±a trÃªn Ä‘áº§u vÃ o vÃ  hiá»‡n táº¡i vÃ  tráº¡ng thÃ¡i áº©n tá»« bÆ°á»›c trÆ°á»›c Ä‘Ã³. 
		CÃ´ng thá»©c tÃ­nh toÃ¡n tráº¡ng thÃ¡i áº©n táº¡i má»—i bÆ°á»›c thá»i gian cÃ³ thá»ƒ Ä‘Æ°á»£c biá»ƒu diá»…n nhÆ° sau 
			h_t = f(W_{xh}x_t + W_{hh}h_{t-1} + b_h)
			
			
		Trong Ä‘Ã³ : 
			h_t : Tráº¡ng thÃ¡i áº©n táº¡i thá»i Ä‘iá»ƒm t 
			x_t : Äáº§u vÃ o táº¡i thá»i Ä‘iá»ƒm t 
			W_{xh} : Ma tráº­n trá»ng sá»‘ káº¿t ná»‘i Ä‘áº§u vÃ o vá»›i tráº¡ng thÃ¡i áº©n. 
			W_{hh} : Ma tráº­n trá»ng sá»‘ káº¿t ná»‘i tráº¡ng thÃ¡i áº©n trÆ°á»›c Ä‘Ã³ vá»›i tráº¡ng thÃ¡i áº©n hiá»‡n táº¡i 
			b_h	   : tham sá»‘ bias cho tráº¡ng thÃ¡i áº©n 
			f 	: HÃ m kÃ­ch hoáº¡t (thÆ°á»ng lÃ  hÃ m tanh hoáº·c ReLU 
			
			)
			
	## Output  
		RNN cÃ³ thá»ƒ táº¡o Ä‘áº§u ra táº¡i má»—i bÆ°á»›c thá»i gian, thÆ°á»ng Ä‘Æ°á»£c tÃ­nh toÃ¡n dá»±a trÃªn tráº¡ng thÃ¡i áº©n táº¡i thá»i Ä‘iá»ƒm Ä‘Ã³. Äáº§u ra y_t táº¡i thá»i Ä‘iá»ƒm t Ä‘Æ°á»£c tÃ­nh báº±ng cÃ´ng thá»©c : 
				y_t = W_{hy}h_t + b_y
				

		Trong Ä‘Ã³ : 
			y_t : Äáº§u ra táº¡i thá»i Ä‘iá»ƒm t 
			W_{hy}	: Ma tráº­n trá»ng sá»‘ káº¿t ná»‘i tráº¡ng thÃ¡i áº©n vá»›i Ä‘áº§u ra 
			b_y		: Tham sá»‘ bias cho Ä‘áº§u ra 
			
	## Backpropagation 
		Backpropagation lÃ  thuáº­t toÃ¡n lan truyá»n ngÆ°á»£c trong cÃ¡c máº¡ng nÆ¡-ron thÃ´ng thÆ°á»ng. á» má»—i lá»›p cá»§a máº¡ng nÆ¡-ron, giÃ¡ trá»‹ Ä‘áº§u ra Ä‘Æ°á»£c tÃ­nh toÃ¡n dá»±a trÃªn Ä‘áº§u vÃ o cÃ¡c trá»ng sá»‘ tÆ°Æ¡ng á»©ng. Sau Ä‘Ã³ , Ä‘á»ƒ tá»‘i Æ°u hÃ³a mÃ´ hÃ¬nh, ta cáº§n tÃ­nh toÃ¡n gradient cá»§a hÃ m máº¥t mÃ¡t \mathcal{L} theo cÃ¡c trá»ng sá»‘ nÃ y , giÃºp ta Ä‘iá»u chá»‰nh cÃ¡c trá»ng sá»‘ sao cho mÃ´ hÃ¬nh cÃ³ thá»ƒ Ä‘Æ°a ra dá»± Ä‘oÃ¡n chÃ­nh xÃ¡c hÆ¡n 
		
		CÃ´ng thá»©c tá»•ng quÃ¡t cá»§a lan truyá»n ngÆ°á»£c lÃ  : 
			\frac{\partial \mathcal{L}}{\partial W} = \frac{\partial \mathcal{L}}{\partial y} \cdot \frac{\partial y}{\partial W}
			
		Trong Ä‘Ã³ : 
			\frac{\partial \mathcal{L}}{\partial W}		: lÃ  gradient cá»§a hÃ m máº¥t mÃ¡t theo trá»ng sá»‘ W 
			\frac{\partial \mathcal{L}}{\partial y}		: lÃ  gradient cá»§a hÃ m máº¥t mÃ¡t theo Ä‘áº§u ra y 
			\frac{\partial y}{\partial W}				: lÃ  gradient cá»§a Ä‘áº§u ra theo trá»ng sá»‘ W 
			
			QuÃ¡ trÃ¬nh nÃ y Ä‘Æ°á»£c Ã¡p dá»¥ng cho táº¥t cáº£ cÃ¡c trá»ng sá»‘ trong máº¡ng vÃ  Ä‘Æ°á»£c thá»±c hiá»‡n tá»« Ä‘áº§u ra trá»Ÿ ngÆ°á»£c láº¡i Ä‘áº§u vÃ o, tá»«ng lá»›p má»™t. Khi Ä‘Ã£ tÃ­nh Ä‘Æ°á»£c gradient cÃ¡c trá»ng sá»‘ sáº½ Ä‘Æ°á»£c cáº­p nháº­t báº±ng cÃ¡ch sá»­ dá»¥ng má»™t phÆ°Æ¡ng phÃ¡p tá»‘i Æ°u hÃ³a nhÆ° gradient descent 
			



	##	Backpropagation Through Time (BPTT) 
			
		Trong cÃ¡c máº¡ng RNN, dá»¯ liá»‡u Ä‘Æ°á»£c xá»­ lÃ½ tuáº§n tá»± qua nhiá»u bÆ°á»›c thá»i gian. Äiá»u nÃ y cÃ³ nghÄ©a lÃ  tráº¡ng thÃ¡i áº©n táº¡i thá»i Ä‘iá»ƒm t phá»¥ thuá»™c vÃ o cáº£ Ä‘áº§u vÃ o táº¡i thá»i Ä‘iá»ƒm Ä‘Ã³ vÃ  tráº¡ng thÃ¡i áº©n tá»« bÆ°á»›c tÆ°á»›c Ä‘Ã³. Do Ä‘Ã³, khi tÃ­nh toÃ¡n gradient, ta khÃ´ng chá»‰ lan truyá»n ngÆ°á»£c qua cÃ¡c lá»›p nhÆ° trong máº¡ng nÆ¡-ron truyá»n thá»‘ng mÃ  cÃ²n pháº£i lan truyá»n qua cÃ¡c bÆ°á»›c thá»i gian trÆ°á»›c Ä‘Ã³. ÄÃ¢y lÃ  lÃ½ do thuáº­t toÃ¡n BPTT Ä‘Æ°á»£c ra Ä‘á»i. 
			https://aicandy.vn/wp-content/uploads/2024/09/aicandy_RNN_backpropagation.jpg
		
		Trong BPTT, máº¡ng RNN Ä‘Æ°á»£c má»Ÿ rá»™ng qua thá»i gian, má»—i bÆ°á»›c thá»i gian cá»§a RNN Ä‘Æ°á»£c coi nhÆ° má»™t lá»›p riÃªng biá»‡t. Khi lan truyá»n ngÆ°á»£c, gradient khÃ´ng chá»‰ Ä‘Æ°á»£c lan truyá»n qua cÃ¡c trá»ng sá»‘ cá»§a cÃ¡c lá»›p trong máº¡ng, mÃ  cÃ²n pháº£i lan truyá»n ngÆ°á»£c qua cÃ¡c bÆ°á»›c thá»i gian Ä‘á»ƒ tÃ­nh toÃ¡n áº£nh hÆ°á»Ÿng cá»§a cÃ¡c tráº¡ng thÃ¡i áº©n trÆ°á»›c Ä‘Ã³ Ä‘áº¿n lá»—i hiá»‡n táº¡i 
		
		### CÃ´ng thá»©c tá»•ng quÃ¡t cá»§a BPTT lÃ  : 
		
			\frac{\partial \mathcal{L}}{\partial W} = \sum_{t=1}^{T} \frac{\partial \mathcal{L}_t}{\partial y_t} \cdot \frac{\partial y_t}{\partial h_t} \cdot \frac{\partial h_t}{\partial h_{t-1}} \cdot \frac{\partial h_{t-1}}{\partial W}


		Trong Ä‘Ã³ : 
			\frac{\partial \mathcal{L}_t}{\partial y_t} : lÃ  gradient cá»§a hÃ m máº¥t mÃ¡t táº¡i bÆ°á»›c thá»i gian t theo Ä‘áº§u ra yt 
			\frac{\partial y_t}{\partial h_t}	: lÃ  gradient cá»§a Ä‘áº§u ra táº¡i thá»i Ä‘iá»ƒm t theo tráº¡ng thÃ¡i áº©n ht 
			\frac{\partial h_t}{\partial h_{t-1}}	: lÃ  gradient cá»§a tráº¡ng thÃ¡i áº©n hiá»‡n táº¡i theo tráº¡ng thÃ¡i áº©n trÆ°á»›c Ä‘Ã³ 
			\frac{\partial h_{t-1}}{\partial W}		: lÃ  gradient cá»§a tráº¡ng thÃ¡i áº©n trÆ°á»›c theo trá»ng sá»‘ W 
			
			
	## Triá»ƒn khai RNN báº±ng PyTorch 
			DÆ°á»›i Ä‘Ã¢y lÃ  cÃ¡ch triá»ƒn khai má»™t máº¡ng RNN Ä‘Æ¡n giáº£n báº±ng PyTorch. Máº¡ng sáº½ dá»± Ä‘oÃ¡n sá»‘ tiáº¿p theo cá»§a chuá»—i [1, 2, 3, 4].
			RNNByPytorch.py


		Giáº£i thÃ­ch:	

	https://aicandy.vn/recurrent-neural-network-rnn-ung-dung-va-cach-hoat-dong/




				
				
				
				
				
		
</pre><a id='backBottom' href='../java-learning-list.html' style='display:none;'>ğŸ”™ Quay láº¡i danh sÃ¡ch</a><br><button onclick='toggleTheme()'>ğŸŒ™ Chuyá»ƒn giao diá»‡n</button></div><script>function toggleTheme() {   let mode = document.body.classList.contains('dark-mode') ? 'light-mode' : 'dark-mode';   document.body.className = mode; localStorage.setItem('theme', mode);   syncTheme();}function applyTheme() {   let savedTheme = localStorage.getItem('theme') || 'dark-mode';   document.body.className = savedTheme;   syncTheme();}function syncTheme() {   let preElement = document.querySelector('pre');   if (document.body.classList.contains('dark-mode')) { preElement.style.background = '#1e1e1e'; preElement.style.color = '#e0e0e0'; }   else { preElement.style.background = '#f5f5f5'; preElement.style.color = '#333333'; }}function checkPageHeight() {   let contentHeight = document.body.scrollHeight;   let windowHeight = window.innerHeight;   if (contentHeight > windowHeight * 1.2) {       document.getElementById('backBottom').style.display = 'block';   } else {       document.getElementById('backBottom').style.display = 'none';   }}</script></body></html>