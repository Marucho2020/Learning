<html><head><title>Lesson 21 == Há»c tÄƒng cÆ°á»ng (Reinforcement Learning): TÃ¬m hiá»ƒu chi tiáº¿t ==========//</title><style>body { font-family: Arial, sans-serif; transition: background 0.3s, color 0.3s; }.dark-mode { background-color: #121212; color: #e0e0e0; }.light-mode { background-color: #ffffff; color: #333333; }h1 { text-align: center; color: #73d9f5; }pre { padding: 15px; border-radius: 5px;       white-space: pre-wrap; word-wrap: break-word;       overflow-x: auto; max-width: 100%;       transition: background 0.3s, color 0.3s; }.dark-mode pre { background: #1e1e1e; color: #e0e0e0; }.light-mode pre { background: #f5f5f5; color: #333333; }#backTop, #backBottom {    font-size: 2em; padding: 20px 40px;    background: #bb86fc; color: white; text-decoration: none;    border-radius: 10px; display: inline-block; text-align: center; }#backTop:hover, #backBottom:hover { background: #9b67e2; }button { font-size: 1.5em; padding: 15px 30px;    background: #03dac6; color: #121212; border: none;    cursor: pointer; border-radius: 5px; display: block; margin: 10px auto; }button:hover { background: #02b8a3; }.dark-mode a { color: #03dac6; } .light-mode a { color: #007bff; }</style></head><body onload='applyTheme(); checkPageHeight()'><div class='container'><a id='backTop' href='../AI-learning-list.html'>ğŸ”™ Quay láº¡i danh sÃ¡ch</a><br><h1>Lesson 21 -- Há»c tÄƒng cÆ°á»ng (Reinforcement Learning): TÃ¬m hiá»ƒu chi tiáº¿t -//</h1><pre>				
		
# 	1. Giá»›i thiá»‡u 
		
Há»c tÄƒng cÆ°á»ng (Reinforcement Learning â€“ RL) lÃ  má»™t nhÃ¡nh quan trá»ng cá»§a há»c mÃ¡y, Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ giáº£i quyáº¿t cÃ¡c bÃ i toÃ¡n mÃ  trong Ä‘Ã³ má»™t tÃ¡c nhÃ¢n (agent) tÆ°Æ¡ng tÃ¡c vá»›i mÃ´i trÆ°á»ng Ä‘á»ƒ há»c cÃ¡ch Ä‘áº¡t Ä‘Æ°á»£c má»¥c tiÃªu thÃ´ng qua pháº§n thÆ°á»Ÿng (reward) vÃ  hÃ¬nh pháº¡t (penalty).

Thay vÃ¬ há»c tá»« má»™t táº­p dá»¯ liá»‡u cá»‘ Ä‘á»‹nh nhÆ° trong há»c cÃ³ giÃ¡m sÃ¡t (supervised learning), tÃ¡c nhÃ¢n sáº½ há»c thÃ´ng qua quÃ¡ trÃ¬nh thá»­ vÃ  sai (trial and error). Má»—i hÃ nh Ä‘á»™ng mÃ  tÃ¡c nhÃ¢n thá»±c hiá»‡n sáº½ nháº­n vá» má»™t pháº§n thÆ°á»Ÿng hoáº·c hÃ¬nh pháº¡t, vÃ  má»¥c tiÃªu cá»§a tÃ¡c nhÃ¢n lÃ  há»c Ä‘Æ°á»£c cÃ¡ch tá»‘i Ä‘a hÃ³a tá»•ng pháº§n thÆ°á»Ÿng nháº­n Ä‘Æ°á»£c trong dÃ i háº¡n.

# 	2. CÃ¡c thÃ nh pháº§n chÃ­nh		
		
	## TÃ¡c nhÃ¢n (Agent) 
		ÄÃ¢y lÃ  thá»±c thá»ƒ Ä‘Æ°a ra cÃ¡c hÃ nh Ä‘á»™ng. TÃ¡c nhÃ¢n cÃ³ thá»ƒ lÃ  má»™t chÆ°Æ¡ng trÃ¬nh AI Ä‘ang chÆ¡i má»™t trÃ² chÆ¡i, má»™t robot Ä‘ang thá»±c hiá»‡n cÃ¡c bÆ°á»›c di chuyá»ƒn, hay tháº­m chÃ­ lÃ  má»™t há»‡ thá»‘ng tá»± Ä‘á»™ng hÃ³a trong nhÃ  mÃ¡y.
		
	## MÃ´i trÆ°á»ng (Environment) 
		LÃ  táº¥t cáº£ nhá»¯ng gÃ¬ xung quanh tÃ¡c nhÃ¢n vÃ  cÃ³ thá»ƒ tÆ°Æ¡ng tÃ¡c vá»›i nÃ³. MÃ´i trÆ°á»ng pháº£n há»“i láº¡i cÃ¡c hÃ nh Ä‘á»™ng cá»§a tÃ¡c nhÃ¢n báº±ng cÃ¡ch cung cáº¥p tráº¡ng thÃ¡i má»›i vÃ  pháº§n thÆ°á»Ÿng tÆ°Æ¡ng á»©ng. MÃ´i trÆ°á»ng cÃ³ thá»ƒ lÃ  váº­t lÃ½ (tháº¿ giá»›i thá»±c) hoáº·c áº£o (mÃ´ phá»ng trÃ² chÆ¡i, mÃ´ phá»ng nhÃ  mÃ¡y).
		
	## Tráº¡ng thÃ¡i (State) 
		Tráº¡ng thÃ¡i biá»ƒu diá»…n má»™t mÃ´ táº£ ngáº¯n gá»n nhÆ°ng Ä‘áº§y Ä‘á»§ vá» tÃ¬nh huá»‘ng hiá»‡n táº¡i cá»§a mÃ´i trÆ°á»ng mÃ  tÃ¡c nhÃ¢n Ä‘ang pháº£i Ä‘á»‘i máº·t. Táº¡i má»—i thá»i Ä‘iá»ƒm, tÃ¡c nhÃ¢n nháº­n má»™t tráº¡ng thÃ¡i tá»« mÃ´i trÆ°á»ng vÃ  sá»­ dá»¥ng tráº¡ng thÃ¡i nÃ y Ä‘á»ƒ quyáº¿t Ä‘á»‹nh hÃ nh Ä‘á»™ng tiáº¿p theo.VÃ­ dá»¥: Trong trÃ² chÆ¡i cá» vua, tráº¡ng thÃ¡i lÃ  vá»‹ trÃ­ cá»§a táº¥t cáº£ cÃ¡c quÃ¢n cá» trÃªn bÃ n cá» táº¡i má»™t thá»i Ä‘iá»ƒm nháº¥t Ä‘á»‹nh.
		
	## 	HÃ nh Ä‘á»™ng (Action) 
		LÃ  má»™t lá»±a chá»n mÃ  tÃ¡c nhÃ¢n cÃ³ thá»ƒ thá»±c hiá»‡n táº¡i má»™t tráº¡ng thÃ¡i cá»¥ thá»ƒ. Má»—i hÃ nh Ä‘á»™ng cÃ³ thá»ƒ dáº«n Ä‘áº¿n má»™t tráº¡ng thÃ¡i má»›i cá»§a mÃ´i trÆ°á»ng. Má»¥c tiÃªu cá»§a tÃ¡c nhÃ¢n lÃ  chá»n hÃ nh Ä‘á»™ng sao cho tá»‘i Ä‘a hÃ³a pháº§n thÆ°á»Ÿng nháº­n Ä‘Æ°á»£c.VÃ­ dá»¥: Trong trÃ² chÆ¡i cá» vua, má»™t hÃ nh Ä‘á»™ng cÃ³ thá»ƒ lÃ  di chuyá»ƒn quÃ¢n mÃ£ tá»« Ã´ nÃ y sang Ã´ khÃ¡c.
		
	## Pháº§n thÆ°á»Ÿng (Reward) 
		LÃ  giÃ¡ trá»‹ pháº£n há»“i mÃ  mÃ´i trÆ°á»ng tráº£ láº¡i sau khi tÃ¡c nhÃ¢n thá»±c hiá»‡n má»™t hÃ nh Ä‘á»™ng. Pháº§n thÆ°á»Ÿng cÃ³ thá»ƒ lÃ  dÆ°Æ¡ng (thÆ°á»Ÿng) náº¿u hÃ nh Ä‘á»™ng cá»§a tÃ¡c nhÃ¢n lÃ  Ä‘Ãºng, hoáº·c Ã¢m (pháº¡t) náº¿u hÃ nh Ä‘á»™ng lÃ  sai. TÃ¡c nhÃ¢n sá»­ dá»¥ng thÃ´ng tin nÃ y Ä‘á»ƒ Ä‘iá»u chá»‰nh hÃ nh Ä‘á»™ng cá»§a mÃ¬nh trong tÆ°Æ¡ng lai.VÃ­ dá»¥: Trong trÃ² chÆ¡i cá» vua, pháº§n thÆ°á»Ÿng cÃ³ thá»ƒ lÃ  giÃ¡ trá»‹ +1 náº¿u tÃ¡c nhÃ¢n báº¯t Ä‘Æ°á»£c quÃ¢n Ä‘á»‘i thá»§ hoáº·c giÃ¡ trá»‹ -1 náº¿u bá»‹ máº¥t quÃ¢n.
		
	## ChÃ­nh sÃ¡ch (Policy)
		ChÃ­nh sÃ¡ch lÃ  chiáº¿n lÆ°á»£c mÃ  tÃ¡c nhÃ¢n sá»­ dá»¥ng Ä‘á»ƒ chá»n hÃ nh Ä‘á»™ng dá»±a trÃªn tráº¡ng thÃ¡i hiá»‡n táº¡i. ChÃ­nh sÃ¡ch cÃ³ thá»ƒ Ä‘Æ°á»£c biá»ƒu diá»…n dÆ°á»›i dáº¡ng má»™t hÃ m toÃ¡n há»c hoáº·c má»™t báº£ng tra cá»©u.VÃ­ dá»¥: Trong trÃ² chÆ¡i cá» vua, chÃ­nh sÃ¡ch cÃ³ thá»ƒ quyáº¿t Ä‘á»‹nh ráº±ng khi má»™t quÃ¢n mÃ£ á»Ÿ vá»‹ trÃ­ X, tÃ¡c nhÃ¢n nÃªn di chuyá»ƒn nÃ³ Ä‘áº¿n vá»‹ trÃ­ Y Ä‘á»ƒ tá»‘i Ä‘a hÃ³a cÆ¡ há»™i tháº¯ng.

	## HÃ m giÃ¡ trá»‹ (Value function) 
		HÃ m giÃ¡ trá»‹ lÃ  má»™t hÃ m dá»± Ä‘oÃ¡n giÃ¡ trá»‹ tÆ°Æ¡ng lai mÃ  tÃ¡c nhÃ¢n cÃ³ thá»ƒ nháº­n Ä‘Æ°á»£c tá»« má»™t tráº¡ng thÃ¡i nháº¥t Ä‘á»‹nh. HÃ m nÃ y giÃºp tÃ¡c nhÃ¢n Ä‘Ã¡nh giÃ¡ lá»£i Ã­ch cá»§a má»™t tráº¡ng thÃ¡i Ä‘á»ƒ chá»n hÃ nh Ä‘á»™ng tá»‘t nháº¥t.VÃ­ dá»¥: Trong má»™t trÃ² chÆ¡i, tráº¡ng thÃ¡i A cÃ³ giÃ¡ trá»‹ 5, tráº¡ng thÃ¡i B cÃ³ giÃ¡ trá»‹ 10. Äiá»u nÃ y cÃ³ nghÄ©a lÃ  tráº¡ng thÃ¡i B cÃ³ thá»ƒ mang láº¡i pháº§n thÆ°á»Ÿng cao hÆ¡n trong dÃ i háº¡n, vÃ  tÃ¡c nhÃ¢n nÃªn cá»‘ gáº¯ng chuyá»ƒn Ä‘áº¿n tráº¡ng thÃ¡i B. 
		
	## HÃ m Q (Q-function) 
		HÃ m Q Ä‘Ã¡nh giÃ¡ giÃ¡ trá»‹ cá»§a viá»‡c thá»±c hiá»‡n má»™t hÃ nh Ä‘á»™ng cá»¥ thá»ƒ táº¡i má»™t tráº¡ng thÃ¡i cá»¥ thá»ƒ. HÃ m nÃ y giÃºp tÃ¡c nhÃ¢n chá»n hÃ nh Ä‘á»™ng tá»‘i Æ°u báº±ng cÃ¡ch so sÃ¡nh giÃ¡ trá»‹ cá»§a cÃ¡c hÃ nh Ä‘á»™ng kháº£ thi. 
		
		CÃ´ng thá»©c cÆ¡ báº£n cho hÃ m Q trong bÃ i toÃ¡n há»c tÄƒng cÆ°á»ng lÃ :	
			Q(s, a) = R(s, a) + \gamma \sum_{sâ€™} P(sâ€™|s, a) \max_{aâ€™} Q(sâ€™, aâ€™)
			
		Trong Ä‘Ã³ : 
			-  Q(s, a)	 lÃ  giÃ¡ trá»‹ Q cá»§a hÃ nh Ä‘á»™ng a táº¡i tráº¡ng thÃ¡i s
			- R(s, a) lÃ  pháº§n thÆ°á»Ÿng nháº­n Ä‘Æ°á»£c sau khi thá»±c hiá»‡n hÃ nh Ä‘á»™ng a táº¡i tráº¡ng thÃ¡i  s
			- \gamma lÃ  há»‡ sá»‘ giáº£m giÃ¡ (discount factor), mÃ´ táº£ má»©c Ä‘á»™ mÃ  tÃ¡c nhÃ¢n Ä‘Ã¡nh giÃ¡ pháº§n thÆ°á»Ÿng trong tÆ°Æ¡ng lai. 
			- P(sâ€™|s, a)  lÃ  xÃ¡c suáº¥t chuyá»ƒn tráº¡ng thÃ¡i tá»«  s sang  s' sau khi thá»±c hiá»‡n hÃ nh Ä‘á»™ng a

		CÃ´ng thá»©c tá»•ng pháº§n thÆ°á»Ÿng tÃ­ch lÅ©y: 
			Pháº§n thÆ°á»Ÿng mÃ  tÃ¡c nhÃ¢n nháº­n Ä‘Æ°á»£c khÃ´ng chá»‰ quan tÃ¢m Ä‘áº¿n pháº§n thÆ°á»Ÿng tá»©c thá»i mÃ  cÃ²n bao gá»“m tá»•ng pháº§n thÆ°á»Ÿng trong tÆ°Æ¡ng lai. Tá»•ng pháº§n thÆ°á»Ÿng tÃ­ch lÅ©y táº¡i thá»i Ä‘iá»ƒm ttt cÃ³ thá»ƒ Ä‘Æ°á»£c tÃ­nh nhÆ° sau: 
				G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} 
				
			Trong Ä‘Ã³ : 
				- G_t lÃ  tá»•ng pháº§n thÆ°á»Ÿng tÃ­ch lÅ©y báº¯t Ä‘áº§u tá»« thá»i Ä‘iá»ƒm t 
				- \gamma Ã  há»‡ sá»‘ giáº£m giÃ¡ (discount factor), Ä‘iá»u chá»‰nh táº§m quan trá»ng cá»§a pháº§n thÆ°á»Ÿng trong tÆ°Æ¡ng lai. 
				- R_t  lÃ  pháº§n thÆ°á»Ÿng nháº­n Ä‘Æ°á»£c táº¡i thá»i Ä‘iá»ƒm t 
				
# CÃ¡c phÆ°Æ¡ng phÃ¡p há»c tÄƒng cÆ°á»ng 
	Há»c tÄƒng cÆ°á»ng (Reinforcement Learning) cÃ³ nhiá»u phÆ°Æ¡ng phÃ¡p khÃ¡c nhau, tÃ¹y thuá»™c vÃ o cÃ¡ch tÃ¡c nhÃ¢n há»c vÃ  tá»‘i Æ°u hÃ³a chÃ­nh sÃ¡ch Ä‘á»ƒ nháº­n Ä‘Æ°á»£c pháº§n thÆ°á»Ÿng cao nháº¥t. DÆ°á»›i Ä‘Ã¢y lÃ  cÃ¡c phÆ°Æ¡ng phÃ¡p phá»• biáº¿n trong há»c tÄƒng cÆ°á»ng:
	
	## 3.1. PhÆ°Æ¡ng phÃ¡p há»c dá»±a trÃªn giÃ¡ trá»‹ (Value-based Methods) 
		PhÆ°Æ¡ng phÃ¡p nÃ y táº­p trung vÃ o viá»‡c Æ°á»›c tÃ­nh giÃ¡ trá»‹ cá»§a cÃ¡c tráº¡ng thÃ¡i hoáº·c cÃ¡c hÃ nh Ä‘á»™ng táº¡i cÃ¡c tráº¡ng thÃ¡i, sau Ä‘Ã³ chá»n hÃ nh Ä‘á»™ng dá»±a trÃªn cÃ¡c giÃ¡ trá»‹ nÃ y. TÃ¡c nhÃ¢n sáº½ há»c má»™t hÃ m giÃ¡ trá»‹ Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ má»©c Ä‘á»™ tá»‘t xáº¥u cá»§a má»™t tráº¡ng thÃ¡i hoáº·c hÃ nh Ä‘á»™ng.
		
		### Q-learning 
			ÄÃ¢y lÃ  thuáº­t toÃ¡n ná»•i tiáº¿ng nháº¥t trong nhÃ³m há»c dá»±a trÃªn giÃ¡ trá»‹. Q-learning sá»­ dá»¥ng hÃ m giÃ¡ trá»‹ Q(s, a) Ä‘á»ƒ Æ°á»›c tÃ­nh giÃ¡ trá»‹ cá»§a má»™t hÃ nh Ä‘á»™ng a táº¡i tráº¡ng thÃ¡i s CÃ´ng thá»©c cáº­p nháº­t Q-learning: 
				Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[ r_{t+1} + \gamma \max_a Q(s_{t+1}, a) â€“ Q(s_t, a_t) \right] 
				
			Trong Ä‘Ã³ 
				a  lÃ  tá»‘c Ä‘á»™ há»c. 
				y   lÃ  há»‡ sá»‘ giáº£m giÃ¡.
				r_{t+1}	lÃ  pháº§n thÆ°á»Ÿng sau khi thá»±c hiá»‡n hÃ nh Ä‘á»™ng  a_t
				 
		### SARSA (State-Action-Reward-State-Action) 
			SARSA lÃ  má»™t phÆ°Æ¡ng phÃ¡p tÆ°Æ¡ng tá»± nhÆ° Q-learning, nhÆ°ng thay vÃ¬ tá»‘i Æ°u hÃ³a theo hÃ nh Ä‘á»™ng tá»‘t nháº¥t tiáº¿p theo, nÃ³ sá»­ dá»¥ng hÃ nh Ä‘á»™ng mÃ  tÃ¡c nhÃ¢n thá»±c sá»± thá»±c hiá»‡n Ä‘á»ƒ cáº­p nháº­t giÃ¡ trá»‹ Q 
			
			CÃ´ng thá»©c cáº­p nháº­t SARSA:			
				Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[ r_{t+1} + \gamma Q(s_{t+1}, a_{t+1}) â€“ Q(s_t, a_t) \right]
				
	## 	3.2. PhÆ°Æ¡ng phÃ¡p há»c chÃ­nh sÃ¡ch (Policy-based Methods) 
		Thay vÃ¬ há»c hÃ m giÃ¡ trá»‹, phÆ°Æ¡ng phÃ¡p há»c chÃ­nh sÃ¡ch trá»±c tiáº¿p há»c má»™t chiáº¿n lÆ°á»£c hoáº·c chÃ­nh sÃ¡ch \pi(a|s) giÃºp tÃ¡c nhÃ¢n chá»n hÃ nh Ä‘á»™ng dá»±a trÃªn tráº¡ng thÃ¡i hiá»‡n táº¡i. 
		
		### Policy Gradient 
			ÄÃ¢y lÃ  má»™t phÆ°Æ¡ng phÃ¡p sá»­ dá»¥ng Ä‘á»™ dá»‘c (gradient) Ä‘á»ƒ Ä‘iá»u chá»‰nh chÃ­nh sÃ¡ch sao cho tÄƒng cÆ°á»ng kháº£ nÄƒng cá»§a tÃ¡c nhÃ¢n trong viá»‡c Ä‘áº¡t pháº§n thÆ°á»Ÿng cao. TÃ¡c nhÃ¢n há»c trá»±c tiáº¿p chiáº¿n lÆ°á»£c chá»n hÃ nh Ä‘á»™ng tá»‘i Æ°u thÃ´ng qua viá»‡c tá»‘i Æ°u hÃ³a hÃ m má»¥c tiÃªu. 
			
			HÃ m máº¥t mÃ¡t cá»§a policy gradient thÆ°á»ng lÃ :
				\ \nabla J(\theta) = \mathbb{E}_{\pi_\theta} \left[ \nabla_\theta \log \pi_\theta (a|s) G_t \right] 
				
			Trong Ä‘Ã³: 
				J(\theta) lÃ  hÃ m má»¥c tiÃªu, \theta lÃ  cÃ¡c tham sá»‘ cá»§a chÃ­nh sÃ¡ch.
				\pi_\theta(a|s) lÃ  xÃ¡c suáº¥t chá»n hÃ nh Ä‘á»™ng a táº¡i tráº¡ng thÃ¡i s dÆ°á»›i chÃ­nh sÃ¡ch hiá»‡n táº¡i. 
				G_t lÃ  tá»•ng pháº§n thÆ°á»Ÿng tÃ­ch lÅ©y tá»« thá»i Ä‘iá»ƒm t 
				
		### REINFORCE	
			REINFORCE lÃ  má»™t thuáº­t toÃ¡n cá»¥ thá»ƒ trong policy gradient. NÃ³ sá»­ dá»¥ng gradient descent Ä‘á»ƒ tá»‘i Æ°u hÃ³a chÃ­nh sÃ¡ch
			\pi_\theta
			
			ChÃ­nh sÃ¡ch Ä‘Æ°á»£c cáº­p nháº­t dá»±a trÃªn tá»•ng pháº§n thÆ°á»Ÿng nháº­n Ä‘Æ°á»£c tá»« má»—i táº­p thá»­ (episode).

	## 3.3. PhÆ°Æ¡ng phÃ¡p há»c lai (Actor-Critic Methods) 
		PhÆ°Æ¡ng phÃ¡p nÃ y káº¿t há»£p cáº£ há»c chÃ­nh sÃ¡ch vÃ  há»c giÃ¡ trá»‹. Actor-Critic sá»­ dá»¥ng hai thÃ nh pháº§n chÃ­nh: 
			- Actor: Chá»‹u trÃ¡ch nhiá»‡m há»c chÃ­nh sÃ¡ch \pi(a|s) tá»« Ä‘Ã³ chá»n hÃ nh Ä‘á»™ng. 
			- Critic: Chá»‹u trÃ¡ch nhiá»‡m há»c hÃ m giÃ¡ trá»‹  V(s) tá»« Ä‘Ã³ Ä‘Ã¡nh giÃ¡ hÃ nh Ä‘á»™ng mÃ  Actor Ä‘Ã£ chá»n. 
			
		Critic Ä‘Ã¡nh giÃ¡ hÃ nh Ä‘á»™ng cá»§a Actor dá»±a trÃªn sá»± chÃªnh lá»‡ch giá»¯a pháº§n thÆ°á»Ÿng thá»±c táº¿ vÃ  giÃ¡ trá»‹ dá»± Ä‘oÃ¡n. Sau Ä‘Ã³, Actor Ä‘iá»u chá»‰nh chÃ­nh sÃ¡ch cá»§a mÃ¬nh Ä‘á»ƒ cáº£i thiá»‡n quyáº¿t Ä‘á»‹nh hÃ nh Ä‘á»™ng.
		ctor-Criti
		###  Advantage Ac (A2C/A3C) 
			ÄÃ¢y lÃ  má»™t thuáº­t toÃ¡n trong phÆ°Æ¡ng phÃ¡p Actor-Critic. A2C lÃ  phiÃªn báº£n Ä‘Æ¡n luá»“ng, trong khi A3C (Asynchronous Advantage Actor-Critic) lÃ  phiÃªn báº£n báº¥t Ä‘á»“ng bá»™, nÆ¡i nhiá»u tÃ¡c nhÃ¢n cÃ³ thá»ƒ há»c song song Ä‘á»ƒ cáº£i thiá»‡n hiá»‡u suáº¥t. 
			
			HÃ m máº¥t mÃ¡t cá»§a A2C thÆ°á»ng lÃ :
			\ \nabla J(\theta) = \mathbb{E}_{\pi_\theta} \left[ \nabla_\theta \log \pi_\theta (a|s) A(s, a) \right]
			
			Trong Ä‘Ã³ A(s, a) = Q(s, a) â€“ V(s)  lÃ  giÃ¡ trá»‹ Advantage (lá»£i tháº¿) cá»§a hÃ nh Ä‘á»™ng so vá»›i cÃ¡c hÃ nh Ä‘á»™ng khÃ¡c táº¡i tráº¡ng thÃ¡i s

	##  PhÆ°Æ¡ng phÃ¡p há»c sÃ¢u tÄƒng cÆ°á»ng (Deep Reinforcement Learning) 
			Khi mÃ´i trÆ°á»ng cÃ³ kÃ­ch thÆ°á»›c khÃ´ng gian tráº¡ng thÃ¡i lá»›n hoáº·c phá»©c táº¡p, cÃ¡c phÆ°Æ¡ng phÃ¡p truyá»n thá»‘ng cÃ³ thá»ƒ khÃ´ng hoáº¡t Ä‘á»™ng tá»‘t. PhÆ°Æ¡ng phÃ¡p há»c sÃ¢u tÄƒng cÆ°á»ng káº¿t há»£p máº¡ng nÆ¡-ron sÃ¢u (deep neural networks) Ä‘á»ƒ xáº¥p xá»‰ hÃ m giÃ¡ trá»‹ vÃ  chÃ­nh sÃ¡ch. 
			
		### 	Deep Q-Network (DQN) 
			ÄÃ¢y lÃ  má»™t phiÃªn báº£n cá»§a Q-learning nhÆ°ng sá»­ dá»¥ng máº¡ng nÆ¡-ron sÃ¢u Ä‘á»ƒ xáº¥p xá»‰ hÃ m giÃ¡ trá»‹ Q(s, a)Máº¡ng nÆ¡-ron nÃ y giÃºp tÃ¡c nhÃ¢n xá»­ lÃ½ Ä‘Æ°á»£c cÃ¡c mÃ´i trÆ°á»ng phá»©c táº¡p vá»›i khÃ´ng gian tráº¡ng thÃ¡i liÃªn tá»¥c hoáº·c quÃ¡ lá»›n Ä‘á»ƒ cÃ³ thá»ƒ biá»ƒu diá»…n báº±ng báº£ng giÃ¡ trá»‹ Q. 
				Q(s, a; \theta) \approx \max_a Q(s, a)
				
			Trong Ä‘Ã³ \theta lÃ  tham sá»‘ cá»§a máº¡ng nÆ¡-ron.
			
		### Deep Deterministic Policy Gradient (DDPG) 
			ÄÃ¢y lÃ  má»™t thuáº­t toÃ¡n há»c chÃ­nh sÃ¡ch káº¿t há»£p vá»›i há»c sÃ¢u, hoáº¡t Ä‘á»™ng tá»‘t trong khÃ´ng gian hÃ nh Ä‘á»™ng liÃªn tá»¥c. DDPG lÃ  má»™t sá»± káº¿t há»£p cá»§a phÆ°Æ¡ng phÃ¡p Actor-Critic vá»›i máº¡ng nÆ¡-ron sÃ¢u.

	##  PhÆ°Æ¡ng phÃ¡p Multi-agent Reinforcement Learning (MARL)
			Trong nhiá»u á»©ng dá»¥ng, cÃ³ nhiá»u tÃ¡c nhÃ¢n (agent) hoáº¡t Ä‘á»™ng trong cÃ¹ng má»™t mÃ´i trÆ°á»ng. PhÆ°Æ¡ng phÃ¡p há»c tÄƒng cÆ°á»ng Ä‘a tÃ¡c nhÃ¢n (Multi-agent Reinforcement Learning â€“ MARL) nghiÃªn cá»©u cÃ¡ch nhiá»u tÃ¡c nhÃ¢n há»c cÃ¡ch há»£p tÃ¡c hoáº·c cáº¡nh tranh Ä‘á»ƒ tá»‘i Æ°u hÃ³a má»¥c tiÃªu cá»§a tá»«ng tÃ¡c nhÃ¢n.
			
		### Cooperative MARL 
			CÃ¡c tÃ¡c nhÃ¢n há»c cÃ¡ch há»£p tÃ¡c Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c má»¥c tiÃªu chung. 
			Nhá»¯ng phÆ°Æ¡ng phÃ¡p nÃ y Ä‘Ã£ vÃ  Ä‘ang Ä‘Æ°á»£c Ã¡p dá»¥ng rá»™ng rÃ£i trong cÃ¡c lÄ©nh vá»±c khÃ¡c nhau nhÆ° chÆ¡i game, robot tá»± hÃ nh, tá»‘i Æ°u hÃ³a há»‡ thá»‘ng, vÃ  há»‡ thá»‘ng Ä‘á» xuáº¥t, Ä‘em láº¡i hiá»‡u quáº£ cao trong cÃ¡c bÃ i toÃ¡n há»c phá»©c táº¡p.
			
# á»¨ng dá»¥ng cá»§a há»c tÄƒng cÆ°á»ng 

	## TrÃ­ tuá»‡ nhÃ¢n táº¡o trong trÃ² chÆ¡i (AI Gaming) 
		RL Ä‘Ã£ Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i trong viá»‡c phÃ¡t triá»ƒn cÃ¡c AI cÃ³ kháº£ nÄƒng chÆ¡i cÃ¡c trÃ² chÆ¡i phá»©c táº¡p. Má»™t vÃ­ dá»¥ ná»•i tiáº¿ng lÃ  AlphaGo cá»§a DeepMind, Ä‘Ã£ Ä‘Ã¡nh báº¡i nhÃ  vÃ´ Ä‘á»‹ch cá» vÃ¢y tháº¿ giá»›i vÃ o nÄƒm 2016. Há»‡ thá»‘ng nÃ y sá»­ dá»¥ng há»c tÄƒng cÆ°á»ng káº¿t há»£p vá»›i máº¡ng nÆ¡-ron Ä‘á»ƒ phÃ¢n tÃ­ch hÃ ng triá»‡u tháº¿ cá» vÃ  Ä‘Æ°a ra chiáº¿n lÆ°á»£c tá»‘i Æ°u. 
		
	## Äiá»u khiá»ƒn Robot (Robotics Control) 
		Trong lÄ©nh vá»±c robot, RL giÃºp robot há»c cÃ¡ch thá»±c hiá»‡n cÃ¡c nhiá»‡m vá»¥ phá»©c táº¡p nhÆ° Ä‘iá»u hÆ°á»›ng, gáº¯p Ä‘á»“ váº­t, vÃ  tá»± Ä‘á»™ng hÃ³a sáº£n xuáº¥t. CÃ¡c thuáº­t toÃ¡n há»c tÄƒng cÆ°á»ng cho phÃ©p robot há»c cÃ¡ch tá»‘i Æ°u hÃ³a hÃ nh Ä‘á»™ng cá»§a mÃ¬nh mÃ  khÃ´ng cáº§n can thiá»‡p tá»« con ngÆ°á»i, vÃ­ dá»¥ nhÆ° Boston Dynamics sá»­ dá»¥ng RL Ä‘á»ƒ Ä‘iá»u khiá»ƒn chuyá»ƒn Ä‘á»™ng linh hoáº¡t cá»§a robot bá»‘n chÃ¢n. 
		
	## Xe tá»± hÃ nh (Autonomous Vehicles) 
		Xe tá»± hÃ nh lÃ  má»™t trong nhá»¯ng á»©ng dá»¥ng ná»•i báº­t cá»§a RL, nÆ¡i cÃ¡c thuáº­t toÃ¡n Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ há»c cÃ¡ch lÃ¡i xe trong cÃ¡c mÃ´i trÆ°á»ng phá»©c táº¡p. CÃ¡c há»‡ thá»‘ng tá»± hÃ nh nhÆ° cá»§a Tesla hay Waymo sá»­ dá»¥ng RL Ä‘á»ƒ cáº£i thiá»‡n kháº£ nÄƒng phÃ¡t hiá»‡n chÆ°á»›ng ngáº¡i váº­t, ra quyáº¿t Ä‘á»‹nh vÃ  Ä‘iá»u khiá»ƒn xe má»™t cÃ¡ch an toÃ n, tháº­m chÃ­ trong nhá»¯ng tÃ¬nh huá»‘ng khÃ´ng lÆ°á»ng trÆ°á»›c. 
		
	##  Quáº£n lÃ½ tÃ i chÃ­nh vÃ  giao dá»‹ch tá»± Ä‘á»™ng (Financial Trading) 
		RL Ä‘ang Ä‘Æ°á»£c Ã¡p dá»¥ng trong thá»‹ trÆ°á»ng tÃ i chÃ­nh Ä‘á»ƒ phÃ¡t triá»ƒn cÃ¡c há»‡ thá»‘ng giao dá»‹ch tá»± Ä‘á»™ng. CÃ¡c thuáº­t toÃ¡n cÃ³ thá»ƒ há»c cÃ¡ch dá»± Ä‘oÃ¡n biáº¿n Ä‘á»™ng giÃ¡ cáº£ vÃ  Ä‘Æ°a ra cÃ¡c quyáº¿t Ä‘á»‹nh mua bÃ¡n sao cho tá»‘i Ä‘a hÃ³a lá»£i nhuáº­n dá»±a trÃªn dá»¯ liá»‡u thá»‹ trÆ°á»ng. Má»™t vÃ­ dá»¥ lÃ  viá»‡c sá»­ dá»¥ng RL trong hedge funds Ä‘á»ƒ tá»‘i Æ°u hÃ³a chiáº¿n lÆ°á»£c Ä‘áº§u tÆ°. 
		
	## 	Tá»‘i Æ°u hÃ³a máº¡ng lÆ°á»›i vÃ  tÃ i nguyÃªn (Network Optimization) 
		Trong cÃ¡c há»‡ thá»‘ng viá»…n thÃ´ng vÃ  quáº£n lÃ½ háº¡ táº§ng máº¡ng, RL giÃºp tá»‘i Æ°u hÃ³a viá»‡c phÃ¢n bá»• tÃ i nguyÃªn, giáº£m thiá»ƒu Ä‘á»™ trá»… vÃ  nÃ¢ng cao hiá»‡u quáº£ truyá»n táº£i. Äiá»u nÃ y ráº¥t quan trá»ng trong cÃ¡c há»‡ thá»‘ng nhÆ° 5G hoáº·c quáº£n lÃ½ bÄƒng thÃ´ng internet, nÆ¡i mÃ  há»‡ thá»‘ng cáº§n pháº£n á»©ng nhanh chÃ³ng vÃ  hiá»‡u quáº£ vá»›i sá»± thay Ä‘á»•i trong yÃªu cáº§u cá»§a ngÆ°á»i dÃ¹ng. 
		
	## Há»‡ thá»‘ng Ä‘á» xuáº¥t (Recommendation Systems) 
		Trong cÃ¡c ná»n táº£ng nhÆ° Netflix, Amazon, vÃ  YouTube, RL Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ tá»‘i Æ°u hÃ³a há»‡ thá»‘ng Ä‘á» xuáº¥t dá»±a trÃªn pháº£n há»“i cá»§a ngÆ°á»i dÃ¹ng. Thay vÃ¬ dá»±a hoÃ n toÃ n vÃ o cÃ¡c phÆ°Æ¡ng phÃ¡p truyá»n thá»‘ng, há»‡ thá»‘ng RL há»c cÃ¡ch Ä‘á» xuáº¥t ná»™i dung hoáº·c sáº£n pháº©m má»™t cÃ¡ch cÃ¡ nhÃ¢n hÃ³a dá»±a trÃªn hÃ nh vi vÃ  sá»Ÿ thÃ­ch cá»§a ngÆ°á»i dÃ¹ng.
		
# 5. VÃ­ dá»¥ mÃ£ nguá»“n sá»­ dá»¥ng PyTorch		

# 6. Káº¿t luáº­n 
	Há»c tÄƒng cÆ°á»ng (Reinforcement Learning) lÃ  má»™t trong nhá»¯ng lÄ©nh vá»±c Ä‘á»™t phÃ¡ cá»§a há»c mÃ¡y, mang láº¡i nhiá»u á»©ng dá»¥ng thá»±c tiá»…n trong Ä‘a dáº¡ng ngÃ nh cÃ´ng nghiá»‡p, tá»« trÃ­ tuá»‡ nhÃ¢n táº¡o trong trÃ² chÆ¡i, Ä‘iá»u khiá»ƒn robot, Ä‘áº¿n xe tá»± hÃ nh vÃ  y táº¿. Vá»›i kháº£ nÄƒng há»c há»i tá»« mÃ´i trÆ°á»ng thÃ´ng qua quÃ¡ trÃ¬nh thá»­ vÃ  sai, cÃ¡c thuáº­t toÃ¡n RL khÃ´ng chá»‰ giÃºp mÃ¡y mÃ³c tá»± Ä‘á»™ng hÃ³a vÃ  tá»‘i Æ°u hÃ³a quy trÃ¬nh, mÃ  cÃ²n má»Ÿ ra tiá»m nÄƒng phÃ¡t triá»ƒn nhá»¯ng há»‡ thá»‘ng thÃ´ng minh hÆ¡n, cÃ³ thá»ƒ ra quyáº¿t Ä‘á»‹nh tá»‘t hÆ¡n trong cÃ¡c tÃ¬nh huá»‘ng phá»©c táº¡p. TÆ°Æ¡ng lai cá»§a há»c tÄƒng cÆ°á»ng há»©a háº¹n tiáº¿p tá»¥c Ä‘Ã³ng gÃ³p quan trá»ng vÃ o sá»± phÃ¡t triá»ƒn cá»§a trÃ­ tuá»‡ nhÃ¢n táº¡o vÃ  cÃ´ng nghá»‡ hiá»‡n Ä‘áº¡i.


</pre><a id='backBottom' href='../AI-learning-list.html' style='display:none;'>ğŸ”™ Quay láº¡i danh sÃ¡ch</a><br><button onclick='toggleTheme()'>ğŸŒ™ Chuyá»ƒn giao diá»‡n</button></div><script>function toggleTheme() {   let mode = document.body.classList.contains('dark-mode') ? 'light-mode' : 'dark-mode';   document.body.className = mode; localStorage.setItem('theme', mode);   syncTheme();}function applyTheme() {   let savedTheme = localStorage.getItem('theme') || 'dark-mode';   document.body.className = savedTheme;   syncTheme();}function syncTheme() {   let preElement = document.querySelector('pre');   if (document.body.classList.contains('dark-mode')) { preElement.style.background = '#1e1e1e'; preElement.style.color = '#e0e0e0'; }   else { preElement.style.background = '#f5f5f5'; preElement.style.color = '#333333'; }}function checkPageHeight() {   let contentHeight = document.body.scrollHeight;   let windowHeight = window.innerHeight;   if (contentHeight > windowHeight * 1.2) {       document.getElementById('backBottom').style.display = 'block';   } else {       document.getElementById('backBottom').style.display = 'none';   }}</script></body></html>